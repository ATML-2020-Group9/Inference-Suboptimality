{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.2 Bernoulli, IWAE fixed.ipynb","provenance":[{"file_id":"1TjVoVbWoJTxkKkTqxXKAKZaRplK_zYeX","timestamp":1588169360887}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8fXNm1kg4YWU","colab_type":"text"},"source":["#Required Terminal Commands"]},{"cell_type":"markdown","metadata":{"id":"wYQoGE2-7u2Q","colab_type":"text"},"source":["First, run this code to mount your drive to your runtime:"]},{"cell_type":"code","metadata":{"id":"AUgI7tKTyAPT","colab_type":"code","outputId":"3987b9a5-5438-4bf0-bf50-2831c9bfda44","executionInfo":{"status":"ok","timestamp":1588173794178,"user_tz":-120,"elapsed":1878,"user":{"displayName":"Hjalmar Wijk","photoUrl":"","userId":"08666726544445093304"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IP5ezukn7yMA","colab_type":"text"},"source":["Now, assuming the shared folder is present in your drive, this step should work. If it doesn't, make sure your ATiML folder isn't nested in another folder."]},{"cell_type":"code","metadata":{"id":"FnbTsSMc3OxB","colab_type":"code","outputId":"915bc7aa-0d77-489f-e769-078c6af9dff3","executionInfo":{"status":"ok","timestamp":1588173797241,"user_tz":-120,"elapsed":4923,"user":{"displayName":"Hjalmar Wijk","photoUrl":"","userId":"08666726544445093304"}},"colab":{"base_uri":"https://localhost:8080/","height":109}},"source":["%cd /content/drive/My\\ Drive/ATiML\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/ATiML\n"," checkpoints   expansion2\t       MNIST_notused\t   Results.gsheet\n"," datasets      Experiment_3\t       old_checkpoints\n"," Exp4\t       Extension_Planar_Flow   Plan_V2.gdoc\n"," Exp5\t      'Final Code'\t      'Poster Plan.gdoc'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jWaxu8G0_3BK","colab_type":"text"},"source":["All of the following commands are just to get the right python libraries in your runtime - I'm not yet sure if you close the window and re-open whether you need to run these again"]},{"cell_type":"code","metadata":{"id":"MjdBBwnTQRf5","colab_type":"code","outputId":"c20918a9-cf78-4ac7-e04d-f0bba8268e67","executionInfo":{"status":"ok","timestamp":1588173799108,"user_tz":-120,"elapsed":6777,"user":{"displayName":"Hjalmar Wijk","photoUrl":"","userId":"08666726544445093304"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!python --version"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Python 3.6.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XFd2Dg1PIG9Z","colab_type":"code","outputId":"2e7ac491-851d-490a-b168-c24e5053d670","executionInfo":{"status":"ok","timestamp":1588173802334,"user_tz":-120,"elapsed":9989,"user":{"displayName":"Hjalmar Wijk","photoUrl":"","userId":"08666726544445093304"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pip3 install tqdm"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L7y8GxxaILzZ","colab_type":"code","outputId":"2459e373-b29a-43f1-af3a-0f98fef416eb","executionInfo":{"status":"ok","timestamp":1588173806173,"user_tz":-120,"elapsed":13819,"user":{"displayName":"Hjalmar Wijk","photoUrl":"","userId":"08666726544445093304"}},"colab":{"base_uri":"https://localhost:8080/","height":92}},"source":["!pip3 install http://download.pytorch.org/whl/cu80/torch-0.2.0.post3-cp36-cp36m-manylinux1_x86_64.whl"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch==0.2.0.post3 from http://download.pytorch.org/whl/cu80/torch-0.2.0.post3-cp36-cp36m-manylinux1_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.2.0.post3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.2.0.post3) (1.18.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.2.0.post3) (3.13)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b3WvYy5CI8Jo","colab_type":"code","outputId":"a8d5cef8-805b-40df-8929-9eb9b5afe818","executionInfo":{"status":"ok","timestamp":1588173809746,"user_tz":-120,"elapsed":17383,"user":{"displayName":"Hjalmar Wijk","photoUrl":"","userId":"08666726544445093304"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["!pip3 install torchvision==0.2"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchvision==0.2 in /usr/local/lib/python3.6/dist-packages (0.2.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2) (0.2.0.post3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2) (1.18.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==0.2) (3.13)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lHJEnpa9JAF1","colab_type":"code","outputId":"62846a6a-f29c-4348-985e-1ee47e26f2a7","executionInfo":{"status":"ok","timestamp":1588173813269,"user_tz":-120,"elapsed":20899,"user":{"displayName":"Hjalmar Wijk","photoUrl":"","userId":"08666726544445093304"}},"colab":{"base_uri":"https://localhost:8080/","height":311}},"source":["!pip3 install visdom"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: visdom in /usr/local/lib/python3.6/dist-packages (0.1.8.9)\n","Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from visdom) (1.18.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom) (2.21.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom) (4.5.3)\n","Requirement already satisfied: torchfile in /usr/local/lib/python3.6/dist-packages (from visdom) (0.1.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom) (1.4.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom) (7.0.0)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom) (19.0.0)\n","Requirement already satisfied: jsonpatch in /usr/local/lib/python3.6/dist-packages (from visdom) (1.25)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from visdom) (1.12.0)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.6/dist-packages (from visdom) (0.57.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2020.4.5.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2.8)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.6/dist-packages (from jsonpatch->visdom) (2.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pvKXSbRr6g7h","colab_type":"text"},"source":["#imports"]},{"cell_type":"code","metadata":{"id":"HKEVXC-hHoX2","colab_type":"code","cellView":"code","colab":{}},"source":["#@title Default title text\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import time\n","import sys\n","import os\n","import math\n","import argparse\n","from tqdm import tqdm\n","import numpy as np\n","# i've commented out this line since it just seems to change the total number of array elements which trigger summarization, and it causes an error.\n","# np.set_printoptions(threshold=np.nan)\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms, utils\n","import visdom"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TsrMoyN34k6Q","colab_type":"text"},"source":["#math_ops file"]},{"cell_type":"code","metadata":{"id":"WAo-UFCGKDIi","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import math\n","import numpy as np\n","import numpy.linalg as linalg\n","\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","\n","def log_normal(x, mean=None, logvar=None,repeat=False): #[1040336] Added the ability to run this on tensors [P,B,Z] to interface with my memory management tweaks\n","    \"\"\"Implementation WITHOUT constant, since the constants in p(z) \n","    and q(z|x) cancels out.\n","    Args:\n","        x: [B,Z]\n","        mean,logvar: [B,Z]\n","\n","    Returns:\n","        output: [B]\n","    \"\"\"\n","    if mean is None:\n","        mean = Variable(torch.zeros(x.size()).type(type(x.data)))\n","    if logvar is None:\n","        logvar = Variable(torch.zeros(x.size()).type(type(x.data)))\n","    if repeat:\n","      D = x.size()[2]\n","      if torch.cuda.is_available():\n","          term1 = D * torch.log(torch.cuda.FloatTensor([2.*math.pi])) #[1]\n","      else:\n","          term1 = D * torch.log(torch.FloatTensor([2.*math.pi])) #[1]\n","\n","\n","      return -.5 * (Variable(term1) + logvar.sum(1) + ((x - mean).pow(2)/torch.exp(logvar)).sum(2))\n","\n","    return -0.5 * (logvar.sum(1) + ((x - mean).pow(2) / torch.exp(logvar)).sum(1))\n","\n","\n","def log_normal_full_cov(x, mean, L):\n","    \"\"\"Log density of full covariance multivariate Gaussian.\n","    Note: results are off by the constant log(), since this \n","    quantity cancels out in p(z) and q(z|x).\"\"\"\n","\n","    def batch_diag(M):\n","        diag = [t.diag() for t in torch.functional.unbind(M)]\n","        diag = torch.functional.stack(diag)\n","        return diag\n","\n","    def batch_inverse(M, damp=False, eps=1e-6):\n","        damp_matrix = Variable(torch.eye(M[0].size(0)).type(M.data.type())).mul_(eps)\n","        inverse = []\n","        for t in torch.functional.unbind(M):\n","            # damping to ensure invertible due to float inaccuracy\n","            # this problem is very UNLIKELY when using double\n","            m = t if not damp else t + damp_matrix\n","            inverse.append(m.inverse())\n","        inverse = torch.functional.stack(inverse)\n","        return inverse\n","\n","    L_diag = batch_diag(L)\n","    term1 = -torch.log(L_diag).sum(1)\n","\n","    L_inverse = batch_inverse(L)\n","    scaled_diff = L_inverse.matmul((x - mean).unsqueeze(2)).squeeze()\n","    term2 = -0.5 * (scaled_diff ** 2).sum(1)\n","\n","    return term1 + term2\n","\n","\n","def log_bernoulli(logit, target, repeat = False): #[1040336] Added the ability to run this on tensors [P,B,X] to interface with my memory management tweaks\n","    \"\"\"\n","    Args:\n","        logit:  [B, X, ?, ?]\n","        target: [B, X, ?, ?]\n","    \n","    Returns:\n","        output:      [B]\n","    \"\"\"\n","    if repeat:\n","      return -(torch.clamp(logit, min=0)- logit * target\n","             + torch.log(1. + torch.exp(-torch.abs(logit)))).sum(2) #sum over dimensions\n","    loss = -F.relu(logit) + torch.mul(target, logit) - torch.log(1. + torch.exp( -logit.abs() ))\n","    while len(loss.size()) > 1:\n","        loss = loss.sum(-1)\n","    return loss\n","\n","def log_bernoulli_cont(logit, target, thresh = 1e-4, repeat = False): #[1040336] Wrote this function, which computed continuous bernoulli likelihood from decoder outputs\n","    ones = Variable(torch.ones(logit.size()).cuda())\n","    logprob = logit-F.relu(logit) - torch.log(ones +torch.exp(-logit.abs()))\n","    loginvprob = logprob -logit\n","    prob = torch.exp(logprob)\n","    logbernoulli = target * logprob + (1.-target)*loginvprob #unnormalized probability, same as regular\n","    #print('Unnormalised:',logbernoulli)\n","    mask = torch.abs(prob - 0.5).ge(thresh).float().cuda()\n","    \n","    safe_mask = mask*prob+(ones-mask)*ones*0.75\n","    critical_mask = mask*thresh+(ones-mask)*prob\n","    safe_values =  mask*torch.log( (torch.log(ones - safe_mask) - torch.log(safe_mask)).div(ones - (ones * 2) * safe_mask) )\n","    #print('Computed far values',far_values)\n","    taylor_values = (ones-mask)*(torch.log(Variable(torch.FloatTensor([2.]).cuda())) + torch.log(1. + torch.pow( 1. - 2. * critical_mask, 2)/3. ))\n","    #print('Calculated close values',close_values)\n","    loss = logbernoulli +safe_values + taylor_values\n","    if repeat == False:\n","      while len(loss.size()) > 1:\n","          loss = loss.sum(-1)\n","\n","      return loss\n","    else:\n","      return loss.sum(2)\n","def mean_cont_bern(prob,thresh = 1e-5):    #[1040336] Wrote this function, which computes mean of continuous bernoulli distribution. Unlike Binary mean is not just prob.\n","    ones = Variable(torch.ones(prob.size()).cuda())\n","    mask = torch.abs(prob - 0.5).ge(thresh).float().cuda()\n","    safe_mask = mask*prob+(ones-mask)*0.75\n","    critical = mask*thresh+(ones-mask)*prob\n","    safe_values =  mask*(torch.div(safe_mask,2*safe_mask-ones)+torch.div(ones, torch.log(ones-safe_mask)-torch.log(safe_mask)))\n","    critical_values = (ones-mask)*0.5\n","    #print('Calculated close values',close_values)\n","    return safe_values + critical_values\n","def mean_squared_error(prediction, target):\n","    prediction, target = flatten(prediction), flatten(target)\n","    diff = prediction - target\n","\n","    return -torch.sum(torch.mul(diff, diff), 1)\n","\n","\n","def discretized_logistic(mu, logs, x):\n","    \"\"\"Probability mass follow discretized logistic. \n","    https://arxiv.org/pdf/1606.04934.pdf. Assuming pixel values scaled to be\n","    within [0,1]. Follows implementation from OpenAI.\n","    \"\"\"\n","    sigmoid = torch.nn.Sigmoid()\n","\n","    s = torch.exp(logs).unsqueeze(-1).unsqueeze(-1)\n","    logp = torch.log(sigmoid((x + 1./256. - mu) / s) - sigmoid((x - mu) / s) + 1e-7)\n","\n","    return logp.sum(-1).sum(-1).sum(-1)\n","\n","\n","def flatten(x):\n","    return x.view(x.size(0), -1)\n","\n","\n","def log_mean_exp(x):\n","    max_, _ = torch.max(x, 1, keepdim=True)\n","    return torch.log(torch.mean(torch.exp(x - max_), 1)) + torch.squeeze(max_)\n","\n","\n","def numpy_nan_guard(arr):\n","    return np.all(arr == arr)\n","\n","\n","def safe_repeat(x, n):\n","    return x.repeat(n, *[1 for _ in range(len(x.size()) - 1)])\n","\n","\n","def sigmoidial_schedule(T, delta=4):\n","    \"\"\"From section 6 of BDMC paper.\"\"\"\n","\n","    def sigmoid(x):\n","        return np.exp(x) / (1. + np.exp(x))\n","\n","    def beta_tilde(t):\n","        return sigmoid(delta * (2.*t / T - 1.))\n","\n","    def beta(t):\n","        return (beta_tilde(t) - beta_tilde(1)) / (beta_tilde(T) - beta_tilde(1))\n","\n","    return [beta(t) for t in range(1, T+1)]\n","\n","\n","def linear_schedule(T):\n","    return np.linspace(0., 1., T)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kgoYsbP44r5h","colab_type":"text"},"source":["#hparams file"]},{"cell_type":"code","metadata":{"id":"5VRLxv-kJexh","colab_type":"code","colab":{}},"source":["class HParams(object):\n","\n","    def __init__(self, **kwargs):\n","        self._items = {}\n","        for k, v in kwargs.items():\n","            self._set(k, v)\n","\n","    def _set(self, k, v):\n","        self._items[k] = v\n","        setattr(self, k, v)\n","\n","    def parse(self, str_value):\n","        hps = HParams(**self._items)\n","        for entry in str_value.strip().split(\",\"):\n","            entry = entry.strip()\n","            if not entry:\n","                continue\n","            key, sep, value = entry.partition(\"=\")\n","            if not sep:\n","                raise ValueError(\"Unable to parse: %s\" % entry)\n","            default_value = hps._items[key]\n","            if isinstance(default_value, bool):\n","                hps._set(key, value.lower() == \"true\")\n","            elif isinstance(default_value, int):\n","                hps._set(key, int(value))\n","            elif isinstance(default_value, float):\n","                hps._set(key, float(value))\n","            else:\n","                hps._set(key, value)\n","        return hps\n","\n","def get_default_hparams():\n","    return HParams(\n","        z_size=50,\n","        act_func=F.elu,\n","        has_flow=False,\n","        large_encoder=False,\n","        wide_encoder=False,\n","        cuda=True,\n","        decode_dist = log_bernoulli,\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XivhktJ14xNs","colab_type":"text"},"source":["#hmc file"]},{"cell_type":"code","metadata":{"id":"jjc-A--nKKmV","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import sys\n","import os\n","import math\n","import torch\n","import numpy as np\n","\n","import torch\n","from torch.autograd import Variable\n","\n","\n","def hmc_trajectory(current_z, current_v, U, grad_U, epsilon, L=10):\n","    \"\"\"This version of HMC follows https://arxiv.org/pdf/1206.1901.pdf.\n","\n","    Args:\n","        U: function to compute potential energy/minus log-density\n","        grad_U: function to compute gradients w.r.t. U\n","        epsilon: (adaptive) step size\n","        L: number of leap-frog steps\n","        current_z: current position\n","    \"\"\"\n","\n","    # as of `torch-0.3.0.post4`, there still is no proper scalar support\n","    assert isinstance(epsilon, Variable)\n","\n","    eps = epsilon.view(-1, 1)\n","    z = current_z\n","    v = current_v - grad_U(z).mul(eps).mul_(.5)\n","\n","    for i in range(1, L+1):\n","        z = z + v.mul(eps)\n","        if i < L:\n","            v = v - grad_U(z).mul(eps)\n","\n","    v = v - grad_U(z).mul(eps).mul_(.5)\n","    v = -v  # this is not needed; only here to conform to the math\n","\n","    return z.detach(), v.detach()\n","\n","\n","def accept_reject(current_z, current_v, \n","                  z, v, \n","                  epsilon, \n","                  accept_hist, hist_len, \n","                  U, K=lambda v: torch.sum(v * v, 1)):\n","    \"\"\"Accept/reject based on Hamiltonians for current and propose.\n","\n","    Args:\n","        current_z: position BEFORE leap-frog steps\n","        current_v: speed BEFORE leap-frog steps\n","        z: position AFTER leap-frog steps\n","        v: speed AFTER leap-frog steps\n","        epsilon: step size of leap-frog.\n","                (This is only needed for adaptive update)\n","        U: function to compute potential energy (MINUS log-density)\n","        K: function to compute kinetic energy (default: kinetic energy in physics w/ mass=1)\n","    \"\"\"\n","\n","    mdtype = type(current_z.data)\n","\n","    current_Hamil = K(current_v) + U(current_z)\n","    propose_Hamil = K(v) + U(z)\n","\n","    prob = torch.exp(current_Hamil - propose_Hamil)\n","    uniform_sample = torch.rand(prob.size())\n","    uniform_sample = Variable(uniform_sample.type(mdtype))\n","    accept = (prob > uniform_sample).type(mdtype)\n","    z = z.mul(accept.view(-1, 1)) + current_z.mul(1. - accept.view(-1, 1))\n","\n","    accept_hist = accept_hist.add(accept)\n","    criteria = (accept_hist / hist_len > 0.65).type(mdtype)\n","    adapt = 1.02 * criteria + 0.98 * (1. - criteria)\n","    epsilon = epsilon.mul(adapt).clamp(1e-4, .5)\n","\n","    # clear previous history & save memory, similar to detach\n","    z = Variable(z.data, requires_grad=True)\n","    epsilon = Variable(epsilon.data)\n","    accept_hist = Variable(accept_hist.data)\n","\n","    return z, epsilon, accept_hist\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G91--Tr_40GA","colab_type":"text"},"source":["#ais file"]},{"cell_type":"code","metadata":{"id":"10yCxvsCJ1Qw","colab_type":"code","colab":{}},"source":["from torch.autograd import Variable\n","from torch.autograd import grad as torchgrad\n","\n","\n","def ais_trajectory(\n","    model,\n","    loader,\n","    mode='forward',\n","    schedule=np.linspace(0., 1., 500),\n","    n_sample=100,\n","    log_likelihood_fn = log_bernoulli #[1040336] Enabled using arbitrary log-likelihood functions in order to use continous bernoulli\n","):\n","    \"\"\"Compute annealed importance sampling trajectories for a batch of data. \n","    Could be used for *both* forward and reverse chain in bidirectional Monte Carlo\n","    (default: forward chain with linear schedule).\n","\n","    Args:\n","        model (vae.VAE): VAE model\n","        loader (iterator): iterator that returns pairs, with first component being `x`,\n","            second would be `z` or label (will not be used)\n","        mode (string): indicate forward/backward chain; must be either `forward` or \n","            'backward' schedule (list or 1D np.ndarray): temperature schedule,\n","            i.e. `p(z)p(x|z)^t`; foward chain has increasing values, whereas\n","            backward has decreasing values\n","        n_sample (int): number of importance samples (i.e. number of parallel chains \n","            for each datapoint)\n","\n","    Returns:\n","        A list where each element is a torch.autograd.Variable that contains the \n","        log importance weights for a single batch of data\n","    \"\"\"\n","\n","    assert mode == 'forward' or mode == 'backward', 'Should have forward/backward mode'\n","\n","    def log_f_i(z, data, t, log_likelihood_fn=log_likelihood_fn):\n","        \"\"\"Unnormalized density for intermediate distribution `f_i`:\n","            f_i = p(z)^(1-t) p(x,z)^(t) = p(z) p(x|z)^t\n","        =>  log f_i = log p(z) + t * log p(x|z)\n","        \"\"\"\n","        zeros = Variable(torch.zeros(B, z_size).type(mdtype))\n","        log_prior = log_normal(z, zeros, zeros)\n","        log_likelihood = log_likelihood_fn(model.decode(z), data)\n","\n","        return log_prior + log_likelihood.mul_(t)\n","\n","    model.eval()\n","\n","    # shorter aliases\n","    z_size = model.z_size\n","    mdtype = model.dtype\n","\n","    _time = time.time()\n","    logws = []  # for output\n","\n","    print ('In %s mode' % mode)\n","\n","    for i, (batch, post_z) in enumerate(loader):\n","\n","        B = batch.size(0) * n_sample\n","        batch = Variable(batch.type(mdtype))\n","        batch = safe_repeat(batch, n_sample)\n","\n","        # batch of step sizes, one for each chain\n","        epsilon = Variable(torch.ones(B).type(model.dtype)).mul_(0.01)\n","        # accept/reject history for tuning step size\n","        accept_hist = Variable(torch.zeros(B).type(model.dtype))\n","        # record log importance weight; volatile=True reduces memory greatly\n","        logw = Variable(torch.zeros(B).type(mdtype), volatile=True)\n","\n","        # initial sample of z\n","        if mode == 'forward':\n","            current_z = Variable(torch.randn(B, z_size).type(mdtype), requires_grad=True)\n","        else:\n","            current_z = Variable(safe_repeat(post_z, n_sample).type(mdtype), requires_grad=True)\n","\n","        for j, (t0, t1) in tqdm(enumerate(zip(schedule[:-1], schedule[1:]), 1), position=0, leave=True):\n","            # update log importance weight\n","            log_int_1 = log_f_i(current_z, batch, t0)\n","            log_int_2 = log_f_i(current_z, batch, t1)\n","            logw.add_(log_int_2 - log_int_1)\n","\n","            # resample speed\n","            current_v = Variable(torch.randn(current_z.size()).type(mdtype))\n","\n","            def U(z):\n","                return -log_f_i(z, batch, t1)\n","\n","            def grad_U(z):\n","                # grad w.r.t. outputs; mandatory in this case\n","                grad_outputs = torch.ones(B).type(mdtype)\n","                # torch.autograd.grad default returns volatile\n","                grad = torchgrad(U(z), z, grad_outputs=grad_outputs)[0]\n","                # avoid humongous gradients\n","                grad = torch.clamp(grad, -10000, 10000)\n","                # needs variable wrapper to make differentiable\n","                grad = Variable(grad.data, requires_grad=True)\n","                return grad\n","\n","            def normalized_kinetic(v):\n","                zeros = Variable(torch.zeros(B, z_size).type(mdtype))\n","                # this is superior to the unnormalized version\n","                return -log_normal(v, zeros, zeros)\n","\n","            z, v = hmc_trajectory(current_z, current_v, U, grad_U, epsilon)\n","\n","            # accept-reject step\n","            current_z, epsilon, accept_hist = accept_reject(\n","                current_z, current_v,\n","                z, v,\n","                epsilon,\n","                accept_hist, j,\n","                U, K=normalized_kinetic\n","            )\n","\n","        # IWAE lower bound\n","        logw = log_mean_exp(logw.view(n_sample, -1).transpose(0, 1))\n","        if mode == 'backward':\n","            logw = -logw\n","        logws.append(logw.data)\n","\n","        print ('Time elapse %.4f, last batch stats %.4f' % \\\n","            (time.time()-_time, logw.mean().cpu().data.numpy()))\n","\n","        _time = time.time()\n","        sys.stdout.flush()  # for debugging\n","\n","    return logws"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-hyYm-w5M_N","colab_type":"text"},"source":["#approx_posts file"]},{"cell_type":"code","metadata":{"id":"4fRZ54OlKnd5","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import torch\n","import torch.utils.data\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","class Flow(nn.Module):\n","    \"\"\"A combination of R-NVP and auxiliary variables.\"\"\"\n","\n","    def __init__(self, model, n_flows=2):\n","        super(Flow, self).__init__()\n","        self.z_size = model.z_size\n","        self.n_flows = n_flows\n","        self._construct_weights()\n","\n","    def forward(self, z, grad_fn=lambda x: 1, x_info=None):\n","        return self._sample(z, grad_fn, x_info)\n","\n","    def _norm_flow(self, params, z, v, grad_fn, x_info):\n","        h = F.elu(params[0][0](torch.cat((z, x_info), dim=1)))\n","        mu = params[0][1](h)\n","        logit = params[0][2](h)\n","        sig = F.sigmoid(logit)\n","\n","        # old CIFAR used the one below\n","        # v = v * sig + mu * grad_fn(z)\n","\n","        # the more efficient one uses the one below\n","        v = v * sig - F.elu(mu) * grad_fn(z)\n","        logdet_v = torch.sum(logit - F.softplus(logit), 1)\n","\n","        h = F.elu(params[1][0](torch.cat((v, x_info), dim=1)))\n","        mu = params[1][1](h)\n","        logit = params[1][2](h)\n","        sig = F.sigmoid(logit)\n","\n","        z = z * sig + mu\n","        logdet_z = torch.sum(logit - F.softplus(logit), 1)\n","        logdet = logdet_v + logdet_z\n","\n","        return z, v, logdet\n","\n","    def _sample(self, z0, grad_fn, x_info, k=1): #[1040336] added the ability to get k samples with the same mean and var\n","        x_info = x_info.repeat(k,1)\n","        B = z0.size(0)\n","        z_size = self.z_size\n","        act_func = F.elu\n","        qv_weights, rv_weights, params = self.qv_weights, self.rv_weights, self.params\n","\n","        out = torch.cat((z0, x_info), dim=1)\n","        for i in range(len(qv_weights)-1):\n","            out = act_func(qv_weights[i](out))\n","        out = qv_weights[-1](out)\n","        mean_v0, logvar_v0 = out[:, :z_size], out[:, z_size:]\n","\n","        eps = Variable(torch.randn(B, z_size).type( type(out.data) ))\n","        v0 = eps.mul(logvar_v0.mul(0.5).exp_()) + mean_v0\n","        logqv0 = log_normal(v0, mean_v0, logvar_v0)\n","\n","        zT, vT = z0, v0\n","        logdetsum = 0.\n","        for i in range(self.n_flows):\n","            zT, vT, logdet = self._norm_flow(params[i], zT, vT, grad_fn, x_info)\n","            logdetsum += logdet\n","\n","        # reverse model, r(vT|x,zT)\n","        out = torch.cat((zT, x_info), dim=1)\n","        for i in range(len(rv_weights)-1):\n","            out = act_func(rv_weights[i](out))\n","        out = rv_weights[-1](out)\n","        mean_vT, logvar_vT = out[:, :z_size], out[:, z_size:]\n","        logrvT = log_normal(vT, mean_vT, logvar_vT)\n","\n","        assert logqv0.size() == (B,)\n","        assert logdetsum.size() == (B,)\n","        assert logrvT.size() == (B,)\n","\n","        logprob = logqv0 - logdetsum - logrvT\n","\n","        return zT, logprob\n","\n","    def _construct_weights(self):\n","        z_size = self.z_size\n","        n_flows = self.n_flows\n","        h_s = 200\n","\n","        qv_arch = rv_arch = [z_size*2, h_s, h_s, z_size*2]\n","        qv_weights, rv_weights = [], []\n","\n","        # q(v|x,z)\n","        id = 0\n","        for ins, outs in zip(qv_arch[:-1], qv_arch[1:]):\n","            cur_layer = nn.Linear(ins, outs)\n","            qv_weights.append(cur_layer)\n","            self.add_module('qz%d' % id, cur_layer)\n","            id += 1\n","\n","        # r(v|x,z)\n","        id = 0\n","        for ins, outs in zip(rv_arch[:-1], rv_arch[1:]):\n","            cur_layer = nn.Linear(ins, outs)\n","            rv_weights.append(cur_layer)\n","            self.add_module('rv%d' % id, cur_layer)\n","            id += 1\n","\n","        # nf\n","        params = []\n","        for i in range(n_flows):\n","            layer_grid = [\n","                [nn.Linear(z_size*2, h_s),\n","                 nn.Linear(h_s, z_size),\n","                 nn.Linear(h_s, z_size)],\n","                [nn.Linear(z_size*2, h_s),\n","                 nn.Linear(h_s, z_size),\n","                 nn.Linear(h_s, z_size)],\n","            ]\n","\n","            params.append(layer_grid)\n","\n","            id = 0\n","            for layer_list in layer_grid:\n","                for layer in layer_list:\n","                    self.add_module('flow%d_layer%d' % (i, id), layer)\n","                    id += 1\n","\n","        self.qv_weights = qv_weights\n","        self.rv_weights = rv_weights\n","        self.params = params\n","\n","        self.sanity_check_param = self.params[0][0][0]._parameters['weight']\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZEnSgbr88Xg","colab_type":"text"},"source":["#VAE and CVAE files"]},{"cell_type":"code","metadata":{"id":"99sKm0DxKgfx","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import time\n","import sys\n","import argparse\n","\n","import torch\n","import torch.utils.data\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils import weight_norm\n","from torch.autograd import Variable\n","from torch.autograd import grad as torchgrad\n","\n","\n","class VAE(nn.Module):\n","    \"\"\"Generic VAE for MNIST and Fashion datasets.\"\"\"\n","    def __init__(self, hps):\n","        super(VAE, self).__init__()\n","\n","        self.z_size = hps.z_size\n","        self.has_flow = hps.has_flow\n","        self.use_cuda = hps.cuda\n","        self.act_func = hps.act_func\n","        self.n_flows = hps.n_flows\n","        self.hamiltonian_flow = hps.hamiltonian_flow\n","        self.decode_dist = hps.decode_dist #[1040336] added the ability to use arbitrary log-likelihoods, in order to use continuous bernoulli\n","\n","        self._init_layers(wide_encoder=hps.wide_encoder)\n","\n","        if self.use_cuda:\n","            self.cuda()\n","            self.dtype = torch.cuda.FloatTensor\n","            torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","    def _init_layers(self, wide_encoder=False):\n","        h_s = 500 if wide_encoder else 200\n","\n","        self.fc1 = nn.Linear(784, h_s)  # assume flattened\n","        self.fc2 = nn.Linear(h_s, h_s)\n","        self.fc3 = nn.Linear(h_s, self.z_size*2)\n","\n","        self.fc4 = nn.Linear(self.z_size, 200)\n","        self.fc5 = nn.Linear(200, 200)\n","        self.fc6 = nn.Linear(200, 784)\n","\n","        self.x_info_layer = nn.Linear(200, self.z_size)\n","\n","        if self.has_flow:\n","            self.q_dist = Flow(self, n_flows=self.n_flows)\n","            if self.use_cuda:\n","                self.q_dist.cuda()\n","\n","    def sample(self, mu, logvar, grad_fn=lambda x: 1, x_info=None, k=1): #[1040336] added the ability to get k samples with the same mean and var\n","        if k>1:\n","          eps = Variable(torch.FloatTensor(k,mu.size()[0],mu.size()[1]).normal_().type(self.dtype))\n","          z = eps.mul(logvar.mul(0.5).exp_()).add_(mu)\n","          logqz = log_normal(z, mu, logvar,repeat = (k>1))\n","        else:\n","          eps = Variable(torch.FloatTensor(mu.size()).normal_().type(self.dtype))\n","          z = eps.mul(logvar.mul(0.5).exp_()).add_(mu)\n","          logqz = log_normal(z, mu, logvar)\n","          \n","        \n","          \n","\n","        if self.has_flow:\n","            z, logprob = self.q_dist.forward(z, grad_fn, x_info)\n","            logqz += logprob\n","\n","        zeros = Variable(torch.zeros(mu.size()).type(self.dtype))\n","        logpz = log_normal(z, zeros, zeros,repeat = (k>1))\n","\n","        return z, logpz, logqz\n","\n","    def encode(self, net):\n","        net = self.act_func(self.fc1(net))\n","        net = self.act_func(self.fc2(net))\n","        x_info = self.act_func(self.x_info_layer(net))\n","        net = self.fc3(net)\n","\n","        mean, logvar = net[:, :self.z_size], net[:, self.z_size:]\n","\n","        return mean, logvar, x_info\n","\n","    def decode(self, net):\n","        net = self.act_func(self.fc4(net))\n","        net = self.act_func(self.fc5(net))\n","        logit = self.fc6(net)\n","\n","        return logit\n","\n","    def forward(self, x, k=1, warmup_const=1.): #[1040336] Modified to save memory by not copying the datameans and variance k times but rather generating k samples from them later.  \n","        mu, logvar, x_info = self.encode(x)\n","        # posterior-aware inference\n","        def U(z):\n","            logpx = self.decoude_dist(self.decode(z), x)\n","            logpz = log_normal(z)\n","            return -logpx - logpz  # energy as -log p(x, z)\n","\n","        def grad_U(z):\n","            grad_outputs = torch.ones(z.size(0)).type(self.dtype)\n","            grad = torchgrad(U(z), z, grad_outputs=grad_outputs, create_graph=True)[0]\n","            # gradient clipping avoid numerical issue\n","            norm = torch.sqrt(torch.norm(grad, p=2, dim=1))\n","            # neither grad clip methods consistently outperforms the other\n","            grad = grad / norm.view(-1, 1)\n","            # grad = torch.clamp(grad, -10000, 10000)\n","            return grad.detach()\n","\n","        if self.hamiltonian_flow:\n","            z, logpz, logqz = self.sample(mu, logvar, grad_fn=grad_U, x_info=x_info, k=k)\n","        else:\n","            z, logpz, logqz = self.sample(mu, logvar, x_info=x_info,k=k)\n","\n","        logit = self.decode(z)\n","        logpx = self.decode_dist(logit, x, repeat = (k>1))\n","        elbo = logpx + logpz - warmup_const * logqz  # custom warmup\n","        # need correction for Tensor.repeat\n","        #elbo = log_mean_exp(elbo.view(k, -1).transpose(0, 1))\n","        #elbo = torch.mean(elbo)\n","        if k>1:\n","            max_ = torch.max(elbo, 0)[0] #[B]\n","            elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n","        elbo = torch.mean(elbo)\n","        logpx = torch.mean(logpx)\n","        logpz = torch.mean(logpz)\n","        logqz = torch.mean(logqz)\n","\n","        return elbo, logpx, logpz, logqz\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vrq4W1tnKtwd","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import time\n","import sys\n","import argparse\n","\n","import torch\n","import torch.utils.data\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils import weight_norm\n","from torch.autograd import Variable\n","from torch.autograd import grad as torchgrad\n","\n","\n","\n","class CVAE(nn.Module):\n","    \"\"\"Convolutional VAE for CIFAR.\"\"\"\n","    def __init__(self, hps):\n","        super(CVAE, self).__init__()\n","\n","        self.z_size = hps.z_size\n","        self.has_flow = hps.has_flow\n","        self.hamiltonian_flow = hps.hamiltonian_flow\n","        self.n_flows = hps.n_flows\n","        self.use_cuda = hps.cuda\n","        self.act_func = hps.act_func\n","\n","        self._init_layers(wide_encoder=hps.wide_encoder)\n","\n","        if self.use_cuda:\n","            self.cuda()\n","            self.dtype = torch.cuda.FloatTensor\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","    def _init_layers(self, wide_encoder=False):\n","\n","        if wide_encoder:\n","            init_channel = 128\n","        else:\n","            init_channel = 64\n","\n","        # encoder\n","        self.conv1 = nn.Conv2d(3, init_channel, 4, 2)\n","        self.conv2 = nn.Conv2d(init_channel, init_channel*2, 4, 2)\n","        self.conv3 = nn.Conv2d(init_channel*2, init_channel*4, 4, 2)\n","        self.fc_enc = nn.Linear(init_channel*4*2*2, self.z_size*2)\n","\n","        self.bn_enc1 = nn.BatchNorm2d(init_channel)\n","        self.bn_enc2 = nn.BatchNorm2d(init_channel*2)\n","        self.bn_enc3 = nn.BatchNorm2d(init_channel*4)\n","\n","        self.x_info_layer = nn.Linear(init_channel*4*2*2, self.z_size)\n","\n","        # decoder\n","        self.fc_dec = nn.Linear(self.z_size, 256*2*2)\n","        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, 2)\n","        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, 2, output_padding=1)\n","        self.deconv3 = nn.ConvTranspose2d(64, 3, 4, 2)\n","\n","        self.bn_dec1 = nn.BatchNorm2d(128)\n","        self.bn_dec2 = nn.BatchNorm2d(64)\n","\n","        self.decoder_layers = []\n","        self.decoder_layers.append(self.deconv1)\n","        self.decoder_layers.append(self.deconv2)\n","        self.decoder_layers.append(self.deconv3)\n","        self.decoder_layers.append(self.fc_dec)\n","        self.decoder_layers.append(self.bn_dec1)\n","        self.decoder_layers.append(self.bn_dec2)\n","\n","        if self.has_flow:\n","            self.q_dist = Flow(self, n_flows=self.n_flows)\n","            if self.use_cuda:\n","                self.q_dist.cuda()\n","\n","    def encode(self, net):\n","\n","        net = self.act_func(self.bn_enc1(self.conv1(net)))\n","        net = self.act_func(self.bn_enc2(self.conv2(net)))\n","        net = self.act_func(self.bn_enc3(self.conv3(net)))\n","        net = net.view(net.size(0), -1)\n","        x_info = self.act_func(self.x_info_layer(net))\n","        net = self.fc_enc(net)\n","        mean, logvar = net[:, :self.z_size], net[:, self.z_size:]\n","\n","        return mean, logvar, x_info\n","\n","    def decode(self, net):\n","\n","        net = self.act_func(self.fc_dec(net))\n","        net = net.view(net.size(0), -1, 2, 2)\n","        net = self.act_func(self.bn_dec1(self.deconv1(net)))\n","        net = self.act_func(self.bn_dec2(self.deconv2(net)))\n","        logit = self.deconv3(net)\n","\n","        return logit\n","\n","    def sample(self, mu, logvar, grad_fn=lambda x: 1, x_info=None):\n","        # grad_fn default is identity, i.e. don't use grad info\n","        eps = Variable(torch.randn(mu.size()).type(self.dtype))\n","        z = eps.mul(logvar.mul(0.5).exp()).add(mu)\n","        logqz = log_normal(z, mu, logvar)\n","\n","        if self.has_flow:\n","            z, logprob = self.q_dist.forward(z, grad_fn, x_info)\n","            logqz += logprob\n","\n","        zeros = Variable(torch.zeros(z.size()).type(self.dtype))\n","        logpz = log_normal(z, zeros, zeros)\n","\n","        return z, logpz, logqz\n","\n","    def forward(self, x, k=1, warmup_const=1.):\n","\n","        x = x.repeat(k, 1, 1, 1)  # for computing iwae bound\n","        mu, logvar, x_info = self.encode(x)\n","\n","        # posterior-aware inference\n","        def U(z):\n","            logpx = self.decode_dist(self.decode(z), x)\n","            logpz = log_normal(z)\n","            return -logpx - logpz  # energy as -log p(x, z)\n","\n","        def grad_U(z):\n","            grad_outputs = torch.ones(z.size(0)).type(self.dtype)\n","            grad = torchgrad(U(z), z, grad_outputs=grad_outputs, create_graph=True)[0]\n","            # gradient clipping by norm avoid numerical issue\n","            norm = torch.sqrt(torch.norm(grad, p=2, dim=1))\n","            grad = grad / norm.view(-1, 1)\n","            return grad.detach()\n","\n","        if self.hamiltonian_flow:\n","            z, logpz, logqz = self.sample(mu, logvar, grad_fn=grad_U, x_info=x_info)\n","        else:\n","            z, logpz, logqz = self.sample(mu, logvar, x_info=x_info)\n","\n","        logit = self.decode(z)\n","        logpx = self.decode_dist(logit, x)\n","        elbo = logpx + logpz - warmup_const * logqz  # custom warmup\n","        # correction for Tensor.repeat\n","        elbo = log_mean_exp(elbo.view(k, -1).transpose(0, 1))\n","        elbo = torch.mean(elbo)\n","\n","        logpx = torch.mean(logpx)\n","        logpz = torch.mean(logpz)\n","        logqz = torch.mean(logqz)\n","\n","        return elbo, logpx, logpz, logqz\n","\n","    def reconstruct_img(self, x):\n","\n","        # for visualization\n","        mu, logvar, x_info = self.encode(x)\n","        z, logpz, logqz = self.sample(mu, logvar)\n","        logit = self.decode(z)\n","        x_hat = torch.sigmoid(logit)\n","\n","        return x_hat\n","\n","    def freeze_decoder(self):\n","        # freeze so that decoder is not optimized\n","        for layer in self.decoder_layers:\n","            for param_name in layer._parameters:\n","                layer._parameters[param_name].requires_grad = False\n","\n","    def unfreeze_decoder(self):\n","        # unfreeze so that decoder is optimized\n","        for layer in self.decoder_layers:\n","            for param_name in layer._parameters:\n","                layer._parameters[param_name].requires_grad = True\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TM2LDLHs60id","colab_type":"text"},"source":["# Utils files"]},{"cell_type":"code","metadata":{"id":"8M0vtuVS660U","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os\n","import gzip\n","import numpy as np\n","\n","\n","def load_mnist(path, kind='train'):\n","    \"\"\"Load MNIST data from `path`\"\"\"\n","\n","    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n","    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n","\n","    with gzip.open(labels_path, 'rb') as lbpath:\n","        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n","\n","    with gzip.open(images_path, 'rb') as imgpath:\n","        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n","                               offset=16).reshape(len(labels), 784)\n","\n","    return images, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziM6TMBK9E2e","colab_type":"text"},"source":["#loader file"]},{"cell_type":"code","metadata":{"id":"ODITnS6339B_","colab_type":"code","colab":{}},"source":[" # Function to binarise the arrays in place\n","def static_binarise(d):\n","  ids = d < 0.5\n","  d[ids] = 0.\n","  d[~ids] = 1.\n","  return(d)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0dGRwYkK6XA","colab_type":"code","colab":{}},"source":["import numpy as np\n","from scipy import io\n","import sys\n","import os\n","import time\n","\n","import torch\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.autograd import Variable\n","import collections\n","import pickle\n","from torch.autograd import Variable\n","\n","\n","class CIFAR10:\n","\n","    def __init__(self,\n","                 part='train',\n","                 batch_size=128,\n","                 partial=None,\n","                 binarize=True,\n","                 valid_size=0.1,\n","                 num_workers=4,\n","                 pin_memory=False):\n","\n","        transform_list = [transforms.ToTensor()]\n","        if binarize:\n","            transform_list.append(lambda x: x >= 0.5)\n","            transform_list.append(lambda x: x.float())\n","\n","        data_transform = transforms.Compose(transform_list)\n","        train_set = datasets.CIFAR10('./datasets', train=True, download=True, transform=data_transform)\n","        valid_set = datasets.CIFAR10('./datasets', train=True, download=True, transform=data_transform)\n","        test_set  = datasets.CIFAR10('./datasets', train=False, download=True, transform=data_transform)\n","\n","        num_train = len(train_set)\n","        indices = list(range(num_train))\n","        split = int(np.floor(valid_size * num_train))\n","        train_idx, valid_idx = indices[split:], indices[:split]\n","\n","        self.loader = {\n","            'train': DataLoader(train_set,\n","                        batch_size=batch_size, sampler=SubsetRandomSampler(train_idx),\n","                        num_workers=num_workers, pin_memory=pin_memory, shuffle=False),\n","            'valid': DataLoader(valid_set,\n","                        batch_size=batch_size, sampler=SubsetRandomSampler(valid_idx),\n","                        num_workers=num_workers, pin_memory=pin_memory, shuffle=False),\n","            'test':  DataLoader(test_set, batch_size=batch_size,\n","                        num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n","        }[part]\n","\n","        self.size = len(self.loader) if partial is None else partial // batch_size\n","        self._iter = iter(self.loader)\n","        self.batch_size = batch_size\n","        self.p = 0\n","\n","    def __iter__(self):\n","        self.p = 0\n","        self._iter = iter(self.loader)\n","        return self\n","\n","    def __next__(self):\n","        self.p += 1\n","        if self.p > self.size:\n","            raise StopIteration\n","        return next(self._iter)\n","\n","    # due to inconsistency between py2 and py3\n","    def next(self):\n","        return self.__next__()\n","\n","# This is a binarized version of MNIST\n","class Larochelle_MNIST:\n","\n","    def __init__(self, part='train', batch_size=128, partial=1000):\n","        with open('datasets/mnist_non_binarised.pkl', 'rb') as f:\n","            # This is checking if you are using a version of Python < 3.0\n","            if sys.version_info[0] < 3:\n","                mnist = pickle.load(f)\n","            else:\n","                mnist = pickle.load(f, encoding='latin1')\n","            train = np.concatenate((mnist[0][0], mnist[1][0]))\n","            # clunky but this is how we'll turn on the binarising functionality for the moment\n","            # just change the below bool to true if you want to binarise the mnist file\n","            binarise = False\n","            if binarise:\n","              self.data = {\n","                  'train': static_binarise(train),\n","                  'test': static_binarise(mnist[2][0]),\n","                  'partial_train': static_binarise(mnist[0][0][:partial]),\n","                  'partial_test': static_binarise(mnist[2][0][:partial]),\n","              }[part]\n","            else:\n","              self.data = {\n","                  'train': train,\n","                  'test': mnist[2][0],\n","                  'partial_train': mnist[0][0][:partial],\n","                  'partial_test': mnist[2][0][:partial],\n","              }[part]\n","        self.size = self.data.shape[0]\n","        self.batch_size = batch_size\n","        self._construct()\n","\n","    def __iter__(self):\n","        return iter(self.batch_list)\n","\n","    def _construct(self):\n","        self.batch_list = []\n","        for i in range(self.size // self.batch_size):\n","            batch = self.data[self.batch_size*i:self.batch_size*(i+1)]\n","            batch = torch.from_numpy(batch)\n","            # placeholder for second entry\n","            self.batch_list.append((batch, None))\n","\n","\n","class Binarized_Omniglot:\n","\n","    def __init__(self, part='train', batch_size=128, partial=1000):\n","        omni_raw = io.loadmat('datasets/chardata.mat')\n","        reshape_data = lambda d: d.reshape(\n","            (-1, 28, 28)).reshape((-1, 28*28), order='fortran')\n","\n","        def static_binarize(d):\n","            # The mask is applied element-wise to the tensor\n","            ids = d < 0.5\n","            d[ids] = 0.\n","            d[~ids] = 1.\n","\n","        train_data = reshape_data(omni_raw['data'].T.astype('float32'))\n","        test_data = reshape_data(omni_raw['testdata'].T.astype('float32'))\n","        static_binarize(train_data)\n","        static_binarize(test_data)\n","\n","        assert train_data.shape == (24345, 784)\n","        assert test_data.shape == (8070, 784)\n","\n","        self.data = {\n","            'train': train_data,\n","            'test':  test_data,\n","            'partial_train': train_data[:partial],\n","            'partial_test': test_data[:partial],\n","        }[part]\n","        self.size = self.data.shape[0]\n","        self.batch_size = batch_size\n","        self._construct()\n","\n","    def __iter__(self):\n","        return iter(self.batch_list)\n","\n","    def _construct(self):\n","        self.batch_list = []\n","        for i in range(self.size // self.batch_size):\n","            batch = self.data[self.batch_size*i:self.batch_size*(i+1)]\n","            batch = torch.from_numpy(batch)\n","            self.batch_list.append((batch, None))\n","\n","\n","class Binarized_Fashion:\n","\n","    def __init__(self, part='train', batch_size=128, partial=1000):\n","\n","        # I copied load_mnist in the colab, we don't need the line below anymore\n","        # from utils.mnist_reader import load_mnist\n","        train_raw, _ = load_mnist('datasets/fashion', kind='train')\n","        test_raw, _ = load_mnist('datasets/fashion', kind='t10k')\n","\n","        grey_scale = lambda x: np.float32(x / 255.)\n","\n","        def static_binarize(d):\n","            ids = d < 0.5\n","            d[ids] = 0.\n","            d[~ids] = 1.\n","\n","        train_data = grey_scale(train_raw)\n","        test_data = grey_scale(test_raw)\n","\n","        static_binarize(train_data)\n","        static_binarize(test_data)\n","\n","        assert train_data.shape == (60000, 784)\n","        assert test_data.shape == (10000, 784)\n","\n","        self.data = {\n","            'train': train_data[:55000],\n","            'valid': train_data[55000:],\n","            'test':  test_data,\n","            'partial_train': train_data[:partial],\n","            'partial_test': test_data[:partial],\n","        }[part]\n","        self.size = self.data.shape[0]\n","        self.batch_size = batch_size\n","        self._construct()\n","\n","    def __iter__(self):\n","        return iter(self.batch_list)\n","\n","    def _construct(self):\n","        self.batch_list = []\n","        for i in range(self.size // self.batch_size):\n","            batch = self.data[self.batch_size*i:self.batch_size*(i+1)]\n","            batch = torch.from_numpy(batch)\n","            self.batch_list.append((batch, None))\n","\n","\n","def get_default_mnist_loader():\n","\n","    kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n","    train_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('./datasets', train=True, download=True,\n","                       transform=transforms.ToTensor()),\n","        batch_size=128, shuffle=True, **kwargs)\n","\n","    test_loader = torch.utils.data.DataLoader(\n","        datasets.MNIST('./datasets', train=False,\n","                       transform=transforms.ToTensor()),\n","        batch_size=100, shuffle=True, **kwargs)\n","\n","    return train_loader, test_loader\n","\n","\n","def get_cifar10_loader(batch_size=100, partial=False, num=1000):\n","\n","    if partial:\n","        train_loader = CIFAR10(part='train', batch_size=batch_size, partial=num)\n","        test_loader  = CIFAR10(part='test', batch_size=4)\n","    else:\n","        train_loader = CIFAR10(part='train', batch_size=batch_size)\n","        test_loader = CIFAR10(part='valid', batch_size=4)  # really validation set\n","\n","    return train_loader, test_loader\n","\n","\n","def get_Larochelle_MNIST_loader(batch_size=100, partial=False, num=1000):\n","\n","    if partial:\n","        train_loader = Larochelle_MNIST(part='partial_train', batch_size=batch_size, partial=num)\n","        test_loader = Larochelle_MNIST(part='partial_test')\n","    else:\n","        train_loader = Larochelle_MNIST(part='train', batch_size=batch_size)\n","        test_loader = Larochelle_MNIST(part='test', batch_size=batch_size)\n","    \n","    return train_loader, test_loader\n","\n","\n","def get_omniglot_loader(batch_size=100, partial=False, num=1000):\n","\n","    if partial:\n","        train_loader = Binarized_Omniglot(part='partial_train', batch_size=batch_size, partial=num)\n","        test_loader = Binarized_Omniglot(part='partial_test')\n","    else:\n","        train_loader = Binarized_Omniglot(part='train', batch_size=batch_size)\n","        test_loader = Binarized_Omniglot(part='valid', batch_size=10)\n","\n","    return train_loader, test_loader\n","\n","\n","def get_fashion_loader(batch_size=100, partial=False, num=1000):\n","\n","    if partial:\n","        train_loader = Binarized_Fashion(part='partial_train', batch_size=batch_size, partial=num)\n","        test_loader = Binarized_Fashion(part='partial_test', batch_size=10)\n","    else:\n","        train_loader = Binarized_Fashion(part='train', batch_size=batch_size)\n","        test_loader = Binarized_Fashion(part='valid', batch_size=10)\n","    \n","    return train_loader, test_loader\n","\n","\n","# if __name__ == '__main__':\n","#     # sanity checking\n","#     train_loader, test_loader = get_cifar10_loader()\n","#     train_loader, test_loader = get_default_mnist_loader()\n","#     for i, (batch, _) in enumerate(train_loader):\n","#         batch = Variable(batch)\n","#         print (i)\n","\n","#     for i, (batch, _) in enumerate(train_loader):\n","#         batch = Variable(batch)\n","#         print (i)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2qfoJcUN9Omw","colab_type":"text"},"source":["#helper file"]},{"cell_type":"code","metadata":{"id":"9n1Gd9nkKSR2","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","\n","def get_loaders(dataset='mnist', evaluate=False, batch_size=100):\n","    if dataset == 'mnist':\n","        train_loader, test_loader = get_Larochelle_MNIST_loader(\n","            batch_size=batch_size,\n","            partial=evaluate, num=1000\n","        )\n","    elif dataset == 'fashion':\n","        train_loader, test_loader = get_fashion_loader(\n","            batch_size=batch_size,\n","            partial=evaluate, num=1000\n","        )\n","    elif dataset == 'cifar':\n","        train_loader, test_loader = get_cifar10_loader(\n","            batch_size=batch_size,\n","            partial=evaluate, num=100\n","        )\n","\n","    return train_loader, test_loader\n","\n","\n","def get_model(dataset, hps):\n","    if dataset == 'mnist' or dataset == 'fashion':\n","        model = VAE(hps)\n","    elif dataset == 'cifar':  # convolutional VAE for CIFAR\n","        model = CVAE(hps)\n","\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jgjku6uLsSw9","colab_type":"text"},"source":["## local_ffg.py"]},{"cell_type":"code","metadata":{"id":"PyH_h8lOsYYG","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import time\n","import sys\n","from tqdm import tqdm\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.utils.data\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","parser_ffg = argparse.ArgumentParser(description='local_factorized_gaussian')\n","# action configuration flags\n","parser_ffg.add_argument('--no-cuda', '-nc', action='store_true')\n","parser_ffg.add_argument('--debug', action='store_true', help='debug mode')\n","\n","# model configuration flags\n","parser_ffg.add_argument('--cont-bernoulli','-cs',action ='store_true') #[1040336] Added the ability to use arbitray log-likelihoods in order to use continous bernoulli \n","parser_ffg.add_argument('--z-size', '-zs', type=int, default=50)\n","parser_ffg.add_argument('--batch-size', '-bs', type=int, default=100)\n","parser_ffg.add_argument('--eval-path', '-ep', type=str, default='model.pth',\n","                    help='path to load evaluation ckpt (default: model.pth)')\n","parser_ffg.add_argument('--dataset', '-d', type=str, default='mnist',\n","                    choices=['mnist', 'fashion', 'cifar'], \n","                    help='dataset to train and evaluate on (default: mnist)')\n","parser_ffg.add_argument('--has-flow', '-hf', action='store_true', help='inference uses FLOW')\n","parser_ffg.add_argument('--n-flows', '-nf', type=int, default=2, help='number of flows')\n","parser_ffg.add_argument('--wide-encoder', '-we', action='store_true',\n","                    help='use wider layer (more hidden units for FC, more channels for CIFAR)')\n","\n","\n","# args.cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","\n","def get_default_hparams_local_ffg():\n","    return HParams(\n","        z_size=args.z_size,\n","        act_func=F.elu,\n","        has_flow=args.has_flow,\n","        n_flows=args.n_flows,\n","        wide_encoder=args.wide_encoder,\n","        cuda=args.cuda,\n","        hamiltonian_flow=False,\n","        decode_dist = args.decode_dist #[1040336] Added the ability to use arbitray log-likelihoods in order to use continous bernoulli\n","    )\n","\n","\n","def optimize_local_gaussian(\n","    log_likelihood,\n","    model,\n","    data_var,\n","    k=100,\n","    check_every=100,\n","    sentinel_thres=10,\n","    debug=True\n","):\n","    \"\"\"data_var should be (cuda) variable.\"\"\"\n","\n","    B = data_var.size()[0]\n","    z_size = model.z_size\n","\n","    data_var = safe_repeat(data_var, k)\n","    zeros = Variable(torch.zeros(B*k, z_size).type(model.dtype))\n","    mean = Variable(torch.zeros(B*k, z_size).type(model.dtype), requires_grad=True)\n","    logvar = Variable(torch.zeros(B*k, z_size).type(model.dtype), requires_grad=True)\n","\n","    optimizer = optim.Adam([mean, logvar], lr=1e-3)\n","    best_avg, sentinel, prev_seq = 999999, 0, []\n","\n","    # perform local opt\n","    time_ = time.time()\n","    for epoch in range(1, 999999):\n","\n","        eps = Variable(torch.FloatTensor(mean.size()).normal_().type(model.dtype))\n","        z = eps.mul(logvar.mul(0.5).exp_()).add_(mean)\n","        x_logits = model.decode(z)\n","\n","        logpz = log_normal(z, zeros, zeros)\n","        logqz = log_normal(z, mean, logvar)\n","        logpx = log_likelihood(x_logits, data_var)\n","\n","        optimizer.zero_grad()\n","        loss = -torch.mean(logpx + logpz - logqz)\n","        loss_np = loss.data.cpu().numpy()\n","        loss.backward()\n","        optimizer.step()\n","\n","        prev_seq.append(loss_np)\n","        if epoch % check_every == 0:\n","            last_avg = np.mean(prev_seq)\n","            if debug:  # debugging helper\n","                sys.stderr.write(\n","                    'Epoch %d, time elapse %.4f, last avg %.4f, prev best %.4f\\n' % \\\n","                    (epoch, time.time()-time_, -last_avg, -best_avg)\n","                )\n","            if last_avg < best_avg:\n","                sentinel, best_avg = 0, last_avg\n","            else:\n","                sentinel += 1\n","            if sentinel > sentinel_thres:\n","                break\n","            prev_seq = []\n","            time_ = time.time()\n","\n","    # evaluation\n","    eps = Variable(torch.FloatTensor(B*k, z_size).normal_().type(model.dtype))\n","    z = eps.mul(logvar.mul(0.5).exp_()).add_(mean)\n","\n","    logpz = log_normal(z, zeros, zeros)\n","    logqz = log_normal(z, mean, logvar)\n","    logpx = log_likelihood(model.decode(z), data_var)\n","    elbo = logpx + logpz - logqz\n","\n","    vae_elbo = torch.mean(elbo)\n","    iwae_elbo = torch.mean(log_mean_exp(elbo.view(k, -1).transpose(0, 1)))\n","\n","    return vae_elbo.data[0], iwae_elbo.data[0]\n","\n","\n","def main_ffg(arg_string):\n","    global args\n","    args = parser_ffg.parse_args(arg_string.split())   #parser_ffg.parse_args()\n","    args.decode_dist = log_bernoulli\n","    if args.cont_bernoulli:\n","        args.decode_dist = log_bernoulli_cont\n","    args.cuda = not args.no_cuda and torch.cuda.is_available()\n","    print(args.cuda)\n","    train_loader, test_loader = get_loaders(\n","        dataset=args.dataset,\n","        evaluate=True, batch_size=args.batch_size\n","    )\n","    print('Done: train_loader, test_loader')\n","    model = get_model(args.dataset, get_default_hparams_local_ffg())\n","    print('Done: model; get_model')\n","    model.load_state_dict(torch.load(args.eval_path)['state_dict'])\n","    print('Done: model.load_state_dict')\n","    model.eval()\n","    print('Done: model.eval')\n","\n","    vae_record, iwae_record = [], []\n","    time_ = time.time()\n","    print('Done: time_ initalise')\n","    for i, (batch, _) in tqdm(enumerate(train_loader)):\n","        batch = Variable(batch.type(model.dtype))\n","        print('Done: batch')\n","        elbo, iwae = optimize_local_gaussian(self.decode_dist, model, batch, debug=args.debug)\n","        print('Done: elbo, iwae')\n","        vae_record.append(elbo)\n","        print('Done: vae_record append elbo')\n","        iwae_record.append(iwae)\n","        print('Done: iwae_record append iwae')\n","        print ('Local opt w/ ffg, batch %d, time elapse %.4f, ELBO %.4f, IWAE %.4f' % \\\n","            (i+1, time.time()-time_, elbo, iwae))\n","        print ('mean of ELBO so far %.4f, mean of IWAE so far %.4f' % \\\n","            (np.nanmean(vae_record), np.nanmean(iwae_record)))\n","        time_ = time.time()\n","        print('Done: time_ loop')\n","\n","    print ('Finishing...')\n","    print ('Average ELBO %.4f, IWAE %.4f' % (np.nanmean(vae_record), np.nanmean(iwae_record)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W_Jt7T8Gwaz_","colab_type":"text"},"source":["## local_flow.py"]},{"cell_type":"code","metadata":{"id":"XiTT0nsZwf7i","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import time\n","import sys\n","from tqdm import tqdm\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.utils.data\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","#   from loader import get_Larochelle_MNIST_loader, get_fashion_loader, get_cifar10_loader\n","#   from vae import VAE\n","#   from cvae import CVAE\n","\n","\n","parser_flow = argparse.ArgumentParser(description='local_expressive')\n","# action configuration flags\n","parser_flow.add_argument('--no-cuda', '-nc', action='store_true')\n","parser_flow.add_argument('--debug', action='store_true', help='debug mode')\n","\n","# model configuration flags\n","parser_flow.add_argument('--z-size', '-zs', type=int, default=50)\n","parser_flow.add_argument('--batch-size', '-bs', type=int, default=100)\n","parser_flow.add_argument('--eval-path', '-ep', type=str, default='model.pth',\n","                    help='path to load evaluation ckpt (default: model.pth)')\n","parser_flow.add_argument('--dataset', '-d', type=str, default='mnist',\n","                    choices=['mnist', 'fashion', 'cifar'], \n","                    help='dataset to train and evaluate on (default: mnist)')\n","parser_flow.add_argument('--has-flow', '-hf', action='store_true', help='inference uses FLOW')\n","parser_flow.add_argument('--n-flows', '-nf', type=int, default=2, help='number of flows')\n","parser_flow.add_argument('--wide-encoder', '-we', action='store_true',\n","                    help='use wider layer (more hidden units for FC, more channels for CIFAR)')\n","parser_flow.add_argument('--cont-bernoulli','-cs',action ='store_true') #[1040336] Added the ability to use arbitray log-likelihoods in order to use continous bernoulli\n","#   args = parser_flow.parse_args()\n","#   args.cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","\n","def get_default_hparams_local_flow():\n","    return HParams(\n","        z_size=args.z_size,\n","        act_func=F.elu,\n","        has_flow=args.has_flow,\n","        n_flows=args.n_flows,\n","        wide_encoder=args.wide_encoder,\n","        cuda=args.cuda,\n","        hamiltonian_flow=False,\n","        decode_dist=args.decode_dist #[1040336] Added the ability to use arbitray log-likelihoods in order to use continous bernoulli\n","    )\n","\n","\n","def optimize_local_expressive(\n","    log_likelihood,\n","    model,\n","    data_var,\n","    k=100,\n","    check_every=100,\n","    sentinel_thres=10,\n","    n_flows=2,\n","    debug=False\n","):\n","    \"\"\"data_var should be (cuda) variable.\"\"\"\n","\n","    def log_joint(x_logits, x, z):\n","        \"\"\"log p(x,z)\"\"\"\n","        zeros = Variable(torch.zeros(z.size()).type(model.dtype))\n","        logpz = log_normal(z, zeros, zeros)\n","        logpx = log_likelihood(x_logits, x)\n","\n","        return logpx + logpz\n","\n","    def norm_flow(params, z, v):\n","\n","        h = F.tanh(params[0][0](z))\n","        mew_ = params[0][1](h)\n","        logit_ = params[0][2](h)\n","        sig_ = F.sigmoid(logit_)\n","\n","        v = v*sig_ + mew_\n","        # numerically stable: log (sigmoid(logit)) = logit - softplus(logit)\n","        logdet_v = torch.sum(logit_ - F.softplus(logit_), 1)\n","\n","        h = F.tanh(params[1][0](v))\n","        mew_ = params[1][1](h)\n","        logit_ = params[1][2](h)\n","        sig_ = F.sigmoid(logit_)\n","\n","        z = z*sig_ + mew_\n","        logdet_z = torch.sum(logit_ - F.softplus(logit_), 1)\n","\n","        logdet = logdet_v + logdet_z\n","\n","        return z, v, logdet\n","\n","    def sample(mean_v0, logvar_v0):\n","\n","        B = mean_v0.size()[0]\n","        eps = Variable(torch.FloatTensor(B, z_size).normal_().type(model.dtype))\n","        v0 = eps.mul(logvar_v0.mul(0.5).exp_()) + mean_v0\n","        logqv0 = log_normal(v0, mean_v0, logvar_v0)\n","\n","        out = v0\n","        for i in range(len(qz_weights)-1):\n","            out = act_func(qz_weights[i](out))\n","        out = qz_weights[-1](out)\n","        mean_z0, logvar_z0 = out[:, :z_size], out[:, z_size:]\n","\n","        eps = Variable(torch.FloatTensor(B, z_size).normal_().type(model.dtype))\n","        z0 = eps.mul(logvar_z0.mul(0.5).exp_()) + mean_z0\n","        logqz0 = log_normal(z0, mean_z0, logvar_z0)\n","\n","        zT, vT = z0, v0\n","        logdetsum = 0.\n","        for i in range(n_flows):\n","            zT, vT, logdet = norm_flow(params[i], zT, vT)\n","            logdetsum += logdet\n","\n","        # reverse model, r(vT|x,zT)\n","        out = zT\n","        for i in range(len(rv_weights)-1):\n","            out = act_func(rv_weights[i](out))\n","        out = rv_weights[-1](out)\n","        mean_vT, logvar_vT = out[:, :z_size], out[:, z_size:]\n","        logrvT = log_normal(vT, mean_vT, logvar_vT)\n","\n","        logq = logqz0 + logqv0 - logdetsum - logrvT\n","\n","        return zT, logq\n","\n","    def get_params():\n","\n","        all_params = []\n","\n","        mean_v = Variable(torch.zeros(B*k, z_size).type(model.dtype), requires_grad=True)\n","        logvar_v = Variable(torch.zeros(B*k, z_size).type(model.dtype), requires_grad=True)\n","\n","        all_params.append(mean_v)\n","        all_params.append(logvar_v)\n","\n","        qz_weights = []  # q(z|x,v)\n","        for ins, outs in zip(qz_arch[:-1], qz_arch[1:]):\n","            cur_layer = nn.Linear(ins, outs)\n","            if args.cuda:\n","                cur_layer.cuda()\n","            qz_weights.append(cur_layer)\n","            all_params.append(cur_layer.weight)\n","\n","        rv_weights = []  # r(v|x,z)\n","        for ins, outs in zip(rv_arch[:-1], rv_arch[1:]):\n","            cur_layer = nn.Linear(ins, outs)\n","            if args.cuda:\n","                cur_layer.cuda()\n","            rv_weights.append(cur_layer)\n","            all_params.append(cur_layer.weight)\n","\n","        params = []\n","        for i in range(n_flows):\n","            layers = [\n","                [nn.Linear(z_size, h_s),\n","                 nn.Linear(h_s, z_size),\n","                 nn.Linear(h_s, z_size)],\n","                [nn.Linear(z_size, h_s),\n","                 nn.Linear(h_s, z_size),\n","                 nn.Linear(h_s, z_size)],\n","            ]\n","\n","            params.append(layers)\n","\n","            for sublist in layers:\n","                for item in sublist:\n","                    all_params.append(item.weight)\n","                    if args.cuda:\n","                        item.cuda()\n","\n","        return (mean_v, logvar_v), all_params, params, qz_weights, rv_weights\n","\n","    # the real shit\n","    B = data_var.size(0)\n","    z_size = args.z_size\n","    qz_arch = rv_arch = [args.z_size, 200, 200, args.z_size*2]\n","    h_s = 200\n","    act_func = F.elu\n","\n","    data_var = safe_repeat(data_var, k)\n","    (mean_v, logvar_v), all_params, params, qz_weights, rv_weights = get_params()\n","\n","    # tile input for IS\n","    optimizer = optim.Adam(all_params, lr=1e-3)\n","    best_avg, sentinel, prev_seq = 999999, 0, []\n","\n","    # perform local opt\n","    time_ = time.time()\n","    for epoch in range(1, 999999):\n","        z, logqz = sample(mean_v, logvar_v)\n","        x_logits = model.decode(z)\n","        logpxz = log_joint(x_logits, data_var, z)\n","\n","        optimizer.zero_grad()\n","        loss = -torch.mean(logpxz - logqz)\n","        loss_np = loss.data.cpu().numpy()\n","        loss.backward()\n","        optimizer.step()\n","\n","        prev_seq.append(loss_np)\n","        if epoch % check_every == 0:\n","            last_avg = np.mean(prev_seq)\n","            if debug:  # debugging helper\n","                sys.stderr.write(\n","                    'Epoch %d, time elapse %.4f, last avg %.4f, prev best %.4f\\n' % \\\n","                    (epoch, time.time()-time_, -last_avg, -best_avg)\n","                )\n","            if last_avg < best_avg:\n","                sentinel, best_avg = 0, last_avg\n","            else:\n","                sentinel += 1\n","            if sentinel > sentinel_thres:\n","                break\n","\n","            prev_seq = []\n","            time_ = time.time()\n","\n","    # evaluation\n","    z, logqz = sample(mean_v, logvar_v)\n","    x_logits = model.decode(z)\n","    logpxz = log_joint(x_logits, data_var, z)\n","    elbo = logpxz - logqz\n","\n","    vae_elbo = torch.mean(elbo)\n","    iwae_elbo = torch.mean(log_mean_exp(elbo.view(k, -1).transpose(0, 1)))\n","\n","    return vae_elbo.data[0], iwae_elbo.data[0]\n","\n","\n","def main_flow(arg_string):\n","    global args\n","    args = parser_flow.parse_args(arg_string.split())   #parser_flow.parse_args()\n","    args.cuda = not args.no_cuda and torch.cuda.is_available()\n","    args.decode_dist = log_bernoulli\n","    if args.cont_bernoulli:\n","        args.decode_dist = log_bernoulli_cont\n","    print(args.cuda)\n","    train_loader, test_loader = get_loaders(\n","        dataset=args.dataset,\n","        evaluate=True, batch_size=1)\n","    model = get_model(args.dataset, get_default_hparams_local_flow())\n","    model.load_state_dict(torch.load(args.eval_path)['state_dict'])\n","    model.eval()\n","\n","    vae_record, iwae_record = [], []\n","    time_ = time.time()\n","    for i, (batch, _) in tqdm(enumerate(train_loader)):\n","        batch = Variable(batch.type(model.dtype))\n","        elbo, iwae = optimize_local_expressive(\n","            args.decode_dist,\n","            model,\n","            batch,\n","            n_flows=args.n_flows, debug=args.debug\n","        )\n","        vae_record.append(elbo)\n","        iwae_record.append(iwae)\n","        print ('Local opt w/ flow, batch %d, time elapse %.4f, ELBO %.4f, IWAE %.4f' % \\\n","            (i+1, time.time()-time_, elbo, iwae))\n","        print ('mean of ELBO so far %.4f, mean of IWAE so far %.4f' % \\\n","            (np.nanmean(vae_record), np.nanmean(iwae_record)))\n","        time_ = time.time()\n","\n","    print ('Finishing...')\n","    print ('Average ELBO %.4f, IWAE %.4f' % (np.nanmean(vae_record), np.nanmean(iwae_record)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bpaXrj3A3_pH","colab_type":"text"},"source":["#Params File\n"]},{"cell_type":"code","metadata":{"id":"g2biGTJWJZjT","colab_type":"code","colab":{}},"source":["parser = argparse.ArgumentParser(description='VAE')\n","# action configuration flags\n","parser.add_argument('--train', '-t', action='store_true')\n","parser.add_argument('--load-path', '-lp', type=str, default='NA',\n","                    help='path to load checkpoint to retrain')\n","parser.add_argument('--load-epoch', '-le', type=int, default=0,\n","                    help='epoch number to start recording when retraining')\n","parser.add_argument('--display-epoch', '-de', type=int, default=10,\n","                    help='print status every so many epochs (default: 10)')\n","parser.add_argument('--eval-iwae', '-ei', action='store_true')\n","parser.add_argument('--eval-ais', '-ea', action='store_true')\n","parser.add_argument('--n-iwae', '-ni', type=int, default=5000,\n","                    help='number of samples for IWAE evaluation (default: 5000)')\n","parser.add_argument('--n-ais-iwae', '-nai', type=int, default=100,\n","                    help='number of IMPORTANCE samples for AIS evaluation (default: 100). \\\n","                          This is different from MC samples.')\n","parser.add_argument('--n-ais-dist', '-nad', type=int, default=10000,\n","                    help='number of distributions for AIS evaluation (default: 10000)')\n","parser.add_argument('--ais-schedule', type=str, default='linear', help='schedule for AIS')\n","\n","parser.add_argument('--no-cuda', '-nc', action='store_true', help='force not use CUDA')\n","parser.add_argument('--visdom', '-v', action='store_true', help='visualize samples')\n","parser.add_argument('--port', '-p', type=int, default=8097, help='port for visdom')\n","parser.add_argument('--save-visdom', default='test', help='visdom save path')\n","parser.add_argument('--encoder-more', action='store_true', help='train the encoder more (5 vs 1)')\n","parser.add_argument('--early-stopping', '-es', action='store_true', help='apply early stopping')\n","parser.add_argument('--epochs', '-e', type=int, default=3280,\n","                    help='total num of epochs for training (default: 3280)')\n","parser.add_argument('--lr-schedule', '-lrs', action='store_true',\n","                    help='apply learning rate schedule')\n","\n","# model configuration flags\n","parser.add_argument('--z-size', '-zs', type=int, default=50,\n","                    help='dimensionality of latent code (default: 50)')\n","parser.add_argument('--batch-size', '-bs', type=int, default=100,\n","                    help='batch size (default: 100)')\n","parser.add_argument('--save-name', '-sn', type=str, default='model.pth',\n","                    help='name to save trained ckpt (default: model.pth)')\n","parser.add_argument('--eval-path', '-ep', type=str, default='model.pth',\n","                    help='path to load evaluation ckpt (default: model.pth)')\n","parser.add_argument('--dataset', '-d', type=str, default='mnist',\n","                    choices=['mnist', 'fashion', 'cifar'],\n","                    help='dataset to train and evaluate on (default: mnist)')\n","parser.add_argument('--wide-encoder', '-we', action='store_true',\n","                    help='use wider layer (more hidden units for FC, more channels for CIFAR)')\n","parser.add_argument('--has-flow', '-hf', action='store_true',\n","                    help='use flow for training and eval')\n","parser.add_argument('--adadelta', '-ad', action='store_true', #[1040336] Did my own version of changed optimiser due to misscommunication on who does what\n","                    help='use AdaDelta optimizer')\n","parser.add_argument('--sgd', '-sgd', action='store_true')#[1040336] Did my own version of changed optimiser due to misscommunication on who does what\n","parser.add_argument('--hamiltonian-flow', '-hamil-f', action='store_true')\n","parser.add_argument('--n-flows', '-nf', type=int, default=2, help='number of flows')\n","parser.add_argument('--warmup', '-w', action='store_true',\n","                    help='apply warmup during training')\n","parser.add_argument('--cont-bernoulli','-cs',action ='store_true') #[1040336] Added the ability to use arbitray log-likelihoods in order to use continous bernoulli\n","parser.add_argument('--get-samples','-gs',action ='store_true') #[1040336] Added the ability samples images from the model\n","parser.add_argument('--samples-name','-sna',type = str, default = 'Samples')\n","\n","\n","def get_default_hparams():\n","    return HParams(\n","        z_size=args.z_size,\n","        act_func=F.elu,\n","        has_flow=args.has_flow,\n","        hamiltonian_flow=args.hamiltonian_flow,\n","        n_flows=args.n_flows,\n","        wide_encoder=args.wide_encoder,\n","        cuda=args.cuda,\n","        decode_dist=args.decode_dist #[1040336] Added the ability to use arbitray log-likelihoods in order to use continous bernoulli\n","    )\n","\n","\n","def train(\n","    model,\n","    train_loader,\n","    test_loader,\n","    k_train=1,  # num iwae sample for training\n","    k_eval=1,  # num iwae sample for eval\n","    epochs=3280,\n","    display_epoch=10,\n","    adadelta = False,\n","    sgd = False,\n","    lr_schedule=True,\n","    warmup=True,\n","    warmup_thres=None,\n","    encoder_more=False,\n","    checkpoints=None,\n","    early_stopping=True,\n","    save=True,\n","    save_path='checkpoints/',\n","    patience=10  # for early-stopping\n","):\n","    print('Training')\n","\n","    if args.load_path != 'NA':\n","        f = args.load_path\n","        model.load_state_dict(torch.load(f)['state_dict'])\n","\n","    # default warmup schedule\n","    if warmup_thres is None:\n","        if 'cifar' in save_path:\n","            warmup_thres = 50.\n","        elif 'mnist' in save_path or 'fashion' in save_path:\n","            warmup_thres = 400.\n","\n","    if checkpoints is None:  # save a checkpoint every display_epoch\n","        checkpoints = [1] + list(range(0, 3280, display_epoch))[1:] + [3280]\n","\n","    time_ = time.time()\n","\n","    if lr_schedule:\n","        current_lr = 1e-3\n","        pow = 0\n","        epoch_elapsed = 0\n","        # pth default: beta_1 = .9, beta_2 = .999, eps = 1e-8\n","        optimizer = optim.Adam(model.parameters(), lr=current_lr, eps=1e-4) \n","    elif adadelta:\n","        optimizer = optim.Adadelta(model.parameters(), lr = 0.9)\n","    elif sgd:\n","        optimizer = optim.SGD(model.parameters(), lr =4e-3)\n","    else:\n","        optimizer = optim.Adam(model.parameters(), lr=1e-4, eps=1e-4)\n","      \n","\n","    num_worse = 0  # compare against `patience` for early-stopping\n","    prev_valid_err = None\n","\n","    for epoch in tqdm(range(1, epochs+1), position=0, leave=True):\n","        warmup_const = min(1., epoch / warmup_thres) if warmup else 1.\n","        # lr schedule from IWAE: https://arxiv.org/pdf/1509.00519.pdf\n","        if lr_schedule:\n","            if epoch_elapsed >= 3 ** pow:\n","                current_lr *= 10. ** (-1. / 7.)\n","                pow += 1\n","                epoch_elapsed = 0\n","                # correct way to do lr decay; also possible w/ `torch.optim.lr_scheduler`\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = current_lr\n","            epoch_elapsed += 1\n","\n","        above_checkpoint_exists = os.path.exists('%s%d_%s' % (save_path, checkpoints[0], args.save_name))\n","        # print(\"epoch = {}, display_epoch = {}, load_epoch = {}\".format(epoch, display_epoch, args.load_epoch))\n","        # print(int(epoch / display_epoch + display_epoch + args.load_epoch))\n","        \n","        if above_checkpoint_exists:\n","          if epoch == checkpoints[0]:\n","            # load that checkpoint\n","            model.load_state_dict(torch.load('%s%d_%s' % (save_path, checkpoints[0], args.save_name))['state_dict'])\n","        else:\n","          model.train()  # crucial for BN to work properly\n","          for _, (batch, _) in enumerate(train_loader):\n","              batch = Variable(batch)\n","              if args.cuda:\n","                  batch = batch.cuda()\n","\n","              # train the encoder more\n","              if encoder_more:\n","                  model.freeze_decoder()\n","                  for _ in range(10):\n","                      optimizer.zero_grad()\n","                      elbo, _, _, _ = model.forward(batch, k_train, warmup_const)\n","                      loss = -elbo\n","                      loss.backward()\n","                      optimizer.step()\n","                  model.unfreeze_decoder()\n","\n","              optimizer.zero_grad()\n","              elbo, _, _, _ = model.forward(batch, k_train, warmup_const)\n","              loss = -elbo\n","              loss.backward()\n","              optimizer.step()\n","\n","        if epoch % display_epoch == 0:\n","            assert(int(epoch / display_epoch) * 10 == checkpoints[0])\n","            model.eval()  # crucial for BN to work properly\n","\n","            train_logpx, test_logpx = [], []\n","            train_logpz, test_logpz = [], []\n","            train_logqz, test_logqz = [], []\n","            train_stats, test_stats = [], []\n","            for _, (batch, _) in enumerate(train_loader):\n","                batch = Variable(batch)\n","                if args.cuda:\n","                    batch = batch.cuda()\n","                elbo, logpx, logpz, logqz = model(batch, k=1)\n","                train_stats.append(elbo.data[0])\n","                train_logpx.append(logpx.data[0])\n","                train_logpz.append(logpz.data[0])\n","                train_logqz.append(logqz.data[0])\n","\n","            for _, (batch, _) in enumerate(test_loader):\n","                batch = Variable(batch)\n","                if args.cuda:\n","                    batch = batch.cuda()\n","                # early stopping with iwae bound\n","                elbo, logpx, logpz, logqz = model(batch, k=k_eval)\n","                test_stats.append(elbo.data[0])\n","                test_logpx.append(logpx.data[0])\n","                test_logpz.append(logpz.data[0])\n","                test_logqz.append(logqz.data[0])\n","            print (\n","                'Train Epoch: [{}/{}]'.format(epoch, epochs),\n","                'Train set ELBO {:.4f}'.format(np.mean(np.asarray(train_stats))),\n","                'Test/Validation set IWAE {:.4f}'.format(np.mean(np.asarray(test_stats))),\n","                'Time: {:.2f}'.format(time.time()-time_),\n","            )\n","            time_ = time.time()\n","\n","            if early_stopping:\n","                curr_valid_err = np.mean(test_stats)\n","\n","                if prev_valid_err is None:  # don't have history yet\n","                    prev_valid_err = curr_valid_err\n","                elif curr_valid_err >= prev_valid_err:  # performance improved\n","                    prev_valid_err = curr_valid_err\n","                    num_worse = 0\n","                else:\n","                    num_worse += 1\n","\n","                if num_worse >= patience:\n","                    print(\"Stopped early\")\n","                    break\n","\n","        if save and epoch in checkpoints:\n","          assert(epoch + args.load_epoch == checkpoints[0])\n","          if not above_checkpoint_exists:\n","            torch.save({\n","                'epoch': epochs + args.load_epoch,\n","                'state_dict': model.state_dict(),\n","            }, '%s%d_%s' % (save_path, epoch + args.load_epoch, args.save_name))\n","          checkpoints = checkpoints[1:]\n","\n","\n","def test_iwae(  \n","    model,\n","    loader,\n","    k=5000,\n","    f='model.pth',\n","    print_res=True\n","): \n","    print('Testing with %d importance samples' % k)\n","    #model.load_state_dict(torch.load(f)['state_dict'])\n","    #model.eval()\n","    time_ = time.time()\n","    elbos = []\n","    for i, (batch, _) in enumerate(loader):\n","        batch = Variable(batch)\n","        if args.cuda:\n","            batch = batch.cuda()\n","        elbo, logpx, logpz, logqz = model(batch, k=k)\n","        elbos.append(elbo.data[0])\n","\n","    mean_ = np.mean(elbos)\n","    if print_res:\n","        print(mean_, 'T:', time.time()-time_)\n","    return mean_\n","\n","\n","def run(arg_string):\n","    global args\n","    args = parser.parse_args(arg_string.split()) #parser.parse_args()\n","    # args.eval_path = '/content/drive/My Drive/ATiML' + args.eval_path\n","    # Sanity check to know which arguments are being parsed\n","    print(args)\n","    args.cuda = not args.no_cuda and torch.cuda.is_available()\n","    print(args.cuda)\n","    args.decode_dist = log_bernoulli #[1040336]\n","    if args.cont_bernoulli:\n","        args.decode_dist = log_bernoulli_cont\n","    train_loader, test_loader = get_loaders(\n","        dataset=args.dataset,\n","        evaluate=args.eval_iwae or args.eval_ais, # HERE\n","        batch_size=args.batch_size\n","    )\n","    model = get_model(args.dataset, get_default_hparams())\n","\n","    if args.train:\n","        save_path = 'checkpoints/%s/%s/%s%s%s/' % ( #[1040336] Added proper naming to alternate models\n","                        args.dataset,\n","                        'warmup' if args.warmup else 'no_warmup',\n","                        'wide_' if args.wide_encoder else '',\n","                        'hamiltonian_flow' if args.hamiltonian_flow else\n","                            'flow' if args.has_flow else 'ffg',\n","                        '_adadelta' if args.adadelta else '_sgd' if args.sgd else '_contbern' if args.cont_bernoulli else ''\n","                    )\n","        if not os.path.exists(save_path):\n","          os.makedirs(save_path)\n","\n","        train(\n","            model, train_loader, test_loader,\n","            display_epoch=args.display_epoch, epochs=args.epochs,\n","            lr_schedule=args.lr_schedule,\n","            adadelta = args.adadelta, #[1040336]\n","            sgd = args.sgd, #[1040336]\n","            warmup=args.warmup,\n","            early_stopping=args.early_stopping,\n","            encoder_more=args.encoder_more,\n","            save=True, save_path=save_path\n","        )\n","\n","    if args.visdom:\n","        vis = visdom.Visdom(env=args.save, port=args.port)\n","        model.load_state_dict(torch.load(args.eval_path)['state_dict'])\n","\n","        # plot original images\n","        batch, _ = train_loader.next()\n","        images = list(batch.numpy())\n","        win_samples = vis.images(images, 10, 2, opts={'caption': 'original images'}, win=None)\n","\n","        # plot reconstructions\n","        batch = Variable(batch.type(model.dtype))\n","        reconstruction = model.reconstruct_img(batch)\n","        images = list(reconstruction.data.cpu().numpy())\n","        win_samples = vis.images(images, 10, 2, opts={'caption': 'reconstruction'}, win=None)\n","\n","    if args.eval_iwae: #[1040336] Fixed crash from appending list to itself, improved downstream memory management to get it to run on colab\n","        # VAE bounds computed w/ 100 MC samples to reduce variance\n","        model.load_state_dict(torch.load(args.eval_path)['state_dict'])\n","        model.eval()\n","        train_res, test_res = [], []\n","        for _ in range(1):\n","            train_val = test_iwae(model, train_loader, k=1, f=args.eval_path)\n","            test_val = test_iwae(model, test_loader, k=1, f=args.eval_path)\n","            #print(\"about to append\")\n","            train_res.append(train_val)\n","            #print(\"appended train_res\")\n","            test_res.append(test_val)\n","            #print(\"appended test_res\")\n","\n","        #print(\"exited for loop\")\n","        #print(\"length of train_res = {}\".format(len(train_res)))\n","        train_mean = np.mean(train_res)\n","        #print(\"finished calculating mean\")\n","        print ('Training set VAE ELBO w/ 100 MC samples: %.4f' % train_mean)\n","        print ('Test set VAE ELBO w/ 100 MC samples: %.4f' % np.mean(test_res))\n","\n","        # IWAE bounds\n","        print('Computing training set IWAE:')\n","        test_iwae(model, train_loader, k=args.n_iwae, f=args.eval_path)\n","        print('Computing test set IWAE:')\n","        test_iwae(model, test_loader, k=args.n_iwae, f=args.eval_path)\n","\n","    if args.eval_ais:\n","        model.load_state_dict(torch.load(args.eval_path)['state_dict'])\n","        schedule_fn = linear_schedule if args.ais_schedule == 'linear' else sigmoidial_schedule\n","        schedule = schedule_fn(args.n_ais_dist)\n","        logws = ais_trajectory(\n","            model, train_loader,\n","            mode='forward', schedule=schedule, n_sample=args.n_ais_iwae, log_likelihood_fn = args.decode_dist\n","        )\n","        print('Average importance weight: {}'.format(np.mean(logws).mean())) #[1040336] This print was missing\n","    if args.get_samples: #[1040336] Added this function to print mnist images\n","        n = 10\n","        model.load_state_dict(torch.load(args.eval_path)['state_dict'])\n","        z = Variable(torch.randn(n,args.z_size))\n","        logits = model.decode(z)\n","        prob = torch.sigmoid(logits)\n","        if args.cont_bernoulli:\n","            prob = mean_cont_bern(prob)\n","        images = prob.view(n, 1, 28, 28).detach()\n","        image = utils.make_grid( images.data, n)\n","        npimg = image.cpu().numpy() # BlacK background\n","        plt.figure(figsize = (10, 10))\n","        plt.title(args.samples_name, fontsize = 20)\n","        plt.axis('off')\n","        plt.imshow(np.transpose(npimg, (1, 2, 0)), \n","                  interpolation = 'nearest')"],"execution_count":0,"outputs":[]}]}