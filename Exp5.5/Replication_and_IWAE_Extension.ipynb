{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LwI3SDt1bx-"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This code contains almost all the code to replicate my (1041079) work in the report: experiment 5.5.\n",
    "PIWAE is in a different file, however, because this code is significantly different.\n",
    "\n",
    "Large amounts of this code are from different files in Chris Cremer's Github (https://github.com/chriscremer/Inference-Suboptimality) and adapted. I've gone through and commented this code in many places, clarifying what things do for my own understanding. This particular version doesn't contain the adaptions to work on Google Colab, it's meant to run locally.\n",
    "\n",
    "I've added several bits to this code, some small and some large. The small changes mainly involve changing parameters and allowing the code to start training from a later Epoch (to work with Colab constantly disconnecting every 30 minutes). The larger changes involve adding the ability to train with the IWAE lower bound. Where I've changed code, I've pointed this out with a comment.\n",
    "\n",
    "I've attempted to put all my code in one file for clarity but in reality I ran everything using several different files. I've done my best to avoid any errors in combining the code, but this is a possible cause if any remain.\n",
    "\n",
    "All my comments start with \"1041079:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "v-61LCRi1Ntb",
    "outputId": "d64d218c-d8e2-447e-aa16-bafc722e1c61"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '../models')\n",
    "sys.path.insert(0, '../models/utils')\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYO6WRj224-1"
   },
   "source": [
    "# Utilities\n",
    "\n",
    "Code from the \"utils\" file, providing utility functions to be used throughout the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOyhgBpr24RW"
   },
   "outputs": [],
   "source": [
    "# 1041079: I've renamed this from lognormal2 to lognormal to make the inporting work\n",
    "def lognormal(x, mean, logvar):\n",
    "    ''' 1041079: Takes in a batch of x vectors and a batch of mean and log std vectors,\n",
    "    returns a batch made up of values of the log of the normal distribution for each specified vector and specified \n",
    "    distribution. P is the number of samples per batch, B is the number of batches and Z is the dimension \n",
    "    (called Z becuase it's usually the dimension of the latent space)\n",
    "    This is different from the function below as it assumes every sample vector x follows the same distribution\n",
    "    '''\n",
    "    '''\n",
    "    x: [P,B,Z]\n",
    "    mean,logvar: [B,Z]\n",
    "    output: [P,B]\n",
    "    '''\n",
    "\n",
    "    ''' 1041079: x is made up of 3 entries - the first one seems to be useless'''\n",
    "    assert len(x.size()) == 3\n",
    "    assert len(mean.size()) == 2\n",
    "    assert len(logvar.size()) == 2\n",
    "    ''' 1041079: This seem to check that the number of batches of x is the same as for the mean'''\n",
    "    assert x.size()[1] == mean.size()[0]\n",
    "\n",
    "    '''1041079: This calculates the constant term in the log of the normal dist'''\n",
    "    D = x.size()[2]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        term1 = D * torch.log(torch.cuda.FloatTensor([2.*math.pi])) #[1]\n",
    "    else:\n",
    "        term1 = D * torch.log(torch.FloatTensor([2.*math.pi])) #[1]\n",
    "\n",
    "\n",
    "    return -.5 * (Variable(term1) + logvar.sum(1) + ((x - mean).pow(2)/torch.exp(logvar)).sum(2))\n",
    "\n",
    "def lognormal333(x, mean, logvar):\n",
    "    ''' 1041079: This is identical to above but where different means and logvariances can exist for each sample'''\n",
    "    '''\n",
    "    x: [P,B,Z]\n",
    "    mean,logvar: [P,B,Z]\n",
    "    output: [P,B]\n",
    "    '''\n",
    "\n",
    "    assert len(x.size()) == 3\n",
    "    assert len(mean.size()) == 3\n",
    "    assert len(logvar.size()) == 3\n",
    "    assert x.size()[0] == mean.size()[0]\n",
    "    assert x.size()[1] == mean.size()[1]\n",
    "\n",
    "    D = x.size()[2]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        term1 = D * torch.log(torch.cuda.FloatTensor([2.*math.pi])) #[1]\n",
    "    else:\n",
    "        term1 = D * torch.log(torch.FloatTensor([2.*math.pi])) #[1]\n",
    "\n",
    "\n",
    "    return -.5 * (Variable(term1) + logvar.sum(2) + ((x - mean).pow(2)/torch.exp(logvar)).sum(2))\n",
    "\n",
    "\n",
    "def log_bernoulli(pred_no_sig, target):\n",
    "    '''\n",
    "    pred_no_sig is [P, B, X] \n",
    "    t is [B, X]\n",
    "    output is [P, B]\n",
    "    '''\n",
    "\n",
    "    assert len(pred_no_sig.size()) == 3\n",
    "    assert len(target.size()) == 2\n",
    "    assert pred_no_sig.size()[1] == target.size()[0]\n",
    "\n",
    "    return -(torch.clamp(pred_no_sig, min=0)\n",
    "                        - pred_no_sig * target\n",
    "                        + torch.log(1. + torch.exp(-torch.abs(pred_no_sig)))).sum(2) #sum over dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZFWzbJ03Lzo"
   },
   "source": [
    "# Generator\n",
    "\n",
    "Class for the generator (decoder) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cvxjq9NY3JBu"
   },
   "outputs": [],
   "source": [
    "''' 1041079: This is the class for the generator (decoder) '''\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, hyper_config):\n",
    "        ''' 1041079: This function initializes the generator model with any decoder weights given in the hyperparameters'''\n",
    "        super(Generator, self).__init__() \n",
    "        '''1041079: This is just a standard required line for all subclasses of the Pytorch nn Module'''\n",
    "\n",
    "        if hyper_config['cuda']:\n",
    "            self.dtype = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            self.dtype = torch.FloatTensor\n",
    "\n",
    "        self.z_size = hyper_config['z_size'] \n",
    "        '''1041079: z_size is latent dimension size'''\n",
    "        self.x_size = hyper_config['x_size']\n",
    "        self.act_func = hyper_config['act_func']\n",
    "\n",
    "        #Decoder\n",
    "        self.decoder_weights = []\n",
    "        self.layer_norms = []\n",
    "        for i in range(len(hyper_config['decoder_arch'])):\n",
    "            self.decoder_weights.append(nn.Linear(hyper_config['decoder_arch'][i][0], hyper_config['decoder_arch'][i][1]))\n",
    "\n",
    "        count =1\n",
    "        for i in range(len(self.decoder_weights)):\n",
    "            self.add_module(str(count), self.decoder_weights[i])\n",
    "            count+=1\n",
    "   \n",
    "\n",
    "    def decode(self, z):\n",
    "        ''' 1041079: This function decodes! Given an input (z) it outputs x'''\n",
    "        k = z.size()[0]\n",
    "        B = z.size()[1]\n",
    "        z = z.view(-1, self.z_size)\n",
    "\n",
    "        out = z\n",
    "        for i in range(len(self.decoder_weights)-1):\n",
    "            out = self.act_func(self.decoder_weights[i](out))\n",
    "            # out = self.act_func(self.layer_norms[i].forward(self.decoder_weights[i](out)))\n",
    "        out = self.decoder_weights[-1](out)\n",
    "\n",
    "        x = out.view(k, B, self.x_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YI6pMX-T3m3b"
   },
   "source": [
    "# Inference Net\n",
    "\n",
    "Class for the Inference Neural Network (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuSpiAR83jEW"
   },
   "outputs": [],
   "source": [
    "''' 1041079: This is the class for the Inference network (encoder) '''\n",
    "class standard(nn.Module):\n",
    "\n",
    "    def __init__(self, hyper_config):\n",
    "        ''' This function initializes the encoder model with any decoder weights given in the hyperparameters'''\n",
    "        super(standard, self).__init__() \n",
    "        '''1041079: This is just a standard required line for all subclasses of the Pytorch nn Module'''\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.dtype = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            self.dtype = torch.FloatTensor\n",
    "\n",
    "        self.hyper_config = hyper_config\n",
    "\n",
    "        self.z_size = hyper_config['z_size']\n",
    "        self.x_size = hyper_config['x_size']\n",
    "        self.act_func = hyper_config['act_func']\n",
    "\n",
    "\n",
    "        #Encoder\n",
    "        self.encoder_weights = []\n",
    "        self.layer_norms = []\n",
    "        for i in range(len(hyper_config['encoder_arch'])):\n",
    "            self.encoder_weights.append(nn.Linear(hyper_config['encoder_arch'][i][0], hyper_config['encoder_arch'][i][1]))\n",
    "\n",
    "        count =1\n",
    "        for i in range(len(self.encoder_weights)):\n",
    "            self.add_module(str(count), self.encoder_weights[i])\n",
    "            count+=1\n",
    "\n",
    "        self.q = hyper_config['q'] #1041079: removed (self.hyper_config) here because class has already been\n",
    "        # initialised in my version of the code\n",
    "\n",
    "\n",
    "    def forward(self, k, x, logposterior):\n",
    "        '''1041079: This function generates latent variable z sampled from distribution due to input x '''\n",
    "        '''\n",
    "        k: number of samples\n",
    "        x: [B,X]\n",
    "        logposterior(z) -> [P,B]\n",
    "        '''\n",
    "\n",
    "        self.B = x.size()[0]\n",
    "\n",
    "        #Encode\n",
    "        out = x\n",
    "        for i in range(len(self.encoder_weights)-1):\n",
    "            out = self.act_func(self.encoder_weights[i](out))\n",
    "\n",
    "        out = self.encoder_weights[-1](out)\n",
    "        mean = out[:,:self.z_size]  #[B,Z]\n",
    "        logvar = out[:,self.z_size:]\n",
    "\n",
    "        # 1041079: HNF not implemented so I've added the following line:\n",
    "        self.hyper_config['hnf'] = False\n",
    "        # and this is the original code:\n",
    "        if self.hyper_config['hnf']:\n",
    "            z, logqz = self.q.sample(mean, logvar, k, logposterior)\n",
    "        else:\n",
    "            z, logqz = self.q.sample(mean, logvar, k)\n",
    "\n",
    "        return z, logqz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlS9P-tZ3stK"
   },
   "source": [
    "# Distributions\n",
    "\n",
    "Classes for the distributions used throughout the rest of the notebook: Gaussian, which is the FFG distribution, and Flow, which is the Flow distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrvE7rzm3ruD"
   },
   "outputs": [],
   "source": [
    "class Gaussian(nn.Module):\n",
    "\n",
    "    def __init__(self, hyper_config): #, mean, logvar):\n",
    "        #mean,logvar: [B,Z]\n",
    "        super(Gaussian, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.dtype = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            self.dtype = torch.FloatTensor\n",
    "\n",
    "        \n",
    "\n",
    "        # self.B = mean.size()[0]\n",
    "        # # self.z_size = mean.size()[1]\n",
    "        self.z_size = hyper_config['z_size']\n",
    "        self.x_size = hyper_config['x_size']\n",
    "        # # dfas\n",
    "\n",
    "        # self.mean = mean\n",
    "        # self.logvar = logvar\n",
    "\n",
    "\n",
    "    def sample(self, mean, logvar, k):\n",
    "        ''' 1041079: Returns k vectors sampled from FFG gaussian with mean vector ``mean'' and log variance ``logvar'''\n",
    "\n",
    "        self.B = mean.size()[0]\n",
    "\n",
    "        eps = Variable(torch.FloatTensor(k, self.B, self.z_size).normal_().type(self.dtype)) #[P,B,Z]\n",
    "        z = eps.mul(torch.exp(.5*logvar)) + mean  #[P,B,Z]\n",
    "        logqz = lognormal(z, mean, logvar) #[P,B]\n",
    "\n",
    "        return z, logqz\n",
    "\n",
    "\n",
    "\n",
    "    def logprob(self, z, mean, logvar):\n",
    "        '''1041079: Return the logs of the probabilities of achieving values z'''\n",
    "\n",
    "        # self.B = mean.size()[0]\n",
    "\n",
    "        # eps = Variable(torch.FloatTensor(k, self.B, self.z_size).normal_().type(self.dtype)) #[P,B,Z]\n",
    "        # z = eps.mul(torch.exp(.5*logvar)) + mean  #[P,B,Z]\n",
    "        logqz = lognormal(z, mean, logvar) #[P,B]\n",
    "\n",
    "        return logqz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Flow(nn.Module):\n",
    "    \n",
    "    def __init__(self, hyper_config):#, mean, logvar):\n",
    "        #mean,logvar: [B,Z]\n",
    "        super(Flow, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.dtype = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            self.dtype = torch.FloatTensor\n",
    "\n",
    "        self.hyper_config = hyper_config\n",
    "        # self.B = mean.size()[0]\n",
    "        self.z_size = hyper_config['z_size']\n",
    "        self.x_size = hyper_config['x_size']\n",
    "\n",
    "        self.act_func = hyper_config['act_func']\n",
    "        \n",
    "\n",
    "        count =1\n",
    "\n",
    "        # f(vT|x,vT)\n",
    "        # rv_arch = [[self.x_size+self.z_size,200],[200,200],[200,self.z_size*2]]\n",
    "        rv_arch = [[self.z_size,50],[50,50],[50,self.z_size*2]]\n",
    "        self.rv_weights = []\n",
    "        for i in range(len(rv_arch)):\n",
    "            layer = nn.Linear(rv_arch[i][0], rv_arch[i][1])\n",
    "            self.rv_weights.append(layer)\n",
    "            self.add_module(str(count), layer)\n",
    "            count+=1\n",
    "\n",
    "\n",
    "        n_flows = 2\n",
    "        self.n_flows = n_flows\n",
    "        h_s = 50\n",
    "\n",
    "        \n",
    "        self.flow_params = []\n",
    "        for i in range(n_flows):\n",
    "            #first is for v, second is for z\n",
    "            self.flow_params.append([\n",
    "                                [nn.Linear(self.z_size, h_s), nn.Linear(h_s, self.z_size), nn.Linear(h_s, self.z_size)],\n",
    "                                [nn.Linear(self.z_size, h_s), nn.Linear(h_s, self.z_size), nn.Linear(h_s, self.z_size)]\n",
    "                                ])\n",
    "        \n",
    "        for i in range(n_flows):\n",
    "\n",
    "            self.add_module(str(count), self.flow_params[i][0][0])\n",
    "            count+=1\n",
    "            self.add_module(str(count), self.flow_params[i][1][0])\n",
    "            count+=1\n",
    "            self.add_module(str(count), self.flow_params[i][0][1])\n",
    "            count+=1\n",
    "            self.add_module(str(count), self.flow_params[i][1][1])\n",
    "            count+=1\n",
    "            self.add_module(str(count), self.flow_params[i][0][2])\n",
    "            count+=1\n",
    "            self.add_module(str(count), self.flow_params[i][1][2])\n",
    "            count+=1\n",
    "    \n",
    "\n",
    "        # # q(v0)\n",
    "        # self.q_v = Gaussian(self.hyper_config, torch.zeros(self.B, self.z_size), torch.zeros(self.B, self.z_size))\n",
    "\n",
    "        # # q(z0)\n",
    "        # self.q_z = Gaussian(self.hyper_config, mean, logvar)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    def norm_flow(self, params, z, v):\n",
    "        # print (z.size())\n",
    "        h = F.tanh(params[0][0](z))\n",
    "        mew_ = params[0][1](h)\n",
    "        # sig_ = F.sigmoid(params[0][2](h)+5.) #[PB,Z]\n",
    "        sig_ = F.sigmoid(params[0][2](h)) #[PB,Z]\n",
    "\n",
    "        v = v*sig_ + mew_\n",
    "        logdet = torch.sum(torch.log(sig_), 1)\n",
    "\n",
    "\n",
    "\n",
    "        h = F.tanh(params[1][0](v))\n",
    "        mew_ = params[1][1](h)\n",
    "        # sig_ = F.sigmoid(params[1][2](h)+5.) #[PB,Z]\n",
    "        sig_ = F.sigmoid(params[1][2](h)) #[PB,Z]\n",
    "        z = z*sig_ + mew_\n",
    "        logdet2 = torch.sum(torch.log(sig_), 1)\n",
    "\n",
    "\n",
    "\n",
    "        #[PB]\n",
    "        logdet = logdet + logdet2\n",
    "        #[PB,Z], [PB]\n",
    "        return z, v, logdet\n",
    "\n",
    "\n",
    "\n",
    "    def sample(self, mean, logvar, k):\n",
    "\n",
    "        self.B = mean.size()[0]\n",
    "        gaus = Gaussian(self.hyper_config)\n",
    "\n",
    "        # q(z0)\n",
    "        z, logqz0 = gaus.sample(mean, logvar, k)\n",
    "\n",
    "        # q(v0)\n",
    "        # 1041079: I added this cuda check (didn't exist in original code)\n",
    "        if torch.cuda.is_available():\n",
    "            zeros = Variable(torch.zeros(self.B, self.z_size)).cuda()\n",
    "        else:\n",
    "            zeros = Variable(torch.zeros(self.B, self.z_size))\n",
    "        v, logqv0 = gaus.sample(zeros, zeros, k)\n",
    "\n",
    "\n",
    "        #[PB,Z]\n",
    "        z = z.view(-1,self.z_size)\n",
    "        v = v.view(-1,self.z_size)\n",
    "\n",
    "        #Transform\n",
    "        logdetsum = 0.\n",
    "        for i in range(self.n_flows):\n",
    "\n",
    "            params = self.flow_params[i]\n",
    "\n",
    "            # z, v, logdet = self.norm_flow([self.flow_params[i]],z,v)\n",
    "            z, v, logdet = self.norm_flow(params,z,v)\n",
    "            logdetsum += logdet\n",
    "\n",
    "        logdetsum = logdetsum.view(k,self.B)\n",
    "\n",
    "        #r(vT|x,zT)\n",
    "        #r(vT|zT)  try that\n",
    "        out = z #[PB,Z]\n",
    "        # print (out.size())\n",
    "        # fasda\n",
    "        for i in range(len(self.rv_weights)-1):\n",
    "            out = self.act_func(self.rv_weights[i](out))\n",
    "        out = self.rv_weights[-1](out)\n",
    "        # print (out)\n",
    "        mean = out[:,:self.z_size]\n",
    "        logvar = out[:,self.z_size:]\n",
    "        # r_vt = Gaussian(self.hyper_config, mean, logvar)\n",
    "\n",
    "\n",
    "\n",
    "        v = v.view(k, self.B, self.z_size)\n",
    "        z = z.view(k, self.B, self.z_size)\n",
    "\n",
    "        mean = mean.contiguous().view(k, self.B, self.z_size)\n",
    "        logvar = logvar.contiguous().view(k, self.B, self.z_size)\n",
    "\n",
    "        # print (mean.size()) #[PB,Z]\n",
    "        # print (v.size())   #[P,B,Z]\n",
    "        # print (self.B)\n",
    "        # print (k)\n",
    "\n",
    "        # logrvT = gaus.logprob(v, mean, logvar)\n",
    "        logrvT = lognormal333(v, mean, logvar)\n",
    "\n",
    "        # print (logqz0.size())\n",
    "        # print (logqv0.size())\n",
    "        # print (logdetsum.size())\n",
    "        # print (logrvT.size())\n",
    "        # fadsf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        logpz = logqz0+logqv0-logdetsum-logrvT\n",
    "\n",
    "        return z, logpz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Of-Tfmeq4Bv2"
   },
   "source": [
    "# Optimize Local q\n",
    "Code to optimise the parameters for the approximate distribution for a local datapoint x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9Mh3-R54AYY"
   },
   "outputs": [],
   "source": [
    "quick = 0\n",
    "\n",
    "\n",
    "def optimize_local_q_dist(logposterior, hyper_config, x, q):\n",
    "    '''1041079: Optimises parameters for approx dist for single datapoint x'''\n",
    "\n",
    "    B = x.size()[0] #batch size\n",
    "    P = 50 # 1041079: this is the number of IWAE samples to take / number of samples of posterior to take for each datapoint.\n",
    "    # They say they used 100 MC samples for this, but the code was set to 50\n",
    "    \n",
    "    z_size = hyper_config['z_size']\n",
    "    x_size = hyper_config['x_size']\n",
    "    if torch.cuda.is_available():\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "        \n",
    "    mean = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n",
    "    logvar = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n",
    "\n",
    "    params = [mean, logvar]\n",
    "    for aaa in q.parameters():\n",
    "        params.append(aaa)\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(params, lr=.001)\n",
    "\n",
    "    last_100 = []\n",
    "    best_last_100_avg = -1\n",
    "    consecutive_worse = 0\n",
    "    for epoch in range(1, 999999):\n",
    "        \n",
    "        # 1041079: I added all this, so optimized parameters are also calculated using IWAE if required\n",
    "        if arch == 'IWAE':\n",
    "            z, logqz = q.sample(mean, logvar, 64)\n",
    "            logpx = logposterior(z)\n",
    "            optimizer.zero_grad()\n",
    "            elbo = logpx - logqz\n",
    "            max_ = torch.max(elbo, 0)[0]\n",
    "            elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_\n",
    "            elbo = torch.mean(elbo)\n",
    "            loss = -elbo\n",
    "        else:\n",
    "            z, logqz = q.sample(mean, logvar, P)\n",
    "            logpx = logposterior(z)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = -(torch.mean(logpx-logqz))\n",
    "            \n",
    "        loss_np = loss.data.cpu().numpy()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        last_100.append(loss_np)\n",
    "        if epoch % 100 ==0:\n",
    "\n",
    "            last_100_avg = np.mean(last_100)\n",
    "            if last_100_avg< best_last_100_avg or best_last_100_avg == -1:\n",
    "                consecutive_worse=0\n",
    "                best_last_100_avg = last_100_avg\n",
    "            else:\n",
    "                consecutive_worse +=1 \n",
    "                if consecutive_worse>10:\n",
    "                    break\n",
    "\n",
    "            if epoch % 2000 ==0:\n",
    "                print (epoch, last_100_avg, consecutive_worse)\n",
    "\n",
    "            last_100 = []\n",
    "\n",
    "\n",
    "\n",
    "    # 1041079: Calculate IWAE first\n",
    "    z, logqz = q.sample(mean, logvar, 5000)\n",
    "    \n",
    "    logpx = logposterior(z)\n",
    "\n",
    "    elbo = logpx-logqz #[P,B]\n",
    "\n",
    "    max_ = torch.max(elbo, 0)[0] #[B]\n",
    "    elbo_ = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n",
    "    iwae = torch.mean(elbo_)\n",
    "    \n",
    "    if arch == \"IWAE\":\n",
    "        '''1041079: Over here we have to change how they calculate the VAE bound, to make it\n",
    "        the PIWAE bound instead - we will output both L_{IWAE}[q*] and L_{MIWAE}[q*]'''\n",
    "        k_total = 5056\n",
    "        z, logqz = q.sample(mean, logvar, k_total)\n",
    "        logpx = logposterior(z)\n",
    "        elbosIWAE = []\n",
    "\n",
    "        for i in range(int(k_total/ k)):\n",
    "            startindex = i * k\n",
    "            endindex = (i + 1) * k\n",
    "            localqz = logqz[startindex : endindex]\n",
    "            localpx = logpx[startindex : endindex]\n",
    "\n",
    "            weights = localpx - localqz #[P,B]\n",
    "            # Compute IWAE bound (generator network)\n",
    "            max_ = torch.max(weights, 0)[0] #[B]\n",
    "            elboIWAE = torch.mean(torch.log(torch.mean(torch.exp(weights - max_), 0)) + max_) #[B]\n",
    "\n",
    "            elbosIWAE.append(elboIWAE.data[0])\n",
    "\n",
    "        vae = np.mean(elbosIWAE)\n",
    "    else:\n",
    "        \n",
    "        z, logqz = q.sample(mean, logvar, 5000)\n",
    "        logpx = logposterior(z)\n",
    "\n",
    "        elbo = logpx-logqz #[P,B]\n",
    "        vae = torch.mean(elbo)\n",
    "\n",
    "    return vae, iwae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhZETsXz2Qjm"
   },
   "source": [
    "# VAE Class\n",
    "Class for the overall VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBuXzTST2NuE"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, hyper_config, seed=1):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        self.z_size = hyper_config['z_size']\n",
    "        self.x_size = hyper_config['x_size']\n",
    "        self.act_func = hyper_config['act_func']\n",
    "\n",
    "        '''1041079: as far as I can tell, hyper_config['q_dist'] always = standard (the class)\n",
    "        Therefore, this line initialises the encoder'''\n",
    "        self.q_dist = hyper_config['q_dist'](hyper_config=hyper_config)\n",
    "\n",
    "\n",
    "        self.generator = Generator(hyper_config=hyper_config)\n",
    "\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.dtype = torch.cuda.FloatTensor\n",
    "            self.q_dist.cuda()\n",
    "            self.generator.cuda() # 1041079: Added generator.cuda here to make this run on graphics card\n",
    "        else:\n",
    "            self.dtype = torch.FloatTensor\n",
    "            \n",
    "\n",
    "\n",
    "    def forward(self, x, k, warmup=1.):\n",
    "\n",
    "        self.B = x.size()[0] #batch size\n",
    "        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n",
    "\n",
    "        self.logposterior = lambda aa: lognormal(aa, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(aa), x)\n",
    "\n",
    "        z, logqz = self.q_dist.forward(k, x, self.logposterior)\n",
    "\n",
    "        logpxz = self.logposterior(z)\n",
    "\n",
    "        #Compute elbo\n",
    "        elbo = logpxz - (warmup*logqz) #[P,B]\n",
    "        if k>1:\n",
    "            max_ = torch.max(elbo, 0)[0] #[B]\n",
    "            elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n",
    "            \n",
    "        elbo = torch.mean(elbo) #[1]\n",
    "        logpxz = torch.mean(logpxz) #[1]\n",
    "        logqz = torch.mean(logqz)\n",
    "\n",
    "        return elbo, logpxz, logqz\n",
    "\n",
    "\n",
    "    def sample_q(self, x, k):\n",
    "\n",
    "        self.B = x.size()[0] #batch size\n",
    "        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n",
    "\n",
    "        self.logposterior = lambda aa: lognormal(aa, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(aa), x)\n",
    "\n",
    "        z, logqz = self.q_dist.forward(k=k, x=x, logposterior=self.logposterior)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "    def logposterior_func(self, x, z):\n",
    "        self.B = x.size()[0] #batch size\n",
    "        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n",
    "\n",
    "        # print (x)  #[B,X]\n",
    "        # print(z)    #[P,Z]\n",
    "        z = Variable(z).type(self.dtype)\n",
    "        z = z.view(-1,self.B,self.z_size)\n",
    "        return lognormal(z, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(z), x)\n",
    "\n",
    "\n",
    "\n",
    "    def logposterior_func2(self, x, z):\n",
    "        self.B = x.size()[0] #batch size\n",
    "        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n",
    "\n",
    "        # print (x)  #[B,X]\n",
    "        # print(z)    #[P,Z]\n",
    "        # z = Variable(z).type(self.dtype)\n",
    "        z = z.view(-1,self.B,self.z_size)\n",
    "\n",
    "        # print (z)\n",
    "        return lognormal(z, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(z), x)\n",
    "\n",
    "\n",
    "\n",
    "    def forward2(self, x, k):\n",
    "\n",
    "        self.B = x.size()[0] #batch size\n",
    "        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n",
    "\n",
    "        self.logposterior = lambda aa: lognormal(aa, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(aa), x)\n",
    "\n",
    "        z, logqz = self.q_dist.forward(k, x, self.logposterior)\n",
    "\n",
    "        logpxz = self.logposterior(z)\n",
    "\n",
    "        #Compute elbo\n",
    "        elbo = logpxz - logqz #[P,B]\n",
    "        # if k>1:\n",
    "        #     max_ = torch.max(elbo, 0)[0] #[B]\n",
    "        #     elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n",
    "            \n",
    "        elbo = torch.mean(elbo) #[1]\n",
    "        logpxz = torch.mean(logpxz) #[1]\n",
    "        logqz = torch.mean(logqz)\n",
    "\n",
    "        return elbo, logpxz, logqz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward3_prior(self, x, k):\n",
    "\n",
    "        self.B = x.size()[0] #batch size\n",
    "        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n",
    "\n",
    "        self.logposterior = lambda aa:  log_bernoulli(self.generator.decode(aa), x) #+ lognormal(aa, self.zeros, self.zeros)\n",
    "\n",
    "        # z, logqz = self.q_dist.forward(k, x, self.logposterior)\n",
    "\n",
    "        z = Variable(torch.FloatTensor(k, self.B, self.z_size).normal_().type(self.dtype)) #[P,B,Z]\n",
    "\n",
    "        logpxz = self.logposterior(z)\n",
    "\n",
    "        #Compute elbo\n",
    "        elbo = logpxz #- logqz #[P,B]\n",
    "        if k>1:\n",
    "            max_ = torch.max(elbo, 0)[0] #[B]\n",
    "            elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n",
    "            \n",
    "        elbo = torch.mean(elbo) #[1]\n",
    "        # logpxz = torch.mean(logpxz) #[1]\n",
    "        # logqz = torch.mean(logqz)\n",
    "\n",
    "        return elbo#, logpxz, logqz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJoKrS8y0_Mv"
   },
   "source": [
    "# Train MNIST\n",
    "Code to train the VAE on Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "agrqe3oluHEu",
    "outputId": "f732a831-dcc9-45ff-c78c-7c5957d790a7"
   },
   "outputs": [],
   "source": [
    "# 1041079: I've added these two lines to allow different architectures to be easily selected,\n",
    "# and allow the code to load a later epoch\n",
    "architecture = 'Standard' # 'Flow' 'LargerDecoder' 'LargerEncoder\n",
    "epoch_number_to_load = None\n",
    "\n",
    "# 1041079: I also added this to automatically choose hyperparams for all the different architectures\n",
    "''' 1041079: I added all of this'''\n",
    "def get_hparams(arch):\n",
    "    if arch == 'Standard':\n",
    "        hyper_config = { \n",
    "                      'x_size': x_size,\n",
    "                      'z_size': z_size,\n",
    "                      'act_func': F.elu, #F.tanh,# F.relu,\n",
    "                      'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n",
    "                      'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n",
    "                      'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n",
    "                      'cuda': 1\n",
    "                  }\n",
    "        q = Gaussian(hyper_config)\n",
    "        # q = Flow(hyper_config)\n",
    "        hyper_config['q'] = q\n",
    "    elif arch == 'LargerEncoder':\n",
    "        hyper_config = { \n",
    "                      'x_size': x_size,\n",
    "                      'z_size': z_size,\n",
    "                      'act_func': F.elu, #F.tanh,# F.relu,\n",
    "                      'encoder_arch': [[x_size,500],[500,500],[500,z_size*2]],\n",
    "                      'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n",
    "                      'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n",
    "                      'cuda': 1\n",
    "                  }\n",
    "    elif arch == 'LargerDecoder':\n",
    "        hyper_config = { \n",
    "                      'x_size': x_size,\n",
    "                      'z_size': z_size,\n",
    "                      'act_func': F.elu, #F.tanh,# F.relu,\n",
    "                      'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n",
    "                      'decoder_arch': [[z_size,500],[500,500],[500,x_size]],\n",
    "                      'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n",
    "                      'cuda': 1\n",
    "                  }\n",
    "        q = Gaussian(hyper_config)\n",
    "        # q = Flow(hyper_config)\n",
    "        hyper_config['q'] = q\n",
    "    elif arch == 'Flow':\n",
    "        hyper_config = { \n",
    "                      'x_size': x_size,\n",
    "                      'z_size': z_size,\n",
    "                      'act_func': F.elu, #F.tanh,# F.relu,\n",
    "                      'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n",
    "                      'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n",
    "                      'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n",
    "                      'cuda': 1\n",
    "                  }\n",
    "        q = Flow(hyper_config)\n",
    "        # q = Flow(hyper_config)\n",
    "        hyper_config['q'] = q\n",
    "    else:\n",
    "        print(\"Unknown architecture\")\n",
    "    \n",
    "    return hyper_config\n",
    "\n",
    "#FASHION\n",
    "def load_mnist(path, kind='train'):\n",
    "\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(-1, 784)\n",
    "\n",
    "    return images#, labels\n",
    "\n",
    "\n",
    "path = '../datasets/fashion'\n",
    "\n",
    "# 1041079: load fashion mnist, store training data in train_x, testing data in test_x\n",
    "train_x = load_mnist(path=path)\n",
    "test_x = load_mnist(path=path, kind='t10k')\n",
    "\n",
    "# 1041079: re-scale all inputs to [0,1] & binarise\n",
    "train_x = train_x / 255.\n",
    "test_x = test_x / 255.\n",
    "\n",
    "print (train_x.shape)\n",
    "print (test_x.shape)\n",
    "\n",
    "\n",
    "\n",
    "#binarize\n",
    "train_x = (train_x > .5).astype(float)\n",
    "test_x = (test_x > .5).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "def train_encoder_and_decoder(model, train_x, test_x, k, batch_size,\n",
    "                    start_at, save_freq, display_epoch, \n",
    "                    path_to_save_variables, epoch_number_to_load=None):\n",
    "\n",
    "    train_y = torch.from_numpy(np.zeros(len(train_x)))\n",
    "    train_x = torch.from_numpy(train_x).float().type(model.dtype)\n",
    "\n",
    "    train_ = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    #IWAE paper training strategy\n",
    "    time_ = time.time()\n",
    "    total_epochs = 0\n",
    "\n",
    "    i_max = 7 \n",
    "\n",
    "    warmup_over_epochs = 100.\n",
    "\n",
    "\n",
    "    all_params = []\n",
    "    for aaa in model.q_dist.parameters():\n",
    "        all_params.append(aaa)\n",
    "    for aaa in model.generator.parameters():\n",
    "        all_params.append(aaa)\n",
    "    # print (len(all_params), 'number of params')\n",
    "\n",
    "    print (model.q_dist)\n",
    "    # print (model.q_dist.q)\n",
    "    print (model.generator)\n",
    "\n",
    "    # fads\n",
    "\n",
    "    # 1041079: I wrote this\n",
    "    if epoch_number_to_load is not None:\n",
    "      min_i = int(np.log(epoch_number_to_load) / np.log(3)) + 1 # change of base formula\n",
    "      min_epoch = epoch_number_to_load + 1\n",
    "      total_epochs = epoch_number_to_load\n",
    "      load_file = path_to_save_variables+'_generator_'+str(epoch_number_to_load)+'.pt'\n",
    "      model.generator.load_state_dict(torch.load(load_file, map_location=lambda storage, loc: storage))\n",
    "      load_file = path_to_save_variables+'_encoder_'+str(epoch_number_to_load)+'.pt'\n",
    "      model.q_dist.load_state_dict(torch.load(load_file, map_location=lambda storage, loc: storage))\n",
    "      print(min_i, min_epoch, total_epochs)\n",
    "    else:\n",
    "      min_epoch = 1\n",
    "      min_i = 0\n",
    "\n",
    "\n",
    "    for i in range(min_i,i_max+1):\n",
    "\n",
    "        lr = .001 * 10**(-i/float(i_max))\n",
    "        print (i, 'LR:', lr)\n",
    "\n",
    "\n",
    "        optimizer = optim.Adam(all_params, lr=lr)\n",
    "\n",
    "        epochs = 3**(i)\n",
    "\n",
    "        for epoch in range(min_epoch, epochs + 1):\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "                batch = Variable(data)#.type(model.dtype)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                warmup = total_epochs/warmup_over_epochs\n",
    "                if warmup > 1.:\n",
    "                    warmup = 1.\n",
    "\n",
    "                elbo, logpxz, logqz = model.forward(batch, k=k, warmup=warmup)\n",
    "\n",
    "                loss = -(elbo)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_epochs += 1\n",
    "\n",
    "\n",
    "            if total_epochs%display_epoch==0:\n",
    "                print ('Train Epoch: {}/{}'.format(epoch, epochs),\n",
    "                    'total_epochs {}'.format(total_epochs),\n",
    "                    'LL:{:.3f}'.format(-loss.data[0]),\n",
    "                    'logpxz:{:.3f}'.format(logpxz.data[0]),\n",
    "                    'logqz:{:.3f}'.format(logqz.data[0]),\n",
    "                    'warmup:{:.3f}'.format(warmup),\n",
    "                    'T:{:.2f}'.format(time.time()-time_),\n",
    "                    )\n",
    "                time_ = time.time()\n",
    "\n",
    "\n",
    "            if total_epochs >= start_at and (total_epochs-start_at)%save_freq==0:\n",
    "                ''' 1041079: this is where the saving happens.'''\n",
    "\n",
    "                # save params\n",
    "                save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n",
    "                torch.save(model.q_dist.state_dict(), save_file)\n",
    "                print ('saved variables ' + save_file)\n",
    "                save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n",
    "                torch.save(model.generator.state_dict(), save_file)\n",
    "                print ('saved variables ' + save_file)\n",
    "        min_epoch = 1 # 1041079: change epochs to start back at 1\n",
    "\n",
    "\n",
    "\n",
    "    # save params\n",
    "    save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n",
    "    torch.save(model.q_dist.state_dict(), save_file)\n",
    "    print ('saved variables ' + save_file)\n",
    "    save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n",
    "    torch.save(model.generator.state_dict(), save_file)\n",
    "    print ('saved variables ' + save_file)\n",
    "\n",
    "\n",
    "    print ('done training')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Which gpu\n",
    "# 1041079: I've commented this out, this seems to be used to only use 1 GPU of many\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "x_size = 784\n",
    "z_size = 20 # 1041079: in original code this was 50 but paper says 20\n",
    "batch_size = 50 # 1041079: in original code this was 20 but paper (section 6.2) says it should be 50\n",
    "k = 1\n",
    "#save params \n",
    "start_at = 50\n",
    "save_freq = 50\n",
    "display_epoch = 3\n",
    "\n",
    "# 1041079: first run standard\n",
    "print(\"Running {} architecture\".format(architecture))\n",
    "hyper_config = get_hparams(architecture)\n",
    "\n",
    "print ('Init model')\n",
    "model = VAE(hyper_config)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "print('\\nModel:', hyper_config,'\\n')\n",
    "\n",
    "path_to_save_variables='./{}/fashion'.format(architecture)\n",
    "\n",
    "print('\\nTraining')\n",
    "\n",
    "train_encoder_and_decoder(model=model, train_x=train_x, test_x=test_x, k=k, batch_size=batch_size,\n",
    "                    start_at=start_at, save_freq=save_freq, display_epoch=display_epoch, \n",
    "                    path_to_save_variables=path_to_save_variables, epoch_number_to_load=epoch_number_to_load)\n",
    "\n",
    "print ('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTvf8jvptnv0"
   },
   "source": [
    "# Calculate Gaps\n",
    "Requires: Saved state_dicts from different epochs\n",
    "Produces: text files\n",
    "\n",
    "This file reads the saved neural networks for each epoch and calculates L[q], L[q*] and log(p(x)) for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "F-wPgFKkta0t",
    "outputId": "a8257149-25ed-4710-8cff-db4c0aad9e5a"
   },
   "outputs": [],
   "source": [
    "# 1041079: I added this, to specify the training k used for the IWAE bound\n",
    "training_k = 64\n",
    "x_size = 784\n",
    "z_size = 20 # 1041079: changed to 20 as above, originally 50\n",
    "\n",
    "def calculate_gaps(epoch, todo, arch):\n",
    "    architecture = arch\n",
    "    # gpu_to_use = '0' #sys.argv[1]\n",
    "    epoch = epoch #sys.argv[2]\n",
    "\n",
    "    todo = todo #sys.argv[3]\n",
    "\n",
    "    params_file = 'fashion'\n",
    "\n",
    "    # n_data =1001\n",
    "    n_data =20 # 1041079: This seems to be the number of datapoints they optimise locally, and the number of datapoints\n",
    "    # they use to check the amortized VAE and amortised IW. Note this was originally set to 20\n",
    "\n",
    "    #to save results\n",
    "    file_ = '../Exp5/{}/over_training_exps/results_'.format(architecture) +str(n_data)+'_fashion_binarized_2_2.txt'\n",
    "\n",
    "\n",
    "    # 1041079: changed all these from 0 to False\n",
    "    compute_local_opt = False\n",
    "    compute_amort = False\n",
    "    compute_local_opt_test = False\n",
    "    compute_amort_test = False\n",
    "\n",
    "    # 1041079: changed all these from 1 to True (since they were evaluated as Booleans)\n",
    "    if todo == 'amort':\n",
    "        compute_amort = True\n",
    "        compute_amort_test = True\n",
    "\n",
    "    elif todo == 'opt_train':\n",
    "        compute_local_opt = True\n",
    "\n",
    "    elif todo == 'opt_valid':\n",
    "        compute_local_opt_test = True\n",
    "\n",
    "    else:\n",
    "        fadsfafd\n",
    "\n",
    "    \n",
    "    def test_vae(model, data_x, batch_size, display, k):\n",
    "\n",
    "        time_ = time.time()\n",
    "        elbos = []\n",
    "        data_index= 0\n",
    "        for i in range(int(len(data_x)/ batch_size)):\n",
    "\n",
    "            batch = data_x[data_index:data_index+batch_size]\n",
    "            data_index += batch_size\n",
    "\n",
    "            batch = Variable(torch.from_numpy(batch)).type(model.dtype)\n",
    "            if arch == \"IWAE\":\n",
    "                for j in range(int(k / training_k)):\n",
    "                    elbo, logpxz, logqz = model(batch, k=training_k)\n",
    "                    elbos.append(elbo.data[0])\n",
    "            else:\n",
    "                elbo, logpxz, logqz = model.forward2(batch, k=k)\n",
    "\n",
    "                elbos.append(elbo.data[0])\n",
    "\n",
    "        mean_ = np.mean(elbos)\n",
    "\n",
    "        return mean_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test(model, data_x, batch_size, display, k):\n",
    "\n",
    "        time_ = time.time()\n",
    "        elbos = []\n",
    "        data_index= 0\n",
    "        for i in range(int(len(data_x)/ batch_size)):\n",
    "\n",
    "            batch = data_x[data_index:data_index+batch_size]\n",
    "            data_index += batch_size\n",
    "\n",
    "            batch = Variable(torch.from_numpy(batch)).type(model.dtype)\n",
    "\n",
    "            elbo, logpxz, logqz = model(batch, k=k)\n",
    "\n",
    "            elbos.append(elbo.data[0])\n",
    "\n",
    "        mean_ = np.mean(elbos)\n",
    "\n",
    "        return mean_\n",
    "\n",
    "    #FASHION\n",
    "    def load_mnist(path, kind='train'):\n",
    "\n",
    "        images_path = os.path.join(path,\n",
    "                                   '%s-images-idx3-ubyte.gz'\n",
    "                                   % kind)\n",
    "\n",
    "        with gzip.open(images_path, 'rb') as imgpath:\n",
    "            images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                                   offset=16).reshape(-1, 784)\n",
    "\n",
    "        return images\n",
    "\n",
    "\n",
    "    path = '../datasets/fashion'\n",
    "\n",
    "    train_x = load_mnist(path=path)\n",
    "    test_x = load_mnist(path=path, kind='t10k')\n",
    "\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    "\n",
    "    print (train_x.shape)\n",
    "    print (test_x.shape)\n",
    "\n",
    "\n",
    "\n",
    "    #binarize\n",
    "    train_x = (train_x > .5).astype(float)\n",
    "    test_x = (test_x > .5).astype(float)\n",
    "\n",
    "\n",
    "    x_size = 784\n",
    "    z_size = 20 # 1041079: originally 50, changed to 20 for same reasons as above\n",
    "    \n",
    "    # 1041079: this is what I've done:\n",
    "    hyper_config = get_hparams(arch)\n",
    "\n",
    "    print ('Init model')\n",
    "    model = VAE(hyper_config)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        print ('using cuda')\n",
    "    else:\n",
    "        print ('no gpus')\n",
    "\n",
    "\n",
    "    print('\\nModel:', hyper_config,'\\n')\n",
    "\n",
    "    print (model.q_dist)\n",
    "    print (model.generator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print ('Load params for decoder')\n",
    "    path_to_load_variables='../Exp5/{}/'.format(arch) +str(params_file)+'_generator_'+str(epoch)+'.pt'\n",
    "    \n",
    "    model.generator.load_state_dict(torch.load(path_to_load_variables, map_location=lambda storage, loc: storage))\n",
    "    print ('loaded variables ' + path_to_load_variables)\n",
    "    print ()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if compute_amort:\n",
    "\n",
    "        print ('Load params for encoder')\n",
    "        path_to_load_variables='../Exp5/{}/'.format(arch) +str(params_file)+'_encoder_'+str(epoch)+'.pt'\n",
    "\n",
    "        model.q_dist.load_state_dict(torch.load(path_to_load_variables, map_location=lambda storage, loc: storage))\n",
    "        print ('loaded variables ' + path_to_load_variables)\n",
    "\n",
    "\n",
    "    vaes = []\n",
    "    iwaes = []\n",
    "    vaes_flex = []\n",
    "    iwaes_flex = []\n",
    "\n",
    "\n",
    "    batch_size = 10\n",
    "\n",
    "\n",
    "    if compute_local_opt:\n",
    "        print ('optmizing local')\n",
    "        for i in range(len(train_x[:n_data])):\n",
    "\n",
    "            print (i)\n",
    "\n",
    "            x = train_x[i]\n",
    "            x = Variable(torch.from_numpy(x)).type(model.dtype)\n",
    "            x = x.view(1,784)\n",
    "\n",
    "            logposterior = lambda aa: model.logposterior_func2(x=x,z=aa)\n",
    "\n",
    "            q_local = Gaussian(hyper_config)\n",
    "\n",
    "            vae, iwae = optimize_local_q_dist(logposterior, hyper_config, x, q_local)\n",
    "            print (vae.data.cpu().numpy(),iwae.data.cpu().numpy(),'reg')\n",
    "            vaes.append(vae.data.cpu().numpy())\n",
    "            iwaes.append(iwae.data.cpu().numpy())\n",
    "\n",
    "        print()\n",
    "        print ('opt vae',np.mean(vaes))\n",
    "        print ('opt iwae',np.mean(iwaes))\n",
    "        print()\n",
    "\n",
    "\n",
    "        with open(file_, 'a') as f:\n",
    "            writer = csv.writer(f, delimiter=' ')\n",
    "\n",
    "            writer.writerow(['training', epoch, 'L_q_star', np.mean(vaes)])\n",
    "            writer.writerow(['training', epoch, 'logpx', np.mean(iwaes)])\n",
    "\n",
    "\n",
    "    if compute_amort:\n",
    "        VAE_train = test_vae(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, batch_size), display=10, k=5000)\n",
    "        IW_train = test(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, batch_size), display=10, k=5000)\n",
    "        print ('amortized VAE',VAE_train)\n",
    "        print ('amortized IW',IW_train)\n",
    "\n",
    "\n",
    "        with open(file_, 'a') as f:\n",
    "            writer = csv.writer(f, delimiter=' ')\n",
    "\n",
    "            writer.writerow(['training', epoch, 'L_q', VAE_train])\n",
    "            writer.writerow(['training', epoch, 'L_q_IWAE', IW_train])\n",
    "\n",
    "\n",
    "    # TEST SET\n",
    "\n",
    "\n",
    "    vaes_test = []\n",
    "    iwaes_test = []\n",
    "\n",
    "\n",
    "    if compute_local_opt_test:\n",
    "        print ('TEST SET')\n",
    "        print ('optmizing local')\n",
    "        for i in range(len(test_x[:n_data])):\n",
    "\n",
    "            print (i)\n",
    "\n",
    "            x = test_x[i]\n",
    "            x = Variable(torch.from_numpy(x)).type(model.dtype)\n",
    "            x = x.view(1,784)\n",
    "\n",
    "            logposterior = lambda aa: model.logposterior_func2(x=x,z=aa)\n",
    "\n",
    "            q_local = Gaussian(hyper_config)\n",
    "            \n",
    "            vae, iwae = optimize_local_q_dist(logposterior, hyper_config, x, q_local)\n",
    "            print (vae.data.cpu().numpy(),iwae.data.cpu().numpy(),'reg')\n",
    "            vaes_test.append(vae.data.cpu().numpy())\n",
    "            iwaes_test.append(iwae.data.cpu().numpy())\n",
    "\n",
    "        print()\n",
    "        print ('opt vae',np.mean(vaes_test))\n",
    "        print ('opt iwae',np.mean(iwaes_test))\n",
    "        print()\n",
    "\n",
    "        with open(file_, 'a') as f:\n",
    "            writer = csv.writer(f, delimiter=' ')\n",
    "\n",
    "            writer.writerow(['validation', epoch, 'L_q_star', np.mean(vaes_test)])\n",
    "            writer.writerow(['validation', epoch, 'logpx', np.mean(iwaes_test)])\n",
    "\n",
    "\n",
    "    if compute_amort_test:\n",
    "        VAE_test = test_vae(model=model, data_x=test_x[:n_data], batch_size=np.minimum(n_data, batch_size), display=10, k=5000)\n",
    "        IW_test = test(model=model, data_x=test_x[:n_data], batch_size=np.minimum(n_data, batch_size), display=10, k=5000)\n",
    "        print ('amortized VAE',VAE_test)\n",
    "        print ('amortized IW',IW_test)\n",
    "\n",
    "\n",
    "        with open(file_, 'a') as f:\n",
    "            writer = csv.writer(f, delimiter=' ')\n",
    "\n",
    "            writer.writerow(['validation', epoch, 'L_q', VAE_test])\n",
    "            writer.writerow(['validation', epoch, 'L_q_IWAE', IW_test])\n",
    "\n",
    "# 1040179: Modify this to change what the code actually does    \n",
    "for arch in ['IWAE']:\n",
    "    for epoch in [250,300,350]:\n",
    "        for todo in ['amort', 'opt_train', 'opt_valid']:\n",
    "            print(\"**** Calculating architecture = {}, todo = {}, epoch = {} ****\".format(arch, todo, epoch))\n",
    "            calculate_gaps(epoch, todo, arch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHgnA9TahA_c"
   },
   "source": [
    "# Plotting The Graphs\n",
    "Read the text files saved in the cell above, plot the specified graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "udpSMovXeCKH",
    "outputId": "24311bce-e658-4b26-fd05-b9b35d7e7925"
   },
   "outputs": [],
   "source": [
    "architecture = 'Standard'\n",
    "\n",
    "# epochs=['100','1000','1900','2800']\n",
    "# epochs=['100','1000','2200']\n",
    "# epochs=['100','2800']\n",
    "epochs = []\n",
    "bounds = ['logpx', 'L_q_star', 'L_q']\n",
    "\n",
    "\n",
    "values = {}\n",
    "values['training'] = {}\n",
    "values['validation'] = {}\n",
    "# for epoch in epochs:\n",
    "#     values['training'][epoch] = {}\n",
    "#     values['validation'][epoch] = {}\n",
    "# for bound in bounds:\n",
    "#     for epoch in epochs:\n",
    "#         values['training'][epoch][bound] = {}\n",
    "#         values['validation'][epoch][bound] = {}\n",
    "\n",
    "\n",
    "    \n",
    "#read values\n",
    "# results_file = 'results_50'\n",
    "# results_file = 'results_2_fashion'\n",
    "# results_file = 'results_10_fashion'\n",
    "# results_file = 'results_100_fashion'\n",
    "results_file = 'results_20_fashion_binarized_2'\n",
    "\n",
    "file_ = '../Exp5/{}/over_training_exps/'.format(architecture) +results_file+'.txt'\n",
    "\n",
    "max_value = None\n",
    "min_value = None\n",
    "\n",
    "with open(file_, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=' ')\n",
    "    for row in reader:\n",
    "        if len(row) and row[0] in ['training','validation']: \n",
    "            # print (row)\n",
    "            dataset = row[0]\n",
    "            epoch = row[1]\n",
    "            bound = row[2]\n",
    "            value = row[3]\n",
    "\n",
    "            if epoch not in values[dataset]:\n",
    "                values[dataset][epoch] = {}\n",
    "                if epoch not in epochs:\n",
    "                    epochs.append(epoch)\n",
    "                    print (epoch)\n",
    "\n",
    "            values[dataset][epoch][bound] = value\n",
    "\n",
    "            if max_value == None or float(value) > max_value:\n",
    "                max_value = float(value)\n",
    "            if min_value == None or float(value) < min_value:\n",
    "                min_value = float(value)\n",
    "\n",
    "# print (values)\n",
    "\n",
    "# #sort epochs\n",
    "# epochs.sort()\n",
    "\n",
    "# print (epochs)\n",
    "# fads\n",
    "\n",
    "#convert to list\n",
    "training_plot = {}\n",
    "for bound in bounds:\n",
    "    values_to_plot = []\n",
    "    for epoch in epochs:\n",
    "        values_to_plot.append(float(values['training'][epoch][bound]))\n",
    "    training_plot[bound] = values_to_plot \n",
    "print (training_plot)\n",
    "\n",
    "\n",
    "validation_plot = {}\n",
    "for bound in bounds:\n",
    "    values_to_plot = []\n",
    "    for epoch in epochs:\n",
    "        values_to_plot.append(float(values['validation'][epoch][bound]))\n",
    "    validation_plot[bound] = values_to_plot \n",
    "print (validation_plot)\n",
    "\n",
    "\n",
    "epochs_float = [float(x) for x in epochs]\n",
    "\n",
    "\n",
    "rows = 1\n",
    "cols = 2\n",
    "\n",
    "legend=False\n",
    "\n",
    "fig = plt.figure(figsize=(8+cols,2+rows), facecolor='white')\n",
    "\n",
    "# ylimits = [-110, -84]\n",
    "ylimits = [min_value, max_value]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training set\n",
    "ax = plt.subplot2grid((rows,cols), (0,0), frameon=False)\n",
    "\n",
    "ax.set_title('Training Set',family='serif')\n",
    "\n",
    "# for bound in bounds:\n",
    "#     ax.plot(epochs_float,training_plot[bound]) #, label=legends[i], c=colors[i], ls=line_styles[i])\n",
    "\n",
    "ax.fill_between(epochs_float, training_plot['logpx'], training_plot['L_q_star'])\n",
    "ax.fill_between(epochs_float, training_plot['L_q_star'], training_plot['L_q'])\n",
    "\n",
    "ax.set_ylim(ylimits)\n",
    "ax.set_xlim([0, 2000])\n",
    "ax.grid(True, alpha=.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Validation set\n",
    "ax = plt.subplot2grid((rows,cols), (0,1), frameon=False)\n",
    "\n",
    "ax.set_title('Validation Set',family='serif')\n",
    "\n",
    "# for bound in bounds:\n",
    "#     ax.plot(epochs_float,validation_plot[bound]) #, label=legends[i], c=colors[i], ls=line_styles[i])\n",
    "\n",
    "ax.grid(True, alpha=.1)\n",
    "\n",
    "ax.fill_between(epochs_float, validation_plot['logpx'], validation_plot['L_q_star'])\n",
    "ax.fill_between(epochs_float, validation_plot['L_q_star'], validation_plot['L_q'])\n",
    "\n",
    "ax.set_ylim(ylimits)\n",
    "\n",
    "ax.set_xlim([0, 2000])\n",
    "\n",
    "# ax.set_yticks()\n",
    "\n",
    "# family='serif'\n",
    "# fontProperties = {'family':'serif'}\n",
    "# ax.set_xticklabels(ax.get_xticks(), fontProperties)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name_file = './Plots/{}/'.format(architecture) +results_file+'.png'\n",
    "name_file2 = './Plots/{}/'.format(architecture) +results_file+'.pdf'\n",
    "# name_file = home+'/Documents/tmp/plot.png'\n",
    "plt.savefig(name_file)\n",
    "plt.savefig(name_file2)\n",
    "print ('Saved fig', name_file)\n",
    "print ('Saved fig', name_file2)\n",
    "\n",
    "\n",
    "\n",
    "print ('DONE')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ATiML_55.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ATML",
   "language": "python",
   "name": "atml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
