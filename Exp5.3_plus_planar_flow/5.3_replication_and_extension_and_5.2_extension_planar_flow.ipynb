{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.3_replication_and_extension_and_5.2_extension_planar_flow.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uK8QJr8UXNu0","colab_type":"text"},"source":["# Notice\n","\n","The file contains codes used by group 9 to replicate and extend 5.3, and to do the planar flow extension of 5.2.\n","\n","This is the code used by candidate 1040250.\n","\n","The following code is largely based on C. Cremer's Github repository of the paper \"Inference Suboptimality of Variational Autoencoder\" (https://github.com/chriscremer/Inference-Suboptimality). Proper citations are included in the heading of each section of my code. Original codes and modifications to C. Cremer's code will be pointed out and explained at appropriate locations.\n","\n","# Reference List:\n","\n","Cremer, Chris, Li, Xuechen, and Duvenaud, David. Inference suboptimality in variational autoencoders.  In Dy, Jennifer G. and Krause, Andreas (eds.),Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,2018, volume 80 of Proceedings of Machine Learning Research, pp. 1086–1094. PMLR, 2018. URL http://proceedings.mlr.press/v80/cremer18a.html."]},{"cell_type":"markdown","metadata":{"id":"54BKoegMrOK1","colab_type":"text"},"source":["# Different installations and Connection to Google Drive\n","\n","(Original command lines written by the group)\n"]},{"cell_type":"code","metadata":{"id":"Jcv3ftlhrRV3","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_sw956RrSLI","colab_type":"code","colab":{}},"source":["%cd /content/drive/My\\ Drive/ATiML"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"npyEzzvT4b7O","colab_type":"code","colab":{}},"source":["!python --version"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTemb4SD4cmK","colab_type":"code","colab":{}},"source":["!pip3 install tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mIxbxHYqrjO7","colab_type":"code","colab":{}},"source":["!pip3 install http://download.pytorch.org/whl/cu80/torch-0.2.0.post3-cp36-cp36m-manylinux1_x86_64.whl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"31iJ8G17rkk_","colab_type":"code","colab":{}},"source":["!pip3 install torchvision==0.2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"crU-SVr3rnMy","colab_type":"code","colab":{}},"source":["!pip3 install visdom"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOZDKL184uQe","colab_type":"text"},"source":["#Import"]},{"cell_type":"code","metadata":{"id":"2lGKbmHh5EWi","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","\n","import time\n","import sys\n","import os\n","import math\n","import argparse\n","from tqdm import tqdm\n","import numpy as np\n","\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import visdom\n","import torch.utils.data\n","import torch.nn as nn\n","\n","import pickle\n","import os\n","import gzip"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0bGUjPfSn6q9","colab_type":"text"},"source":["# Some functions needed for VAE model\n","\n","Modified from Chris Cremer's util.py file (Cremer et al. (2018)) in his Github Code of the paper. Some codes in that file have been deleted in the following."]},{"cell_type":"code","metadata":{"id":"KGxnVXrZn8_R","colab_type":"code","colab":{}},"source":["\n","def lognormal2(x, mean, logvar):\n","    '''\n","    x: [P,B,Z]\n","    mean,logvar: [B,Z]\n","    output: [P,B]\n","    '''\n","\n","    assert len(x.size()) == 3\n","    assert len(mean.size()) == 2\n","    assert len(logvar.size()) == 2\n","    assert x.size()[1] == mean.size()[0]\n","\n","    D = x.size()[2]\n","\n","    if torch.cuda.is_available():\n","        term1 = D * torch.log(torch.cuda.FloatTensor([2.*math.pi])) #[1]\n","    else:\n","        term1 = D * torch.log(torch.FloatTensor([2.*math.pi])) #[1]\n","\n","\n","    return -.5 * (Variable(term1) + logvar.sum(1) + ((x - mean).pow(2)/torch.exp(logvar)).sum(2))\n","\n","\n","def lognormal333(x, mean, logvar):\n","    '''\n","    x: [P,B,Z]\n","    mean,logvar: [P,B,Z]\n","    output: [P,B]\n","    '''\n","\n","    assert len(x.size()) == 3\n","    assert len(mean.size()) == 3\n","    assert len(logvar.size()) == 3\n","    assert x.size()[0] == mean.size()[0]\n","    assert x.size()[1] == mean.size()[1]\n","\n","    D = x.size()[2]\n","\n","    if torch.cuda.is_available():\n","        term1 = D * torch.log(torch.cuda.FloatTensor([2.*math.pi])) #[1]\n","    else:\n","        term1 = D * torch.log(torch.FloatTensor([2.*math.pi])) #[1]\n","\n","\n","    return -.5 * (Variable(term1) + logvar.sum(2) + ((x - mean).pow(2)/torch.exp(logvar)).sum(2))\n","\n","\n","\n","    \n","def log_bernoulli(pred_no_sig, target):\n","    '''\n","    pred_no_sig is [P, B, X] \n","    t is [B, X]\n","    output is [P, B]\n","    '''\n","\n","    assert len(pred_no_sig.size()) == 3\n","    assert len(target.size()) == 2\n","    assert pred_no_sig.size()[1] == target.size()[0]\n","\n","    return -(torch.clamp(pred_no_sig, min=0)\n","                        - pred_no_sig * target\n","                        + torch.log(1. + torch.exp(-torch.abs(pred_no_sig)))).sum(2) #sum over dimensions\n","\n","\n","\n","\n","\n","\n","\n","\n","def lognormal3(x, mean, logvar):\n","    '''\n","    x: [P]\n","    mean,logvar: [P]\n","    output: [1]\n","    '''\n","\n","    return -.5 * (logvar.sum(0) + ((x - mean).pow(2)/torch.exp(logvar)).sum(0))\n","\n","\n","\n","\n","def lognormal4(x, mean, logvar):\n","    '''\n","    x: [B,X]\n","    mean,logvar: [X]\n","    output: [B]\n","    '''\n","    # print x.size()\n","    # print mean.size()\n","    # print logvar.size()\n","    # print mean\n","    # print logvar\n","    D = x.size()[1]\n","    # print D\n","    term1 = D * torch.log(torch.FloatTensor([2.*math.pi])) #[1]\n","    # print term1\n","    # print logvar.sum(0)\n","\n","    aaa = -.5 * (term1 + logvar.sum(0) + ((x - mean).pow(2)/torch.exp(logvar)).sum(1))\n","    # print aaa.size()\n","\n","    return aaa\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0Qu-BloajN7","colab_type":"text"},"source":["# Generator of the VAE\n","\n","Modified from Chris Cremer's generator.py file (Cremer et al. (2018)) in his Github Code of the paper. Some codes in that file have been deleted in the following."]},{"cell_type":"code","metadata":{"id":"b9ZEj_kkoTnY","colab_type":"code","colab":{}},"source":["\n","class Generator(nn.Module):\n","\n","    def __init__(self, hyper_config):\n","        super(Generator, self).__init__()\n","\n","        if hyper_config['cuda']:\n","            self.dtype = torch.cuda.FloatTensor\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","        self.z_size = hyper_config['z_size']\n","        self.x_size = hyper_config['x_size']\n","        self.act_func = hyper_config['act_func']\n","\n","        #Decoder\n","        self.decoder_weights = []\n","        self.layer_norms = []\n","        for i in range(len(hyper_config['decoder_arch'])):\n","            self.decoder_weights.append(nn.Linear(hyper_config['decoder_arch'][i][0], hyper_config['decoder_arch'][i][1]))\n","\n","        count =1\n","        for i in range(len(self.decoder_weights)):\n","            self.add_module(str(count), self.decoder_weights[i])\n","            count+=1\n","   \n","\n","    def decode(self, z):\n","        k = z.size()[0]\n","        B = z.size()[1]\n","        z = z.view(-1, self.z_size)\n","\n","\n","        out = z\n","        for i in range(len(self.decoder_weights)-1):\n","            out = self.act_func(self.decoder_weights[i](out))\n","        out = self.decoder_weights[-1](out)\n","\n","        x = out.view(k, B, self.x_size)\n","        return x\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PFKMRG_iaq_4","colab_type":"text"},"source":["# Approximate Posterior Distributions used by the paper\n","\n","Modified from Chris Cremer's distributions.py file (Cremer et al. (2018)) in his Github Code of the paper. Some codes (e.g. HNF(nn.Module)) in that file have been deleted.\n","\n","Note that \"Flow\" refers to Auxilairy Flow distributions, and \"Flow1\" refers to Flow distributions."]},{"cell_type":"code","metadata":{"id":"OXG5_VtBnoqY","colab_type":"code","colab":{}},"source":["\n","\n","class Gaussian(nn.Module):\n","\n","    def __init__(self, hyper_config):\n","        #mean,logvar: [B,Z]\n","        super(Gaussian, self).__init__()\n","\n","        if torch.cuda.is_available():\n","            self.dtype = torch.cuda.FloatTensor\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","        self.z_size = hyper_config['z_size']\n","        self.x_size = hyper_config['x_size']\n","\n","\n","\n","    def sample(self, mean, logvar, k):\n","\n","        self.B = mean.size()[0]\n","\n","        eps = Variable(torch.FloatTensor(k, self.B, self.z_size).normal_().type(self.dtype)) #[P,B,Z]\n","        z = eps.mul(torch.exp(.5*logvar)) + mean  #[P,B,Z]\n","        logqz = lognormal2(z, mean, logvar) #[P,B]\n","\n","        return z, logqz\n","\n","\n","\n","    def logprob(self, z, mean, logvar):\n","\n","        logqz = lognormal2(z, mean, logvar) #[P,B]\n","\n","        return logqz\n","\n","\n","\n","\n","\n","\n","\n","class Flow(nn.Module):\n","\n","    def __init__(self, hyper_config):\n","        #mean,logvar: [B,Z]\n","        super(Flow, self).__init__()\n","\n","        if torch.cuda.is_available():\n","            self.dtype = torch.cuda.FloatTensor\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","        self.hyper_config = hyper_config\n","        # self.B = mean.size()[0]\n","        self.z_size = hyper_config['z_size']\n","        self.x_size = hyper_config['x_size']\n","\n","        self.act_func = hyper_config['act_func']\n","        \n","\n","        count =1\n","\n","        # f(vT|x,vT)\n","        rv_arch = [[self.z_size,50],[50,50],[50,self.z_size*2]]\n","        self.rv_weights = []\n","        for i in range(len(rv_arch)):\n","            layer = nn.Linear(rv_arch[i][0], rv_arch[i][1])\n","            self.rv_weights.append(layer)\n","            self.add_module(str(count), layer)\n","            count+=1\n","\n","\n","        n_flows = 2\n","        self.n_flows = n_flows\n","        h_s = 50\n","\n","        \n","        self.flow_params = []\n","        for i in range(n_flows):\n","            #first is for v, second is for z\n","            self.flow_params.append([\n","                                [nn.Linear(self.z_size, h_s), nn.Linear(h_s, self.z_size), nn.Linear(h_s, self.z_size)],\n","                                [nn.Linear(self.z_size, h_s), nn.Linear(h_s, self.z_size), nn.Linear(h_s, self.z_size)]\n","                                ])\n","        \n","        for i in range(n_flows):\n","\n","            self.add_module(str(count), self.flow_params[i][0][0])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][1][0])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][0][1])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][1][1])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][0][2])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][1][2])\n","            count+=1\n","    \n","\n","\n"," \n","\n","\n","    def norm_flow(self, params, z, v):\n","        h = F.tanh(params[0][0](z))\n","        mew_ = params[0][1](h)\n","        sig_ = F.sigmoid(params[0][2](h)) #[PB,Z]\n","\n","        v = v*sig_ + mew_\n","        logdet = torch.sum(torch.log(sig_), 1)\n","\n","\n","\n","        h = F.tanh(params[1][0](v))\n","        mew_ = params[1][1](h)\n","        sig_ = F.sigmoid(params[1][2](h)) #[PB,Z]\n","        z = z*sig_ + mew_\n","        logdet2 = torch.sum(torch.log(sig_), 1)\n","\n","\n","\n","        #[PB]\n","        logdet = logdet + logdet2\n","        #[PB,Z], [PB]\n","        return z, v, logdet\n","\n","\n","\n","    def sample(self, mean, logvar, k):\n","\n","        self.B = mean.size()[0]\n","        gaus = Gaussian(self.hyper_config)\n","\n","        # q(z0)\n","        z, logqz0 = gaus.sample(mean, logvar, k)\n","\n","        # q(v0)\n","        zeros = Variable(torch.zeros(self.B, self.z_size)).cuda()\n","        v, logqv0 = gaus.sample(zeros, zeros, k)\n","\n","\n","        #[PB,Z]\n","        z = z.view(-1,self.z_size)\n","        v = v.view(-1,self.z_size)\n","\n","        #Transform\n","        logdetsum = 0.\n","        for i in range(self.n_flows):\n","\n","            params = self.flow_params[i]\n","\n","            z, v, logdet = self.norm_flow(params,z,v)\n","            logdetsum += logdet\n","\n","        logdetsum = logdetsum.view(k,self.B)\n","\n","\n","        out = z #[PB,Z]\n","\n","        for i in range(len(self.rv_weights)-1):\n","            out = self.act_func(self.rv_weights[i](out))\n","        out = self.rv_weights[-1](out)\n","        mean = out[:,:self.z_size]\n","        logvar = out[:,self.z_size:]\n","\n","\n","\n","        v = v.view(k, self.B, self.z_size)\n","        z = z.view(k, self.B, self.z_size)\n","\n","        mean = mean.contiguous().view(k, self.B, self.z_size)\n","        logvar = logvar.contiguous().view(k, self.B, self.z_size)\n","\n","\n","\n","        logrvT = lognormal333(v, mean, logvar)\n","\n","\n","\n","\n","\n","        logpz = logqz0+logqv0-logdetsum-logrvT\n","\n","        return z, logpz\n","\n","\n","\n","\n","\n","\n","#NO AUX VAF\n","class Flow1(nn.Module):\n","\n","    def __init__(self, hyper_config):\n","        #mean,logvar: [B,Z]\n","        super(Flow1, self).__init__()\n","\n","        if torch.cuda.is_available():\n","            self.dtype = torch.cuda.FloatTensor\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","        self.hyper_config = hyper_config\n","        self.z_size = hyper_config['z_size']\n","        self.x_size = hyper_config['x_size']\n","\n","        self.act_func = hyper_config['act_func']\n","        \n","\n","        count =1\n","\n","\n","        n_flows = 2\n","        self.n_flows = n_flows\n","        h_s = 50\n","\n","        self.z_half_size = int(self.z_size / 2)\n","\n","        \n","        self.flow_params = []\n","        for i in range(n_flows):\n","            self.flow_params.append([\n","                                [nn.Linear(self.z_half_size, h_s), nn.Linear(h_s, self.z_half_size), nn.Linear(h_s, self.z_half_size)],\n","                                [nn.Linear(self.z_half_size, h_s), nn.Linear(h_s, self.z_half_size), nn.Linear(h_s, self.z_half_size)]\n","                                ])\n","        \n","        for i in range(n_flows):\n","\n","            self.add_module(str(count), self.flow_params[i][0][0])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][1][0])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][0][1])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][1][1])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][0][2])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][1][2])\n","            count+=1\n","    \n","\n","\n","\n","    def norm_flow(self, params, z1, z2):\n","        h = F.tanh(params[0][0](z1))\n","        mew_ = params[0][1](h)\n","        sig_ = F.sigmoid(params[0][2](h)) #[PB,Z]\n","\n","        z2 = z2*sig_ + mew_\n","        logdet = torch.sum(torch.log(sig_), 1)\n","\n","\n","        h = F.tanh(params[1][0](z2))\n","        mew_ = params[1][1](h)\n","        sig_ = F.sigmoid(params[1][2](h)) #[PB,Z]\n","        z1 = z1*sig_ + mew_\n","        logdet2 = torch.sum(torch.log(sig_), 1)\n","\n","\n","\n","        #[PB]\n","        logdet = logdet + logdet2\n","        #[PB,Z], [PB]\n","        return z1, z2, logdet\n","\n","\n","\n","    def sample(self, mean, logvar, k):\n","\n","        self.B = mean.size()[0]\n","        gaus = Gaussian(self.hyper_config)\n","\n","        # q(z0)\n","        z, logqz0 = gaus.sample(mean, logvar, k)\n","\n","        #[PB,Z]\n","        z = z.view(-1,self.z_size)\n","\n","        #Split z  [PB,Z/2]\n","        z1 = z.narrow(1, 0, self.z_half_size)\n","        z2 = z.narrow(1, self.z_half_size, self.z_half_size) \n","\n","        #Transform\n","        logdetsum = 0.\n","        for i in range(self.n_flows):\n","\n","            params = self.flow_params[i]\n","\n","            z1, z2, logdet = self.norm_flow(params,z1,z2)\n","            logdetsum += logdet\n","\n","        logdetsum = logdetsum.view(k,self.B)\n","\n","        #Put z back together  [PB,Z]\n","        z = torch.cat([z1,z2],1)\n","\n","        z = z.view(k, self.B, self.z_size)\n","\n","\n","        logpz = logqz0-logdetsum\n","\n","        return z, logpz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"26JeytQAhEyy","colab_type":"text"},"source":["# Approximate Posterior: 4-Flow\n","\n","This code is exactly the same as the code for Flow1(nn.Module), but self.n_flow is set to 4 to perform 4 Flow transformations."]},{"cell_type":"code","metadata":{"id":"gevT4dK6hJ_T","colab_type":"code","colab":{}},"source":["\n","class Four_Flow1(nn.Module):\n","\n","    def __init__(self, hyper_config):\n","        #mean,logvar: [B,Z]\n","        super(Four_Flow1, self).__init__()\n","\n","        if torch.cuda.is_available():\n","            self.dtype = torch.cuda.FloatTensor\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","        self.hyper_config = hyper_config\n","        self.z_size = hyper_config['z_size']\n","        self.x_size = hyper_config['x_size']\n","\n","        self.act_func = hyper_config['act_func']\n","        \n","\n","        count =1\n","\n","\n","        n_flows = 4\n","        self.n_flows = n_flows\n","        h_s = 50\n","\n","        self.z_half_size = int(self.z_size / 2)\n","\n","        \n","        self.flow_params = []\n","        for i in range(n_flows):\n","            self.flow_params.append([\n","                                [nn.Linear(self.z_half_size, h_s), nn.Linear(h_s, self.z_half_size), nn.Linear(h_s, self.z_half_size)],\n","                                [nn.Linear(self.z_half_size, h_s), nn.Linear(h_s, self.z_half_size), nn.Linear(h_s, self.z_half_size)]\n","                                ])\n","        \n","        for i in range(n_flows):\n","\n","            self.add_module(str(count), self.flow_params[i][0][0])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][1][0])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][0][1])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][1][1])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][0][2])\n","            count+=1\n","            self.add_module(str(count), self.flow_params[i][1][2])\n","            count+=1\n","    \n","\n","\n","\n","    def norm_flow(self, params, z1, z2):\n","        h = F.tanh(params[0][0](z1))\n","        mew_ = params[0][1](h)\n","        sig_ = F.sigmoid(params[0][2](h)) #[PB,Z]\n","\n","        z2 = z2*sig_ + mew_\n","        logdet = torch.sum(torch.log(sig_), 1)\n","\n","\n","        h = F.tanh(params[1][0](z2))\n","        mew_ = params[1][1](h)\n","        sig_ = F.sigmoid(params[1][2](h)) #[PB,Z]\n","        z1 = z1*sig_ + mew_\n","        logdet2 = torch.sum(torch.log(sig_), 1)\n","\n","\n","\n","        #[PB]\n","        logdet = logdet + logdet2\n","        #[PB,Z], [PB]\n","        return z1, z2, logdet\n","\n","\n","\n","    def sample(self, mean, logvar, k):\n","\n","        self.B = mean.size()[0]\n","        gaus = Gaussian(self.hyper_config)\n","\n","        # q(z0)\n","        z, logqz0 = gaus.sample(mean, logvar, k)\n","\n","        #[PB,Z]\n","        z = z.view(-1,self.z_size)\n","\n","        #Split z  [PB,Z/2]\n","        z1 = z.narrow(1, 0, self.z_half_size)\n","        z2 = z.narrow(1, self.z_half_size, self.z_half_size) \n","\n","        #Transform\n","        logdetsum = 0.\n","        for i in range(self.n_flows):\n","\n","            params = self.flow_params[i]\n","\n","            z1, z2, logdet = self.norm_flow(params,z1,z2)\n","            logdetsum += logdet\n","\n","        logdetsum = logdetsum.view(k,self.B)\n","\n","        #Put z back together  [PB,Z]\n","        z = torch.cat([z1,z2],1)\n","\n","        z = z.view(k, self.B, self.z_size)\n","\n","\n","        logpz = logqz0-logdetsum\n","\n","        return z, logpz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGQeth0lLoRl","colab_type":"text"},"source":["# Original Approximate Posterior: Planar_Flow\n","\n","This is a original piece of code written by an member of the group to do the planar flow extension."]},{"cell_type":"code","metadata":{"id":"-oJ8LnhqLrxx","colab_type":"code","colab":{}},"source":["\n","class Planar_Flow(nn.Module):\n","\n","    def __init__(self, hyper_config):#, mean, logvar):\n","        #mean,logvar: [B,Z]\n","        super(Planar_Flow, self).__init__()\n","\n","        if torch.cuda.is_available():\n","            self.dtype = torch.cuda.FloatTensor\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","        self.hyper_config = hyper_config\n","        self.z_size = hyper_config['z_size']\n","        self.x_size = hyper_config['x_size']\n","\n","        self.act_func = hyper_config['act_func']\n","      \n","\n","\n","        \n","        self.flow_params = []\n","\n","\n","\n","\n","\n","\n","\n","\n","    def sample(self, mean, logvar, k, w1, u1, b1, w2, u2, b2):\n","\n","        self.B = mean.size()[0]\n","        gaus = Gaussian(self.hyper_config)\n","\n","        # mean, logvar, w1, u1, w2, u2: should have size [B,Z]\n","        # b1, b2 should have size [B,1]\n","\n","\n","\n","        # q(z0); z: [P,B,Z]\n","        z, logqz0 = gaus.sample(mean, logvar, k)\n","\n","        #[PB,Z]\n","        z = z.view(-1,self.z_size)\n","        w1 = w1.repeat(k,1,1)\n","        w1 = w1.view(-1,self.z_size)\n","        w2 = w2.repeat(k,1,1)\n","        w2 = w2.view(-1,self.z_size)\n","        u1 = u1.repeat(k,1,1)\n","        u1 = u1.view(-1,self.z_size)\n","        u2 = u2.repeat(k,1,1)\n","        u2 = u2.view(-1,self.z_size)\n","        b1 = b1.repeat(k,1,1)\n","        b1 = b1.view(-1,1)\n","        b2 = b2.repeat(k,1,1)\n","        b2 = b2.view(-1,1)\n","\n","\n","\n","        # First Planar Flow transformation\n","        # [PB]\n","        wu_1 = torch.sum(torch.mul(w1 , u1) , 1)\n","\n","        # [PB,1]\n","        wu_1_transpose = wu_1.view(-1,1)\n","\n","        wz_1 = torch.sum(torch.mul(w1 , z) , 1)\n","\n","        # [PB,1]\n","        wz_1_transpose = wz_1.view(-1,1)\n","\n","        # [PB]\n","        m_wu_1 = torch.log(1. + torch.exp(wu_1)) - 1.\n","\n","        # [PB,1]\n","        m_wu_1_transpose = m_wu_1.view(-1,1)\n","\n","        # [PB]\n","        sq_norm_of_w1 = torch.sum(torch.mul(w1 , w1) , 1)\n","        # [PB,1]\n","        sq_norm_of_w1_transpose = sq_norm_of_w1.view(-1,1)\n","\n","        # [PB,Z]\n","        u1_prime = u1 + ((m_wu_1_transpose - wu_1_transpose)*w1)/sq_norm_of_w1_transpose\n","\n","        # [PB,Z]\n","        z_first = z + torch.tanh(wz_1_transpose  + b1)*u1_prime\n","\n","        # [PB,1]\n","        logdet_first = torch.log(torch.abs(1. + (1. - (torch.tanh(wz_1_transpose  + b1)).pow(2))*((torch.sum(torch.mul(w1 , u1_prime),1)).view(-1,1))))\n","        \n","        # [PB]\n","        logdet_first = logdet_first.view(-1)\n","\n","\n","        # Second Planar Flow Transformation\n","        # [PB]\n","        wu_2 = torch.sum(torch.mul(w2 , u2) , 1)\n","\n","        # [PB,1]\n","        wu_2_transpose = wu_2.view(-1,1)\n","\n","        wz_2 = torch.sum(torch.mul(w1 , z_first) , 1)\n","\n","        # [PB,1]\n","        wz_2_transpose = wz_2.view(-1,1)\n","\n","        # [PB]\n","        m_wu_2 = torch.log(1. + torch.exp(wu_2)) - 1.\n","\n","        # [PB,1]\n","        m_wu_2_transpose = m_wu_2.view(-1,1)\n","\n","        # [PB]\n","        sq_norm_of_w2 = torch.sum(torch.mul(w2 , w2) , 1)\n","        # [PB,1]\n","        sq_norm_of_w2_transpose = sq_norm_of_w2.view(-1,1)\n","\n","        # [PB,Z]\n","        u2_prime = u2 + ((m_wu_2_transpose - wu_2_transpose)*w2)/sq_norm_of_w2_transpose\n","\n","        # [PB,Z]\n","        z_second = z_first + torch.tanh(wz_2_transpose  + b2)*u2_prime\n","\n","        # [PB,1]\n","        logdet_second = torch.log(torch.abs(1. + (1. - (torch.tanh(wz_2_transpose  + b2)).pow(2))*((torch.sum(torch.mul(w2 , u2_prime),1)).view(-1,1))))\n","\n","        # [PB]\n","        logdet_second = logdet_second.view(-1)\n","\n","\n","\n","        logdetsum = 0.\n","\n","        logdetsum = logdetsum - logdet_first - logdet_second\n","\n","\n","        logdetsum = logdetsum.view(k,self.B)\n","\n","        z_second = z_second.view(k, self.B, self.z_size)\n","\n","\n","        logpz = logqz0-logdetsum\n","\n","        return z_second, logpz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMxrGRs4bL1r","colab_type":"text"},"source":["# Encoder of the VAE\n","\n","Modified from Chris Cremer's inference_net.py file (Cremer et al. (2018)) in his Github Code of the paper. Some codes in that file have been deleted. Moreover, the forward function has been modified to account for the use of planar flow approximate posterior."]},{"cell_type":"code","metadata":{"id":"H-5-OMRcfuUz","colab_type":"code","colab":{}},"source":["\n","\n","class standard(nn.Module):\n","\n","    def __init__(self, hyper_config):\n","        super(standard, self).__init__()\n","\n","        if torch.cuda.is_available():\n","            self.dtype = torch.cuda.FloatTensor\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","        self.hyper_config = hyper_config\n","\n","        self.z_size = hyper_config['z_size']\n","        self.x_size = hyper_config['x_size']\n","        self.act_func = hyper_config['act_func']\n","\n","\n","        #Encoder\n","        self.encoder_weights = []\n","        self.layer_norms = []\n","        for i in range(len(hyper_config['encoder_arch'])):\n","            self.encoder_weights.append(nn.Linear(hyper_config['encoder_arch'][i][0], \n","                                                  hyper_config['encoder_arch'][i][1]))\n","\n","\n","        count =1\n","        for i in range(len(self.encoder_weights)):\n","            self.add_module(str(count), self.encoder_weights[i])\n","            count+=1\n","       \n","\n","\n","\n","        # self.q = Gaussian(self.hyper_config) #, mean, logvar)\n","        # self.q = Flow(self.hyper_config)#, mean, logvar)\n","        self.q = hyper_config['q']\n","\n","\n","    def forward(self, k, x, logposterior):\n","        '''\n","        k: number of samples\n","        x: [B,X]\n","        logposterior(z) -> [P,B]\n","        '''\n","\n","        self.B = x.size()[0]\n","\n","        #Encode\n","        out = x\n","        for i in range(len(self.encoder_weights)-1):\n","            out = self.act_func(self.encoder_weights[i](out))\n","\n","        out = self.encoder_weights[-1](out)\n","        mean = out[:,:self.z_size]  #[B,Z]\n","        logvar = out[:,self.z_size:2*self.z_size]\n","        if self.hyper_config['planar_flow']:\n","          w1 = out[:,2*self.z_size:3*self.z_size]\n","          u1 = out[:,3*self.z_size:4*self.z_size]\n","          w2 = out[:,4*self.z_size:5*self.z_size]\n","          u2 = out[:,5*self.z_size:6*self.z_size]\n","          b1 = out[:,6*self.z_size:(6*self.z_size + 1)]\n","          b2 = out[:,(6*self.z_size + 1):(6*self.z_size + 2)]\n","\n","\n","\n","        if self.hyper_config['hnf']:\n","            z, logqz = self.q.sample(mean, logvar, k, logposterior)\n","        elif self.hyper_config['planar_flow']:\n","            z, logqz = self.q.sample(mean, logvar, k, w1, u1, b1, w2, u2, b2)\n","        else:\n","            z, logqz = self.q.sample(mean, logvar, k)\n","\n","        return z, logqz\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZLTA7Rh62SNZ","colab_type":"text"},"source":["# the whole VAE model for training\n","\n","Modified from Chris Cremer's vae_2.py file (Cremer et al. (2018)) in his Github Code of the paper. Some codes in that file have been deleted."]},{"cell_type":"code","metadata":{"id":"IlOosf3x2WH9","colab_type":"code","colab":{}},"source":["\n","\n","class VAE(nn.Module):\n","    def __init__(self, hyper_config, seed=1):\n","        super(VAE, self).__init__()\n","\n","        torch.manual_seed(seed)\n","\n","\n","        self.z_size = hyper_config['z_size']\n","        self.x_size = hyper_config['x_size']\n","        self.act_func = hyper_config['act_func']\n","\n","\n","        self.q_dist = hyper_config['q_dist'](hyper_config=hyper_config)\n","\n","\n","        self.generator = Generator(hyper_config=hyper_config)\n","\n","\n","        if torch.cuda.is_available():\n","            self.dtype = torch.cuda.FloatTensor\n","            self.q_dist.cuda()\n","        else:\n","            self.dtype = torch.FloatTensor\n","            \n","\n","\n","\n","    def forward(self, x, k, warmup=1.):\n","\n","        self.B = x.size()[0] #batch size\n","        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","        self.logposterior = lambda aa: lognormal2(aa, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(aa), x)\n","\n","        z, logqz = self.q_dist.forward(k, x, self.logposterior)\n","\n","        logpxz = self.logposterior(z)\n","\n","        #Compute elbo\n","        elbo = logpxz - (warmup*logqz) #[P,B]\n","        if k>1:\n","            max_ = torch.max(elbo, 0)[0] #[B]\n","            elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n","            \n","        elbo = torch.mean(elbo) #[1]\n","        logpxz = torch.mean(logpxz) #[1]\n","        logqz = torch.mean(logqz)\n","\n","        return elbo, logpxz, logqz\n","\n","\n","    def sample_q(self, x, k):\n","\n","        self.B = x.size()[0] #batch size\n","        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","        self.logposterior = lambda aa: lognormal2(aa, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(aa), x)\n","\n","        z, logqz = self.q_dist.forward(k=k, x=x, logposterior=self.logposterior)\n","\n","        return z\n","\n","\n","    def logposterior_func(self, x, z):\n","        self.B = x.size()[0] #batch size\n","        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","\n","        z = Variable(z).type(self.dtype)\n","        z = z.view(-1,self.B,self.z_size)\n","        return lognormal2(z, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(z), x)\n","\n","\n","\n","    def logposterior_func2(self, x, z):\n","        self.B = x.size()[0] #batch size\n","        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","\n","        z = z.view(-1,self.B,self.z_size)\n","\n","        return lognormal2(z, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(z), x)\n","\n","\n","\n","    def forward2(self, x, k):\n","\n","        self.B = x.size()[0] #batch size\n","        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","        self.logposterior = lambda aa: lognormal2(aa, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(aa), x)\n","\n","        z, logqz = self.q_dist.forward(k, x, self.logposterior)\n","\n","        logpxz = self.logposterior(z)\n","\n","        #Compute elbo\n","        elbo = logpxz - logqz #[P,B]\n","            \n","        elbo = torch.mean(elbo) #[1]\n","        logpxz = torch.mean(logpxz) #[1]\n","        logqz = torch.mean(logqz)\n","\n","        return elbo, logpxz, logqz\n","\n","\n","\n","\n","    def forward3_prior(self, x, k):\n","\n","        self.B = x.size()[0] #batch size\n","        self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","        self.logposterior = lambda aa:  log_bernoulli(self.generator.decode(aa), x)\n","\n","        z = Variable(torch.FloatTensor(k, self.B, self.z_size).normal_().type(self.dtype)) #[P,B,Z]\n","\n","        logpxz = self.logposterior(z)\n","\n","        #Compute elbo\n","        elbo = logpxz #- logqz #[P,B]\n","        if k>1:\n","            max_ = torch.max(elbo, 0)[0] #[B]\n","            elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n","            \n","        elbo = torch.mean(elbo) #[1]\n","\n","        return elbo\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wS43m39lb6mZ","colab_type":"text"},"source":["# A function needed for computing L(q*)\n","\n","Modified from Chris Cremer's optimize_local_q.py file (Cremer et al. (2018)) in his Github Code of the paper. Some codes in that file have been deleted. Modifications has been made so that this code can be used to compute L(q*) for planar flow."]},{"cell_type":"code","metadata":{"id":"DoMJ1D3Fpjze","colab_type":"code","colab":{}},"source":["\n","def optimize_local_q_dist(logposterior, hyper_config, x, q):\n","\n","    B = x.size()[0] #batch size\n","    P = 100\n","\n","    z_size = hyper_config['z_size']\n","    x_size = hyper_config['x_size']\n","    if torch.cuda.is_available():\n","        dtype = torch.cuda.FloatTensor\n","    else:\n","        dtype = torch.FloatTensor\n","        \n","\n","\n","    if hyper_config['planar_flow']:\n","        w1 = Variable((torch.zeros(B, z_size).normal_()).type(dtype), requires_grad=True)\n","        w2 = Variable((torch.zeros(B, z_size).normal_()).type(dtype), requires_grad=True)\n","        u1 = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n","        u2 = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n","        b1 = Variable(torch.zeros(B, 1).type(dtype), requires_grad=True)\n","        b2 = Variable(torch.zeros(B, 1).type(dtype), requires_grad=True)\n","        mean = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n","        logvar = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n","\n","        params = [mean, logvar, w1, u1, w2, u2, b1, b2]\n","\n","    else:\n","        mean = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n","        logvar = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n","\n","        params = [mean, logvar]\n","\n","\n","    for aaa in q.parameters():\n","        params.append(aaa)\n","\n","\n","    optimizer = optim.Adam(params, lr=.001)\n","\n","    last_100 = []\n","    best_last_100_avg = -1\n","    consecutive_worse = 0\n","    for epoch in range(1, 999999):\n","\n","\n","\n","        if hyper_config['planar_flow']:\n","           z, logqz = q.sample(mean, logvar, P, w1, u1, b1, w2, u2, b2)\n","        else:\n","           z, logqz = q.sample(mean, logvar, P)\n","\n","        logpx = logposterior(z)\n","\n","        optimizer.zero_grad()\n","\n","\n","        loss = -(torch.mean(logpx-logqz)) \n","        loss_np = loss.data.cpu().numpy()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        last_100.append(loss_np)\n","        if epoch % 100 ==0:\n","\n","            last_100_avg = np.mean(last_100)\n","            if last_100_avg< best_last_100_avg or best_last_100_avg == -1:\n","                consecutive_worse=0\n","                best_last_100_avg = last_100_avg\n","            else:\n","                consecutive_worse +=1 \n","                # print(consecutive_worse)\n","                if consecutive_worse> 10:\n","                    # print ('done')\n","                    break\n","\n","            if epoch % 2000 ==0:\n","                print (epoch, last_100_avg, consecutive_worse)\n","            # print (torch.mean(logpx))\n","\n","            last_100 = []\n","\n","\n","\n","    # Compute VAE and IWAE bounds\n","\n","\n","    if hyper_config['planar_flow']:\n","      z, logqz = q.sample(mean, logvar, 5000, w1, u1, b1, w2, u2, b2)\n","    else:\n","      z, logqz = q.sample(mean, logvar, 5000)\n","\n","\n","    logpx = logposterior(z)\n","\n","    elbo = logpx-logqz #[P,B]\n","    vae = torch.mean(elbo)\n","\n","    max_ = torch.max(elbo, 0)[0] #[B]\n","    elbo_ = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n","    iwae = torch.mean(elbo_)\n","\n","    return vae, iwae"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JbszgNTdFkew","colab_type":"text"},"source":["# Function for computing AIS estimation of log(p(x))\n","\n","This whole piece of code is directly copied from all the codes present in ais_4.py file of C. Cremer's Github repository for Cremer et al. (2018)."]},{"cell_type":"code","metadata":{"id":"pTeRLuH-GkWI","colab_type":"code","colab":{}},"source":["\n","\n","# This one samples the prior distribution\n","\n","# Use this to time everytthing\n","\n","\n","import math\n","import torch\n","from torch.autograd import Variable\n","import numpy as np\n","\n","\n","\n","import time\n","\n","\n","def test_ais(model, data_x, batch_size, display, k, n_intermediate_dists):\n","\n","\n","    def intermediate_dist(t, z, mean, logvar, zeros, batch):\n","        # logp1 = lognormal(z, mean, logvar)  #[P,B]\n","        log_prior = lognormal2(z, zeros, zeros)  #[P,B]\n","        log_likelihood = log_bernoulli(model.generator.decode(z), batch)\n","        # logpT = log_prior + log_likelihood\n","        # log_intermediate_2 = (1-float(t))*logp1 + float(t)*logpT\n","\n","        log_intermediate_2 = log_prior + float(t)*log_likelihood\n","\n","        return log_intermediate_2\n","\n","\n","    def hmc(z, intermediate_dist_func):\n","\n","        if torch.cuda.is_available():\n","            v = Variable(torch.FloatTensor(z.size()).normal_(), volatile=volatile_, requires_grad=requires_grad).cuda()\n","        else:\n","            v = Variable(torch.FloatTensor(z.size()).normal_()) \n","\n","        v0 = v\n","        z0 = z\n","\n","        # print (intermediate_dist_func(z))\n","        # fasdf\n","        gradients = torch.autograd.grad(outputs=intermediate_dist_func(z), inputs=z,\n","                          grad_outputs=grad_outputs,\n","                          create_graph=True, retain_graph=retain_graph, only_inputs=True)[0]\n","\n","        gradients = gradients.detach()\n","\n","        v = v + .5 *step_size*gradients\n","        z = z + step_size*v\n","\n","        for LF_step in range(n_HMC_steps):\n","\n","            # log_intermediate_2 = intermediate_dist(t1, z, mean, logvar, zeros, batch)\n","            gradients = torch.autograd.grad(outputs=intermediate_dist_func(z), inputs=z,\n","                              grad_outputs=grad_outputs,\n","                              create_graph=True, retain_graph=retain_graph, only_inputs=True)[0]\n","            gradients = gradients.detach()\n","            v = v + step_size*gradients\n","            z = z + step_size*v\n","\n","        # log_intermediate_2 = intermediate_dist(t1, z, mean, logvar, zeros, batch)\n","        gradients = torch.autograd.grad(outputs=intermediate_dist_func(z), inputs=z,\n","                          grad_outputs=grad_outputs,\n","                          create_graph=True, retain_graph=retain_graph, only_inputs=True)[0]\n","        gradients = gradients.detach()\n","        v = v + .5 *step_size*gradients\n","\n","        return z0, v0, z, v\n","\n","\n","    def mh_step(z0, v0, z, v, step_size, intermediate_dist_func):\n","\n","        logpv0 = lognormal2(v0, zeros, zeros) #[P,B]\n","        hamil_0 =  intermediate_dist_func(z0) + logpv0\n","        \n","        logpvT = lognormal2(v, zeros, zeros) #[P,B]\n","        hamil_T = intermediate_dist_func(z) + logpvT\n","\n","        accept_prob = torch.exp(hamil_T - hamil_0)\n","\n","        if torch.cuda.is_available():\n","            rand_uni = Variable(torch.FloatTensor(accept_prob.size()).uniform_(), volatile=volatile_, requires_grad=requires_grad).cuda()\n","        else:\n","            rand_uni = Variable(torch.FloatTensor(accept_prob.size()).uniform_())\n","\n","        accept = accept_prob > rand_uni\n","\n","        if torch.cuda.is_available():\n","            accept = accept.type(torch.FloatTensor).cuda()\n","        else:\n","            accept = accept.type(torch.FloatTensor)\n","        \n","        accept = accept.view(k, int(model.B), 1)\n","\n","        z = (accept * z) + ((1-accept) * z0)\n","\n","        #Adapt step size\n","        avg_acceptance_rate = torch.mean(accept)\n","\n","        if avg_acceptance_rate.cpu().data.numpy() > .65:\n","            step_size = 1.02 * step_size\n","        else:\n","            step_size = .98 * step_size\n","\n","        if step_size < 0.0001:\n","            step_size = 0.0001\n","        if step_size > 0.5:\n","            step_size = 0.5\n","\n","        return z, step_size\n","\n","\n","\n","\n","    # n_intermediate_dists = 10\n","    n_HMC_steps = 10\n","    step_size = .1\n","\n","    retain_graph = False\n","    volatile_ = False\n","    requires_grad = False\n","\n","    time_ = time.time()\n","\n","    logws = []\n","    data_index= 0\n","    for i in range(int(len(data_x)/ batch_size)):\n","\n","        #AIS\n","\n","        schedule = np.linspace(0.,1.,n_intermediate_dists)\n","        model.B = batch_size\n","\n","        batch = data_x[data_index:data_index+batch_size]\n","        data_index += batch_size\n","\n","        B = int(model.B)\n","\n","        if torch.cuda.is_available():\n","            batch = Variable(torch.from_numpy(batch).type(model.dtype), volatile=volatile_, requires_grad=requires_grad).cuda()\n","            zeros = Variable(torch.zeros(B, int(model.z_size)).type(model.dtype), volatile=volatile_, requires_grad=requires_grad).cuda() # [B,Z]\n","            logw = Variable(torch.zeros(k, B).type(model.dtype), volatile=True, requires_grad=requires_grad).cuda()\n","            grad_outputs = torch.ones(k, B).cuda()\n","        else:\n","            batch = Variable(torch.from_numpy(batch))\n","            zeros = Variable(torch.zeros(model.B, model.z_size)) # [B,Z]\n","            logw = Variable(torch.zeros(k, model.B))\n","            grad_outputs = torch.ones(k, model.B)\n","\n","\n","        # #Encode x\n","        # mean, logvar = model.encode(batch) #[B,Z]\n","        # #Init z\n","        # z, logpz, logqz = model.sample(mean, logvar, k=k)  #[P,B,Z], [P,B], [P,B]\n","\n","\n","        # z, logqz = model.q_dist.forward(k=k, x=batch, logposterior=model.logposterior)\n","\n","\n","        # z = Variable(torch.FloatTensor(k, model.B, model.z_size).normal_().type(model.dtype),requires_grad=True)\n","\n","        z = Variable(torch.FloatTensor(k, B, model.z_size).normal_().type(model.dtype))\n","\n","        time_2 = time.time()\n","        for (t0, t1) in zip(schedule[:-1], schedule[1:]):\n","\n","\n","            #logw = logw + logpt-1(zt-1) - logpt(zt-1)  t, z, mean, logvar, zeros, batch\n","            # log_intermediate_1 = intermediate_dist(t0, z, mean, logvar, zeros, batch)\n","            # log_intermediate_2 = intermediate_dist(t1, z, mean, logvar, zeros, batch)\n","            log_intermediate_1 = intermediate_dist(t=t0, z=z, mean=zeros, logvar=zeros, zeros=zeros, batch=batch)\n","            log_intermediate_2 = intermediate_dist(t=t1, z=z, mean=zeros, logvar=zeros, zeros=zeros, batch=batch)\n","\n","\n","            logw += log_intermediate_2 - log_intermediate_1\n","\n","            # print('ere')\n","\n","            z = z.data\n","\n","            z = Variable(z, requires_grad=True)\n","\n","\n","            #HMC dynamics\n","            # intermediate_dist_func = lambda aaa: intermediate_dist(t1, aaa, mean, logvar, zeros, batch)\n","            intermediate_dist_func = lambda aaa: intermediate_dist(t1, aaa, zeros, zeros, zeros, batch)\n","\n","            # print (t1)\n","            time_1 = time.time()\n","            z0, v0, z, v = hmc(z, intermediate_dist_func)\n","            # print (t0, 'time to do hmc', time.time()-time_1)\n","\n","\n","            #MH step\n","            z, step_size = mh_step(z0, v0, z, v, step_size, intermediate_dist_func)\n","\n","            z = z.detach()\n","\n","\n","\n","\n","\n","\n","\n","\n","        # print ('time to do whole schedule', time.time()-time_2)\n","        # fasd\n","        #log sum exp\n","        max_ = torch.max(logw,0)[0] #[B]\n","        logw = torch.log(torch.mean(torch.exp(logw - max_), 0)) + max_ #[B]\n","\n","        logws.append(torch.mean(logw.cpu()).data.numpy())\n","\n","\n","        if i%display==0:\n","            print (i,len(data_x)/ batch_size, np.mean(logws),step_size, time.time()-time_)\n","\n","    mean_ = np.mean(logws)\n","    print(mean_, 'T:', time.time()-time_)\n","    return mean_\n","     \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37R7gBKQxIv_","colab_type":"text"},"source":["# Loading MNIST (non-binarized)"]},{"cell_type":"code","metadata":{"id":"vgTjPIc9xM1H","colab_type":"code","colab":{}},"source":["import pickle\n","import numpy as np\n","\n","\n","with open('datasets/mnist_non_binarised.pkl' , 'rb') as f:\n","  train_x , valid_x , test_x = pickle.load(f , encoding = 'latin1')\n","\n","\n","\n","train_x = train_x[0]\n","valid_x = valid_x[0]\n","test_x = test_x[0]\n","\n","\n","train_x = np.asarray(train_x)\n","valid_x = np.asarray(valid_x)\n","test_x = np.asarray(test_x)\n","\n","type(train_x)\n","\n","print ('Train', train_x.shape)\n","print ('Valid', valid_x.shape)\n","print ('Test', test_x.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BDLcKCxVYICO","colab_type":"text"},"source":["# Loading MNIST (binarized)"]},{"cell_type":"code","metadata":{"id":"G8Rl9XKN7vgq","colab_type":"code","colab":{}},"source":["import pickle\n","\n","\n","with open('datasets/binMNIST_train.pkl' , 'rb') as f:\n","  train_x = pickle.load(f , encoding = 'latin1')\n","\n","\n","with open('datasets/binMNIST_valid.pkl' , 'rb') as f:\n","  valid_x = pickle.load(f , encoding = 'latin1')\n","\n","\n","with open('datasets/binMNIST_test.pkl' , 'rb') as f:\n","  test_x = pickle.load(f , encoding = 'latin1')\n","\n","print ('Train', train_x.shape)\n","print ('Valid', valid_x.shape)\n","print ('Test', test_x.shape)\n","\n","type(train_x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvWQC8vvA8d0","colab_type":"code","colab":{}},"source":["print(torch.cuda.is_available())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehnPKsLzJfRW","colab_type":"text"},"source":["# Function for training a complete VAE.\n","\n","Modified from Chris Cremer's train_mnist.py file (Cremer et al. (2018)) in the flow_effect_on_amort_exp folder of his Github Code  of the paper. Modifications are made mainly to account for reloading a checkpoint saved. There is also an extension of 5.2 about planar flow written (original work)."]},{"cell_type":"code","metadata":{"id":"_zv3xdtsl8Qe","colab_type":"code","colab":{}},"source":["\n","\n","\n","\n","def train_encoder_and_decoder(model, train_x, test_x, k, batch_size,\n","                    start_at, save_freq, display_epoch, \n","                    path_to_save_variables, ckpt_number):\n","\n","    train_y = torch.from_numpy(np.zeros(len(train_x)))\n","    train_x = torch.from_numpy(train_x).float().type(model.dtype)\n","\n","    train_ = torch.utils.data.TensorDataset(train_x, train_y)\n","    train_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size, shuffle=True)\n","\n","    #IWAE paper training strategy\n","    time_ = time.time()\n","    total_epochs = ckpt_number\n","\n","    i_max = 7\n","\n","    warmup_over_epochs = 100.\n","\n","\n","    all_params = []\n","    for aaa in model.q_dist.parameters():\n","        all_params.append(aaa)\n","    for aaa in model.generator.parameters():\n","        all_params.append(aaa)\n","\n","    print (model.q_dist)\n","    print (model.generator)\n","\n","\n","\n","\n","    for i in range(0,i_max+1):\n","        \n","        number_of_training = sum([3**x for x in list(range(i+1))])\n","\n","        previous_slots_number = number_of_training - 3**i\n","\n","        if ckpt_number < number_of_training and ckpt_number < previous_slots_number:\n","\n","\n","          lr = .001 * 10**(-i/float(i_max))\n","          print (i, 'LR:', lr)\n","\n","\n","          optimizer = optim.Adam(all_params, lr=lr)\n","\n","\n","          epochs = 3**(i)\n","\n","\n","\n","\n","\n","\n","          for epoch in range(1, epochs + 1):\n","\n","              for batch_idx, (data, target) in enumerate(train_loader):\n","                  \n","                  batch = Variable(data)\n","\n","                  optimizer.zero_grad()\n","\n","                  warmup = total_epochs/warmup_over_epochs\n","                  if warmup > 1.:\n","                    warmup = 1.\n","\n","                  elbo, logpxz, logqz = model.forward(batch, k=k, warmup=warmup)\n","\n","                  loss = -(elbo)\n","                  loss.backward()\n","                  optimizer.step()\n","\n","              total_epochs += 1\n","\n","              if total_epochs%display_epoch==0:\n","                print ('Train Epoch: {}/{}'.format(epoch, epochs),\n","                    'total_epochs {}'.format(total_epochs),\n","                    'LL:{:.3f}'.format(-loss.data[0]),\n","                    'logpxz:{:.3f}'.format(logpxz.data[0]),\n","                    'logqz:{:.3f}'.format(logqz.data[0]),\n","                    'warmup:{:.3f}'.format(warmup),\n","                    'T:{:.2f}'.format(time.time()-time_),\n","                    )\n","                time_ = time.time()\n","\n","              \n","              if total_epochs >= start_at and (total_epochs-start_at)%save_freq==0:\n","\n","                # save params\n","                save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n","                torch.save(model.q_dist.state_dict(), save_file)\n","                print ('saved variables ' + save_file)\n","                save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n","                torch.save(model.generator.state_dict(), save_file)\n","                print ('saved variables ' + save_file)\n","\n","\n","\n","\n","        if ( ckpt_number < number_of_training ) and ( ckpt_number >= previous_slots_number ) :\n","\n","\n","          lr = .001 * 10**(-i/float(i_max))\n","          print (i, 'LR:', lr)\n","\n","\n","          optimizer = optim.Adam(all_params, lr=lr)\n","\n","\n","          epochs = 3**(i)\n","\n","\n","\n","\n","\n","\n","          for epoch in range(1 + ckpt_number - previous_slots_number, epochs + 1):\n","\n","              for batch_idx, (data, target) in enumerate(train_loader):\n","                  \n","                  batch = Variable(data)\n","\n","                  optimizer.zero_grad()\n","\n","                  warmup = total_epochs/warmup_over_epochs\n","                  if warmup > 1.:\n","                    warmup = 1.\n","\n","                  elbo, logpxz, logqz = model.forward(batch, k=k, warmup=warmup)\n","\n","                  loss = -(elbo)\n","                  loss.backward()\n","                  optimizer.step()\n","\n","              total_epochs += 1\n","\n","              if total_epochs%display_epoch==0:\n","                print ('Train Epoch: {}/{}'.format(epoch, epochs),\n","                    'total_epochs {}'.format(total_epochs),\n","                    'LL:{:.3f}'.format(-loss.data[0]),\n","                    'logpxz:{:.3f}'.format(logpxz.data[0]),\n","                    'logqz:{:.3f}'.format(logqz.data[0]),\n","                    'warmup:{:.3f}'.format(warmup),\n","                    'T:{:.2f}'.format(time.time()-time_),\n","                    )\n","                time_ = time.time()\n","\n","              \n","              if total_epochs >= start_at and (total_epochs-start_at)%save_freq==0:\n","\n","                # save params\n","                save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n","                torch.save(model.q_dist.state_dict(), save_file)\n","                print ('saved variables ' + save_file)\n","                save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n","                torch.save(model.generator.state_dict(), save_file)\n","                print ('saved variables ' + save_file)\n","\n","\n","\n","\n","    # save params\n","    save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n","    torch.save(model.q_dist.state_dict(), save_file)\n","    print ('saved variables ' + save_file)\n","    save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n","    torch.save(model.generator.state_dict(), save_file)\n","    print ('saved variables ' + save_file)\n","\n","\n","    print ('done training')\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N7NjHnFSzbGg","colab_type":"text"},"source":["Training the initial encoder of expt 5.3"]},{"cell_type":"code","metadata":{"id":"gR8TsSbWnMnJ","colab_type":"code","colab":{}},"source":["\n","\n","\n","x_size = 784\n","z_size = 50\n","batch_size = 20\n","k = 1\n","#save params \n","start_at = 100\n","save_freq = 100\n","display_epoch = 3\n","\n","hyper_config = { \n","                'x_size': x_size,\n","                'z_size': z_size,\n","                'act_func': F.tanh,# F.relu,\n","                'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n","                'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n","                'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","                'cuda': 1 ,\n","                'hnf': 0,\n","                'planar_flow': 0\n","            }\n","\n","hyper_config['q'] = Gaussian(hyper_config)\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","print('\\nModel:', hyper_config,'\\n')\n","\n","\n","\n","path_to_save_variables='Experiment_3/train_mnist_ckpts/' #.pt'\n","\n","\n","\n","# load generators and encoders\n","# path_to_load_variables_decoder = 'Experiment_3/train_mnist_ckpts/_generator_2200.pt'\n","# path_to_load_variables_encoder = 'Experiment_3/train_mnist_ckpts/_encoder_2200.pt'\n","# model.generator.load_state_dict(torch.load(path_to_load_variables_decoder , map_location=lambda storage, loc: storage))\n","# model.q_dist.load_state_dict(torch.load(path_to_load_variables_encoder , map_location=lambda storage, loc: storage))\n","# print('loaded previous checkpoints (decoder): ' + path_to_load_variables_decoder)\n","# print('loaded previous checkpoints (encoder): ' + path_to_load_variables_encoder)\n","\n","\n","\n","\n","print('\\nTraining')\n","\n","\n","\n","train_encoder_and_decoder(model=model, train_x=train_x, test_x=test_x, k=k, batch_size=batch_size,\n","                    start_at=start_at, save_freq=save_freq, display_epoch=display_epoch, \n","                    path_to_save_variables=path_to_save_variables , ckpt_number = 0)\n","\n","print ('Done.')\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lnApPmbQz8x1","colab_type":"text"},"source":["Extension to 5.2: training VAE with Planar Flow with a different encoder architecture: 784 - 500 - 302."]},{"cell_type":"code","metadata":{"id":"Heoik_8natFv","colab_type":"code","colab":{}},"source":["\n","\n","x_size = 784\n","z_size = 50\n","batch_size = 100\n","k = 1\n","#save params \n","start_at = 100\n","save_freq = 100\n","display_epoch = 3\n","\n","hyper_config = { \n","                'x_size': x_size,\n","                'z_size': z_size,\n","                'act_func': F.elu,# F.relu,\n","                'encoder_arch': [[x_size,500],[500,((z_size*6)+2)]],\n","                'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n","                'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","                'cuda': 1 ,\n","                'hnf': 0,\n","                'planar_flow': 1\n","            }\n","\n","\n","# hyper_config['q'] = Flow1(hyper_config)\n","hyper_config['q'] = Planar_Flow(hyper_config)\n","\n","\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","print('\\nModel:', hyper_config,'\\n')\n","\n","\n","\n","path_to_save_variables='Extension_Planar_Flow/train_mnist_ckpts/' #.pt'\n","\n","\n","\n","# load generators and encoders\n","# path_to_load_variables_decoder = 'Extension_Planar_Flow/train_mnist_ckpts/_generator_1000.pt'\n","# path_to_load_variables_encoder = 'Extension_Planar_Flow/train_mnist_ckpts/_encoder_1000.pt'\n","# model.generator.load_state_dict(torch.load(path_to_load_variables_decoder , map_location=lambda storage, loc: storage))\n","# model.q_dist.load_state_dict(torch.load(path_to_load_variables_encoder , map_location=lambda storage, loc: storage))\n","# print('loaded previous checkpoints (decoder): ' + path_to_load_variables_decoder)\n","# print('loaded previous checkpoints (encoder): ' + path_to_load_variables_encoder)\n","\n","\n","\n","\n","print('\\nTraining')\n","\n","\n","\n","train_encoder_and_decoder(model=model, train_x=train_x, test_x=test_x, k=k, batch_size=batch_size,\n","                    start_at=start_at, save_freq=save_freq, display_epoch=display_epoch, \n","                    path_to_save_variables=path_to_save_variables , ckpt_number = 0)\n","\n","print ('Done.')\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3_4S55T4_IO8","colab_type":"text"},"source":["# Re-training encoders using decoder of trained VAE with FFG\n","\n","Modified from Chris Cremer's train_encoder_only_2.py file (Cremer et al. (2018)) in the folder flow_effect_on_amort_exp of his Github Code of the paper. Some codes in that file have been deleted. Codes has been modified for reloading of checkpoints of training."]},{"cell_type":"code","metadata":{"id":"VrbU7yGe_PNh","colab_type":"code","colab":{}},"source":["\n","\n","def train_encoder_only(model, train_x, test_x, k, batch_size,\n","                    start_at, save_freq, display_epoch, \n","                    path_to_save_variables , ckpt_number):\n","\n","    train_y = torch.from_numpy(np.zeros(len(train_x)))\n","    train_x = torch.from_numpy(train_x).float().type(model.dtype)\n","\n","    train_ = torch.utils.data.TensorDataset(train_x, train_y)\n","    train_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size, shuffle=True)\n","\n","    #IWAE paper training strategy\n","    time_ = time.time()\n","    total_epochs = ckpt_number\n","\n","    i_max = 7\n","\n","    warmup_over_epochs = 100.\n","\n","\n","    all_params = []\n","    for aaa in model.q_dist.parameters():\n","        all_params.append(aaa)\n","    # for aaa in model.generator.parameters():\n","    #     all_params.append(aaa)\n","    # print (len(all_params), 'number of params')\n","\n","    print (model.q_dist)\n","    print (model.generator)\n","\n","\n","\n","    for i in range(0,i_max+1):\n","\n","        number_of_training = sum([3**x for x in list(range(i+1))])\n","\n","        previous_slots_number = number_of_training - 3**i\n","\n","        if ckpt_number < number_of_training and ckpt_number < previous_slots_number:\n","\n","          lr = .001 * 10**(-i/float(i_max))\n","          print (i, 'LR:', lr)\n","\n","          optimizer = optim.Adam(all_params, lr=lr)\n","\n","          epochs = 3**(i)\n","\n","          for epoch in range(1, epochs + 1):\n","\n","             for batch_idx, (data, target) in enumerate(train_loader):\n","\n","                 batch = Variable(data)\n","\n","                 optimizer.zero_grad()\n","\n","                 warmup = total_epochs/warmup_over_epochs\n","                 if warmup > 1.:\n","                     warmup = 1.\n","\n","                 elbo, logpxz, logqz = model.forward(batch, k=k, warmup=warmup)\n","\n","                 loss = -(elbo)\n","                 loss.backward()\n","                 optimizer.step()\n","\n","             total_epochs += 1\n","\n","\n","             if total_epochs%display_epoch==0:\n","                 print ('Train Epoch: {}/{}'.format(epoch, epochs),\n","                    'total_epochs {}'.format(total_epochs),\n","                    'LL:{:.3f}'.format(-loss.data[0]),\n","                    'logpxz:{:.3f}'.format(logpxz.data[0]),\n","                    'logqz:{:.3f}'.format(logqz.data[0]),\n","                    'warmup:{:.3f}'.format(warmup),\n","                    'T:{:.2f}'.format(time.time()-time_),\n","                    )\n","                 time_ = time.time()\n","\n","\n","             if total_epochs >= start_at and (total_epochs-start_at)%save_freq==0:\n","\n","                 # save params\n","                 save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n","                 torch.save(model.q_dist.state_dict(), save_file)\n","                 print ('saved variables ' + save_file)\n","                 # save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n","                 # torch.save(model.generator.state_dict(), save_file)\n","                 # print ('saved variables ' + save_file)\n","\n","\n","\n","\n","        if ckpt_number < number_of_training and ckpt_number >= previous_slots_number:\n","\n","          lr = .001 * 10**(-i/float(i_max))\n","          print (i, 'LR:', lr)\n","\n","          optimizer = optim.Adam(all_params, lr=lr)\n","\n","          epochs = 3**(i)\n","\n","          for epoch in range(1 + ckpt_number - previous_slots_number , epochs + 1):\n","\n","             for batch_idx, (data, target) in enumerate(train_loader):\n","\n","                 batch = Variable(data)\n","\n","                 optimizer.zero_grad()\n","\n","                 warmup = total_epochs/warmup_over_epochs\n","                 if warmup > 1.:\n","                     warmup = 1.\n","\n","                 elbo, logpxz, logqz = model.forward(batch, k=k, warmup=warmup)\n","\n","                 loss = -(elbo)\n","                 loss.backward()\n","                 optimizer.step()\n","\n","             total_epochs += 1\n","\n","\n","             if total_epochs%display_epoch==0:\n","                 print ('Train Epoch: {}/{}'.format(epoch, epochs),\n","                    'total_epochs {}'.format(total_epochs),\n","                    'LL:{:.3f}'.format(-loss.data[0]),\n","                    'logpxz:{:.3f}'.format(logpxz.data[0]),\n","                    'logqz:{:.3f}'.format(logqz.data[0]),\n","                    'warmup:{:.3f}'.format(warmup),\n","                    'T:{:.2f}'.format(time.time()-time_),\n","                    )\n","                 time_ = time.time()\n","\n","\n","             if total_epochs >= start_at and (total_epochs-start_at)%save_freq==0:\n","\n","                 # save params\n","                 save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n","                 torch.save(model.q_dist.state_dict(), save_file)\n","                 print ('saved variables ' + save_file)\n","                 # save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n","                 # torch.save(model.generator.state_dict(), save_file)\n","                 # print ('saved variables ' + save_file)\n","\n","\n","\n","\n","\n","    # save params\n","    save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n","    torch.save(model.q_dist.state_dict(), save_file)\n","    print ('saved variables ' + save_file)\n","    # save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n","    # torch.save(model.generator.state_dict(), save_file)\n","    # print ('saved variables ' + save_file)\n","\n","\n","    print ('done training')\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UeNckMFO6_Il","colab_type":"text"},"source":["Code to run to re-train encoders on the decoder of the initial VAE. 3 points to note:\n","\n","1. hyper_config['q'] (approximate posterior) has to be chosen\n","\n","2. 'encoder_arch' in hyper_config has to be chosen, to change the number of hidden layers\n","\n","3. path_to_save_variables (folder to save training checkpoints) has to be chosen for different experiments."]},{"cell_type":"code","metadata":{"id":"pFkO5rnflaAi","colab_type":"code","colab":{}},"source":["\n","\n","x_size = 784\n","z_size = 50\n","batch_size = 20\n","k = 1\n","#save params \n","start_at = 100\n","save_freq = 100\n","display_epoch = 3\n","\n","\n","\n","\n","\n","#flow1\n","hyper_config = { \n","                'x_size': x_size,\n","                'z_size': z_size,\n","                'act_func': F.tanh,  #F.elu, #,# F.relu,\n","                # 'encoder_arch': [[x_size,200],[200,z_size*2]],\n","                'encoder_arch': [[x_size,z_size*2]],\n","                'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n","                'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","                'cuda': 1,\n","                'hnf': 0,\n","                'planar_flow':0\n","            }\n","\n","# hyper_config['q'] = Flow(hyper_config)\n","# hyper_config['q'] = Flow1(hyper_config)\n","# hyper_config['q'] = Four_Flow1(hyper_config)\n","hyper_config['q'] = Gaussian(hyper_config)\n","\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","print('\\nModel:', hyper_config,'\\n')\n","\n","\n","\n","\n","\n","# path_to_load_variables=''\n","# path_to_save_variables = 'Experiment_3/Encoder_1Hidden_FFG_ckpts/'\n","# path_to_save_variables = 'Experiment_3/Encoder_1Hidden_Flow1_ckpts/'\n","path_to_save_variables = 'Experiment_3/Encoder_0Hidden_FFG_ckpts/'\n","# path_to_save_variables = 'Experiment_3/Encoder_0Hidden_Flow_ckpts/'\n","# path_to_save_variables = 'Experiment_3/Encoder_0Hidden_Flow1_ckpts/'\n","# path_to_save_variables = 'Experiment_3/Encoder_0Hidden_4_Flow1_ckpts/'\n","# path_to_save_variables = 'Chris/encoder_only_Flow1/' #.pt'\n","\n","\n","\n","\n","\n","# load generator\n","print ('Load params for decoder')\n","path_to_load_variables_generator = 'Experiment_3/train_mnist_ckpts/_generator_3280.pt'\n","model.generator.load_state_dict(torch.load(path_to_load_variables_generator, map_location=lambda storage, loc: storage))\n","print ('loaded variables ' + path_to_load_variables_generator)\n","# print ()\n","\n","\n","# load encoder\n","# print ('Load params for encoder')\n","# path_to_load_variables_encoder = 'Experiment_3/Encoder_1Hidden_FFG_ckpts/_encoder_2000.pt'\n","# model.q_dist.load_state_dict(torch.load(path_to_load_variables_encoder, map_location=lambda storage, loc: storage))\n","# print ('loaded variables ' + path_to_load_variables_encoder)\n","# print ()\n","\n","\n","\n","\n","print('\\nTraining')\n","\n","\n","train_encoder_only(model=model, train_x=train_x, test_x=test_x, k=k, batch_size=batch_size,\n","                    start_at=start_at, save_freq=save_freq, display_epoch=display_epoch, \n","                    path_to_save_variables=path_to_save_variables , ckpt_number = 0)\n","\n","print ('Done.')\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_LloRMrLKkAK","colab_type":"text"},"source":["# Compute Approximation and Amortization errors\n","\n","Modified from Chris Cremer's compute_gaps.py file (Cremer et al. (2018)) in the folder flow_effect_on_amort_exp of his Github Code of the paper. Just a few codes in that file have been deleted. The original code in compute_gaps.py for evaluating L(q^star) been adapted to be a function, to make re-loading of L(q^star) values for previous datapoints possible.\n","\n","test is the function for computing IWAE bound. test_vae is the function for computing L(q) - amortized ELBO bound.\n"]},{"cell_type":"code","metadata":{"id":"aZanthpNnOM5","colab_type":"code","colab":{}},"source":["\n","\n","def test_vae(model, data_x, batch_size, display, k):\n","    \n","    time_ = time.time()\n","    elbos = []\n","    data_index= 0\n","    for i in range(int(len(data_x)/ batch_size)):\n","\n","        batch = data_x[data_index:data_index+batch_size]\n","        data_index += batch_size\n","\n","        batch = Variable(torch.from_numpy(batch)).type(model.dtype)\n","\n","        elbo, logpxz, logqz = model.forward2(batch, k=k)\n","\n","        elbos.append(elbo.data[0])\n","\n","\n","\n","    mean_ = np.mean(elbos)\n","\n","\n","    return mean_\n","\n","\n","\n","\n","\n","def test(model, data_x, batch_size, display, k):\n","    \n","    time_ = time.time()\n","    elbos = []\n","    data_index= 0\n","    for i in range(int(len(data_x)/ batch_size)):\n","\n","        batch = data_x[data_index:data_index+batch_size]\n","        data_index += batch_size\n","\n","        batch = Variable(torch.from_numpy(batch)).type(model.dtype)\n","\n","        elbo, logpxz, logqz = model(batch, k=k) # model\n","\n","        elbos.append(elbo.data[0])\n","\n","\n","\n","    mean_ = np.mean(elbos)\n","\n","\n","    return mean_\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z59rHoDtTMmN","colab_type":"text"},"source":["This function is used for computing L(q*)"]},{"cell_type":"code","metadata":{"id":"r_eNZw64jvT5","colab_type":"code","colab":{}},"source":["def compute_local_optimum(n_data , train_x , model , q_dist , \n","                          hyper_config , ckpt_number , directory_to_save_ckpts):\n","\n","    if ckpt_number == 0:\n","       vaes = []\n","       iwaes = []\n","\n","    if ckpt_number > 0:\n","       \n","       path_to_load_ckpts = directory_to_save_ckpts + '/vae_iwae_' + str(ckpt_number) + '.npz'\n","\n","       print('Loading L(q*), IWAE for previous datapoints: ' , path_to_load_ckpts)\n","\n","       previous_data = np.load(path_to_load_ckpts)\n","    \n","       vaes = list(previous_data['vaes'])\n","       iwaes = list(previous_data['iwaes'])\n","       print('Opt vae from checkpoint: ' , np.mean(vaes) , 'Opt iwae from checkpoint: ' , np.mean(iwaes))\n","    \n","\n","    print('optimizing local')\n","\n","    if ckpt_number == 0:\n","       for i in range(0, n_data):\n","\n","         print('Current datapoint: ' , i + 500)\n","\n","         x = train_x[i+500]\n","         x = Variable(torch.from_numpy(x)).type(model.dtype)\n","         x = x.view(1,784)\n","\n","\n","         logposterior = lambda aa: model.logposterior_func2(x=x,z=aa)\n","       \n","         if q_dist == 'Gaussian':\n","           q_local = Gaussian(hyper_config)\n","\n","         if q_dist == 'Flow':\n","           q_local = Flow(hyper_config).cuda()\n","\n","         if q_dist == 'Flow1':\n","           q_local = Flow1(hyper_config).cuda()\n","\n","         if q_dist == 'Four_Flow1':\n","           q_local = Four_Flow1(hyper_config).cuda()\n","\n","         if q_dist == 'Planar_Flow':\n","           q_local = Planar_Flow(hyper_config).cuda()\n","\n","\n","\n","         vae, iwae = optimize_local_q_dist(logposterior, hyper_config, x, q_local)\n","         print (vae.data.cpu().numpy(),iwae.data.cpu().numpy(),'reg')\n","         vaes.append(vae.data.cpu().numpy())\n","         iwaes.append(iwae.data.cpu().numpy())\n","         print ('average of opt vae so far',np.mean(vaes))\n","         print ('average of opt iwae so far',np.mean(iwaes))\n","         print(' ')\n","\n","         if i > ckpt_number and i % 10 == 0:\n","\n","           path_to_save_ckpts = directory_to_save_ckpts + '/vae_iwae_' + str(i)\n","           np.savez(path_to_save_ckpts , vaes=vaes , iwaes=iwaes)\n","           print('checkpoint created. Epoch: ' , i)\n","\n","\n","    if ckpt_number > 0:\n","       for i in range(ckpt_number + 1 , n_data):\n","\n","         print('Current datapoint: ' , i + 500)\n","\n","         x = train_x[i+500]\n","         x = Variable(torch.from_numpy(x)).type(model.dtype)\n","         x = x.view(1,784)\n","\n","\n","         logposterior = lambda aa: model.logposterior_func2(x=x,z=aa)\n","       \n","         if q_dist == 'Gaussian':\n","           q_local = Gaussian(hyper_config)\n","\n","         if q_dist == 'Flow':\n","           q_local = Flow(hyper_config).cuda()\n","\n","         if q_dist == 'Flow1':\n","           q_local = Flow1(hyper_config).cuda()\n","        \n","         if q_dist == 'Four_Flow1':\n","           q_local = Four_Flow1(hyper_config).cuda()\n","\n","         if q_dist == 'Planar_Flow':\n","           q_local = Planar_Flow(hyper_config).cuda()\n","\n","\n","         vae, iwae = optimize_local_q_dist(logposterior, hyper_config, x, q_local)\n","         print (vae.data.cpu().numpy(),iwae.data.cpu().numpy(),'reg')\n","         vaes.append(vae.data.cpu().numpy())\n","         iwaes.append(iwae.data.cpu().numpy())\n","         print ('average of opt vae so far',np.mean(vaes))\n","         print ('average of opt iwae so far',np.mean(iwaes))\n","         print(' ')\n","\n","         if i > ckpt_number and i % 10 == 0:\n","\n","           path_to_save_ckpts = directory_to_save_ckpts + '/vae_iwae_' + str(i)\n","           np.savez(path_to_save_ckpts , vaes=vaes , iwaes=iwaes)\n","           print('checkpoint created. Epoch: ' , i)\n","\n","\n","\n","    path_to_save_ckpts = directory_to_save_ckpts + '/vae_iwae_ending_result'\n","    np.savez(path_to_save_ckpts , vaes=vaes , iwaes=iwaes)\n","    \n","    dddd\n","\n","\n","    print ('Ending opt vae',np.mean(vaes))\n","    print ('Ending opt iwae',np.mean(iwaes))\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p61sZ_VtXAPM","colab_type":"text"},"source":["Compute AIS, IWAE, amortized L(q) of trained models"]},{"cell_type":"code","metadata":{"id":"e4AeZdrgiNv8","colab_type":"code","colab":{}},"source":["\n","\n","\n","\n","x_size = 784\n","z_size = 50\n","# batch_size = 20\n","# k = 1\n","#save params \n","# start_at = 100\n","# save_freq = 100\n","# display_epoch = 3\n","\n","\n","\n","\n","hyper_config = { \n","                'x_size': x_size,\n","                'z_size': z_size,\n","                'act_func': F.tanh,# F.relu,\n","                'encoder_arch': [[x_size,200],[200,z_size*2]],\n","                # 'encoder_arch': [[x_size,z_size*2]],\n","                # 'encoder_arch': [[x_size,500],[500,z_size*2]],\n","                'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n","                'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","                'cuda': 1,\n","                'hnf':0,\n","                'planar_flow': 0\n","                # 'planar_flow': 1\n","            }\n","\n","\n","q = Gaussian(hyper_config)\n","# q = Flow(hyper_config)\n","# q = Flow1(hyper_config)\n","# q = Four_Flow1(hyper_config)\n","# q = Planar_Flow(hyper_config)\n","hyper_config['q'] = q\n","\n","\n","\n","\n","\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","    model.cuda()\n","print('\\nModel:', hyper_config,'\\n')\n","\n","\n","\n","\n","\n","\n","\n","print('load params for decoder')\n","path_to_load_variables_decoder = 'Experiment_3/train_mnist_ckpts/_generator_3280.pt'\n","# path_to_load_variables_decoder = 'Extension_Planar_Flow/train_mnist_ckpts/_generator_3280.pt'\n","model.generator.load_state_dict(torch.load(path_to_load_variables_decoder, map_location=lambda storage, loc: storage))\n","print('loaded variables ' + path_to_load_variables_decoder)\n","\n","\n","\n","print('load params for encoder')\n","path_to_load_variables_encoder = 'Experiment_3/Encoder_Gaussian_ckpts/_encoder_3280.pt'\n","# path_to_load_variables_decoder = 'Extension_Planar_Flow/train_mnist_ckpts/_encoder_3280.pt'\n","# path_to_load_variables_encoder = 'Experiment_3/Encoder_Flow_ckpts/_encoder_3280.pt'\n","# path_to_load_variables_encoder = 'Experiment_3/Encoder_Flow1_ckpts/_encoder_3280.pt'\n","# path_to_load_variables_encoder = 'Experiment_3/Encoder_4_Flow1_ckpts/_encoder_3280.pt'\n","# path_to_load_variables_encoder = 'Experiment_3/Encoder_1Hidden_FFG_ckpts/_encoder_3280.pt'\n","# path_to_load_variables_encoder = 'Experiment_3/Encoder_1Hidden_Flow1_ckpts/_encoder_3280.pt'\n","model.q_dist.load_state_dict(torch.load(path_to_load_variables_encoder, map_location=lambda storage, loc: storage))\n","print('loaded variables ' + path_to_load_variables_encoder)\n","\n","\n","\n","\n","n_data = 1000\n","\n","\n","\n","VAE_train = test_vae(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, 50), display=10, k=5000)\n","IW_train = test(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, 50), display=10, k=5000)\n","print ('amortized VAE',VAE_train)\n","print ('amortized IW',IW_train)\n","\n","\n","\n","\n","\n","\n","AIS_train = test_ais(model=model, data_x=train_x[:n_data], batch_size=n_data, display=2, k=50, n_intermediate_dists=500)\n","print ('AIS_train',AIS_train)\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjIPKNdtXfjD","colab_type":"text"},"source":["Compute L(q*) for a trained decoder"]},{"cell_type":"code","metadata":{"id":"UBKCRy3DkcRz","colab_type":"code","colab":{}},"source":["q_ = 'Gaussian'\n","# q_ = 'Flow'\n","# q_ = 'Flow1'\n","# q_ = 'Four_Flow1'\n","# q_ = 'Planar_Flow'\n","\n","\n","\n","compute_local_optimum(n_data=1000 , train_x=train_x , model=model , q_dist = q_ , \n","                          hyper_config=hyper_config , ckpt_number = 0 , \n","                      directory_to_save_ckpts = 'Experiment_3/Encoder_1Hidden_FFG_ckpts')\n","\n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]}]}