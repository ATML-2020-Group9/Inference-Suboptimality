{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5_4-Change in decoder's size.ipynb","provenance":[{"file_id":"1XbagRb0QRRtsoN0Zu9AU4n36HYtkOzzk","timestamp":1588187699452}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO/VBQYspaHLLGrgoFr9h2y"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x9TRGMwUXeOQ","colab_type":"text"},"source":["# Required terminal commands"]},{"cell_type":"code","metadata":{"id":"7ZRB3D8q-SGy","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vx3G28MV-2C4","colab_type":"code","colab":{}},"source":["%cd /content/drive/My\\ Drive/ATiML\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWZrjk5d-3he","colab_type":"code","colab":{}},"source":["!python --version"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpuE6gCQ-6qs","colab_type":"code","colab":{}},"source":["!pip3 install tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4rK_VAd-8mw","colab_type":"code","colab":{}},"source":["!pip3 install http://download.pytorch.org/whl/cu80/torch-0.2.0.post3-cp36-cp36m-manylinux1_x86_64.whl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qIebOp8-_JH","colab_type":"code","colab":{}},"source":["!pip3 install torchvision==0.2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2sc0UHeP_Ble","colab_type":"code","colab":{}},"source":["!pip3 install visdom"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c9Fhyx_16ZWC","colab_type":"text"},"source":["# Chris Cremer code"]},{"cell_type":"markdown","metadata":{"id":"YkeGpj-M_fzW","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"dpz56ZY1_fRl","colab_type":"code","colab":{}},"source":["import torch\n","from torch.autograd import Variable\n","import torch.utils.data\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import pickle\n","import time\n","import os\n","import math\n","import gzip\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f0A_1JPm-l1x","colab_type":"text"},"source":["# Supplementary files"]},{"cell_type":"code","metadata":{"id":"F5qs6solAJVo","colab_type":"code","colab":{}},"source":["def lognormal(x, mean, logvar):\n","  '''\n","  x: [P,B,Z]\n","  mean,logvar: [B,Z]\n","  output: [P,B]\n","  '''\n","\n","  assert len(x.size()) == 3\n","  assert len(mean.size()) == 2\n","  assert len(logvar.size()) == 2\n","  assert x.size()[1] == mean.size()[0]\n","\n","  D = x.size()[2]\n","\n","  if torch.cuda.is_available():\n","    term1 = D * torch.log(torch.cuda.FloatTensor([2.*math.pi])) #[1]\n","  else:\n","    term1 = D * torch.log(torch.FloatTensor([2.*math.pi])) #[1]\n","\n","\n","  return -.5 * (Variable(term1) + logvar.sum(1) + ((x - mean).pow(2)/torch.exp(logvar)).sum(2))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxMEnTRADssJ","colab_type":"code","colab":{}},"source":["def lognormal333(x, mean, logvar):\n","  '''\n","  x: [P,B,Z]\n","  mean,logvar: [P,B,Z]\n","  output: [P,B]\n","  '''\n","\n","  assert len(x.size()) == 3\n","  assert len(mean.size()) == 3\n","  assert len(logvar.size()) == 3\n","  assert x.size()[0] == mean.size()[0]\n","  assert x.size()[1] == mean.size()[1]\n","\n","  D = x.size()[2]\n","\n","  if torch.cuda.is_available():\n","    term1 = D * torch.log(torch.cuda.FloatTensor([2.*math.pi])) #[1]\n","  else:\n","    term1 = D * torch.log(torch.FloatTensor([2.*math.pi])) #[1]\n","\n","\n","  return -.5 * (Variable(term1) + logvar.sum(2) + ((x - mean).pow(2)/torch.exp(logvar)).sum(2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1-JdS2XAR0Q","colab_type":"code","colab":{}},"source":["def log_bernoulli(pred_no_sig, target):\n","  '''\n","  pred_no_sig is [P, B, X] \n","  t is [B, X]\n","  output is [P, B]\n","  '''\n","\n","  assert len(pred_no_sig.size()) == 3\n","  assert len(target.size()) == 2\n","  assert pred_no_sig.size()[1] == target.size()[0]\n","\n","  return -(torch.clamp(pred_no_sig, min=0)\n","                      - pred_no_sig * target\n","                      + torch.log(1. + torch.exp(-torch.abs(pred_no_sig)))).sum(2) #sum over dimensions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VmMiHG5_Ix5","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","\n","  def __init__(self, hyper_config):\n","    super(Generator, self).__init__()\n","\n","    if hyper_config['cuda']:\n","      self.dtype = torch.cuda.FloatTensor\n","    else:\n","      self.dtype = torch.FloatTensor\n","\n","    self.z_size = hyper_config['z_size']\n","    self.x_size = hyper_config['x_size']\n","    self.act_func = hyper_config['act_func']\n","\n","    #Decoder\n","    self.decoder_weights = []\n","    self.layer_norms = []\n","    for i in range(len(hyper_config['decoder_arch'])):\n","      self.decoder_weights.append(nn.Linear(hyper_config['decoder_arch'][i][0], hyper_config['decoder_arch'][i][1]))\n","\n","    count =1\n","    for i in range(len(self.decoder_weights)):\n","      self.add_module(str(count), self.decoder_weights[i])\n","      count+=1\n","\n","\n","  def decode(self, z):\n","    k = z.size()[0]\n","    B = z.size()[1]\n","    z = z.view(-1, self.z_size)\n","\n","    out = z\n","    for i in range(len(self.decoder_weights)-1):\n","      out = self.act_func(self.decoder_weights[i](out))\n","    # out = self.act_func(self.layer_norms[i].forward(self.decoder_weights[i](out)))\n","    out = self.decoder_weights[-1](out)\n","\n","    x = out.view(k, B, self.x_size)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t06jbl17_dtv","colab_type":"code","colab":{}},"source":["class VAE(nn.Module):\n","  def __init__(self, hyper_config, seed=1):\n","    super(VAE, self).__init__()\n","\n","    torch.manual_seed(seed)\n","\n","\n","    self.z_size = hyper_config['z_size']\n","    self.x_size = hyper_config['x_size']\n","    self.act_func = hyper_config['act_func']\n","\n","\n","    self.q_dist = hyper_config['q_dist'](hyper_config=hyper_config)\n","    # self.q_dist = hyper_config['q_dist'](self, hyper_config=hyper_config)\n","    # print (self.q_dist.parameters())\n","\n","\n","    self.generator = Generator(hyper_config=hyper_config)\n","    # print (self.generator.parameters())\n","    # fasd\n","\n","\n","    # print ('Encoder')\n","    # for aaa in self.q_dist.parameters():\n","    #     # print (aaa)\n","    #     print (aaa.size())\n","    # print ('Decoder')\n","    # for aaa in self.generator.parameters():\n","    #     # print (aaa)\n","    #     print (aaa.size())\n","    # # fasdfs\n","\n","    # if hyper_config['']\n","    # os.environ['CUDA_VISIBLE_DEVICES'] = hyper_config['cuda']\n","\n","\n","    if torch.cuda.is_available():\n","      self.dtype = torch.cuda.FloatTensor\n","      self.q_dist.cuda()\n","    else:\n","      self.dtype = torch.FloatTensor\n","        \n","\n","    # #Decoder\n","    # self.decoder_weights = []\n","    # self.layer_norms = []\n","    # for i in range(len(hyper_config['decoder_arch'])):\n","    #     self.decoder_weights.append(nn.Linear(hyper_config['decoder_arch'][i][0], hyper_config['decoder_arch'][i][1]))\n","\n","    #     # if i != len(hyper_config['decoder_arch'])-1:\n","    #     #     self.layer_norms.append(LayerNorm(hyper_config['decoder_arch'][i][1]))\n","\n","    # count =1\n","    # for i in range(len(self.decoder_weights)):\n","    #     self.add_module(str(count), self.decoder_weights[i])\n","    #     count+=1\n","\n","        # if i != len(hyper_config['decoder_arch'])-1:\n","        #     self.add_module(str(count), self.layer_norms[i])\n","        #     count+=1    \n","\n","    # self.hyper_config = hyper_config\n","\n","    # # See params\n","    # print('all')\n","    # for aaa in self.parameters():\n","    #     # print (aaa)\n","    #     print (aaa.size())\n","    # fsadfsa\n","\n","\n","  # def decode(self, z):\n","  #     k = z.size()[0]\n","  #     B = z.size()[1]\n","  #     z = z.view(-1, self.z_size)\n","\n","  #     out = z\n","  #     for i in range(len(self.decoder_weights)-1):\n","  #         out = self.act_func(self.decoder_weights[i](out))\n","  #         # out = self.act_func(self.layer_norms[i].forward(self.decoder_weights[i](out)))\n","  #     out = self.decoder_weights[-1](out)\n","\n","  #     x = out.view(k, B, self.x_size)\n","  #     return x\n","\n","\n","  def forward(self, x, k, warmup=1.):\n","\n","    self.B = x.size()[0] #batch size\n","    self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","    self.logposterior = lambda aa: lognormal(aa, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(aa), x)\n","\n","    z, logqz = self.q_dist.forward(k, x, self.logposterior)\n","\n","    logpxz = self.logposterior(z)\n","\n","    #Compute elbo\n","    elbo = logpxz - (warmup*logqz) #[P,B]\n","    if k>1:\n","      max_ = torch.max(elbo, 0)[0] #[B]\n","      elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n","        \n","    elbo = torch.mean(elbo) #[1]\n","    logpxz = torch.mean(logpxz) #[1]\n","    logqz = torch.mean(logqz)\n","\n","    return elbo, logpxz, logqz\n","\n","\n","  def sample_q(self, x, k):\n","\n","    self.B = x.size()[0] #batch size\n","    self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","    self.logposterior = lambda aa: lognormal(aa, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(aa), x)\n","\n","    z, logqz = self.q_dist.forward(k=k, x=x, logposterior=self.logposterior)\n","\n","    return z\n","\n","\n","  def logposterior_func(self, x, z):\n","    self.B = x.size()[0] #batch size\n","    self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","    # print (x)  #[B,X]\n","    # print(z)    #[P,Z]\n","    z = Variable(z).type(self.dtype)\n","    z = z.view(-1,self.B,self.z_size)\n","    return lognormal(z, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(z), x)\n","\n","\n","\n","  def logposterior_func2(self, x, z):\n","    self.B = x.size()[0] #batch size\n","    self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","    # print (x)  #[B,X]\n","    # print(z)    #[P,Z]\n","    # z = Variable(z).type(self.dtype)\n","    z = z.view(-1,self.B,self.z_size)\n","\n","    # print (z)\n","    return lognormal(z, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(z), x)\n","\n","\n","\n","  def forward2(self, x, k):\n","\n","    self.B = x.size()[0] #batch size\n","    self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","    self.logposterior = lambda aa: lognormal(aa, self.zeros, self.zeros) + log_bernoulli(self.generator.decode(aa), x)\n","\n","    z, logqz = self.q_dist.forward(k, x, self.logposterior)\n","\n","    logpxz = self.logposterior(z)\n","\n","    #Compute elbo\n","    elbo = logpxz - logqz #[P,B]\n","    # if k>1:\n","    #     max_ = torch.max(elbo, 0)[0] #[B]\n","    #     elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n","        \n","    elbo = torch.mean(elbo) #[1]\n","    logpxz = torch.mean(logpxz) #[1]\n","    logqz = torch.mean(logqz)\n","\n","    return elbo, logpxz, logqz\n","\n","\n","\n","\n","  def forward3_prior(self, x, k):\n","\n","    self.B = x.size()[0] #batch size\n","    self.zeros = Variable(torch.zeros(self.B, self.z_size).type(self.dtype))\n","\n","    self.logposterior = lambda aa:  log_bernoulli(self.generator.decode(aa), x) #+ lognormal(aa, self.zeros, self.zeros)\n","\n","    # z, logqz = self.q_dist.forward(k, x, self.logposterior)\n","\n","    z = Variable(torch.FloatTensor(k, self.B, self.z_size).normal_().type(self.dtype)) #[P,B,Z]\n","\n","    logpxz = self.logposterior(z)\n","\n","    #Compute elbo\n","    elbo = logpxz #- logqz #[P,B]\n","    if k>1:\n","      max_ = torch.max(elbo, 0)[0] #[B]\n","      elbo = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n","        \n","    elbo = torch.mean(elbo) #[1]\n","    # logpxz = torch.mean(logpxz) #[1]\n","    # logqz = torch.mean(logqz)\n","\n","    return elbo#, logpxz, logqz\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","  # def train(self, train_x, k, epochs, batch_size, display_epoch, learning_rate):\n","\n","  #     optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","  #     time_ = time.time()\n","  #     n_data = len(train_x)\n","  #     arr = np.array(range(n_data))\n","\n","  #     for epoch in range(1, epochs + 1):\n","\n","  #         #shuffle\n","  #         np.random.shuffle(arr)\n","  #         train_x = train_x[arr]\n","\n","  #         data_index= 0\n","  #         for i in range(int(n_data/batch_size)):\n","  #             batch = train_x[data_index:data_index+batch_size]\n","  #             data_index += batch_size\n","\n","  #             batch = Variable(torch.from_numpy(batch)).type(self.dtype)\n","  #             optimizer.zero_grad()\n","\n","  #             elbo, logpxz, logqz = self.forward(batch, k=k)\n","\n","  #             loss = -(elbo)\n","  #             loss.backward()\n","  #             optimizer.step()\n","\n","\n","  #         if epoch%display_epoch==0:\n","  #             print ('Train Epoch: {}/{}'.format(epoch, epochs),\n","  #                 'LL:{:.3f}'.format(-loss.data[0]),\n","  #                 'logpxz:{:.3f}'.format(logpxz.data[0]),\n","  #                 # 'logpz:{:.3f}'.format(logpz.data[0]),\n","  #                 'logqz:{:.3f}'.format(logqz.data[0]),\n","  #                 'T:{:.2f}'.format(time.time()-time_),\n","  #                 )\n","\n","  #             time_ = time.time()\n","\n","\n","\n","\n","\n","  # def test(self, data_x, batch_size, display, k):\n","      \n","  #     time_ = time.time()\n","  #     elbos = []\n","  #     data_index= 0\n","  #     for i in range(int(len(data_x)/ batch_size)):\n","\n","  #         batch = data_x[data_index:data_index+batch_size]\n","  #         data_index += batch_size\n","\n","  #         batch = Variable(torch.from_numpy(batch)).type(self.dtype)\n","\n","  #         elbo, logpxz, logqz = self(batch, k=k)\n","\n","  #         elbos.append(elbo.data[0])\n","\n","  #         if i%display==0:\n","  #             print (i,len(data_x)/ batch_size, np.mean(elbos))\n","\n","  #     mean_ = np.mean(elbos)\n","  #     print(mean_, 'T:', time.time()-time_)\n","\n","\n","\n","\n","\n","  # def load_params(self, path_to_load_variables=''):\n","  #     # model.load_state_dict(torch.load(path_to_load_variables))\n","  #     self.load_state_dict(torch.load(path_to_load_variables, map_location=lambda storage, loc: storage))\n","  #     print ('loaded variables ' + path_to_load_variables)\n","\n","\n","  # def save_params(self, path_to_save_variables=''):\n","  #     torch.save(self.state_dict(), path_to_save_variables)\n","  #     print ('saved variables ' + path_to_save_variables)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","  # if __name__ == \"__main__\":\n","\n","  #     load_params = 0\n","  #     train_ = 1\n","  #     eval_IW = 1\n","  #     eval_AIS = 0\n","\n","  #     print ('Loading data')\n","  #     with open(home+'/Documents/MNIST_data/mnist.pkl','rb') as f:\n","  #         mnist_data = pickle.load(f, encoding='latin1')\n","\n","  #     train_x = mnist_data[0][0]\n","  #     valid_x = mnist_data[1][0]\n","  #     test_x = mnist_data[2][0]\n","\n","  #     train_x = np.concatenate([train_x, valid_x], axis=0)\n","\n","  #     print (train_x.shape)\n","\n","  #     x_size = 784\n","  #     z_size = 50\n","\n","  #     hyper_config = { \n","  #                     'x_size': x_size,\n","  #                     'z_size': z_size,\n","  #                     'act_func': F.tanh,# F.relu,\n","  #                     'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n","  #                     'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n","  #                     'q_dist': hnf,#aux_nf,#flow1,#standard,#, #, #, #,#, #,# ,\n","  #                     'n_flows': 2,\n","  #                     'qv_arch': [[x_size,200],[200,200],[200,z_size*2]],\n","  #                     'qz_arch': [[x_size+z_size,200],[200,200],[200,z_size*2]],\n","  #                     'rv_arch': [[x_size+z_size,200],[200,200],[200,z_size*2]],\n","  #                     'flow_hidden_size': 100\n","  #                 }\n","\n","\n","  #     model = VAE(hyper_config)\n","\n","  #     if torch.cuda.is_available():\n","  #         model.cuda()\n","\n","\n","\n","  #     #Train params\n","  #     learning_rate = .0001\n","  #     batch_size = 100\n","  #     epochs = 3000\n","  #     display_epoch = 2\n","  #     k = 1\n","\n","  #     path_to_load_variables=''\n","  #     # path_to_load_variables=home+'/Documents/tmp/pytorch_bvae.pt'\n","  #     path_to_save_variables=home+'/Documents/tmp/pytorch_vae'+str(epochs)+'.pt'\n","  #     # path_to_save_variables=''\n","\n","\n","\n","  #     if load_params:\n","  #         print ('\\nLoading parameters')\n","  #         model.load_params(path_to_save_variables)\n","\n","  #     if train_:\n","\n","  #         print('\\nTraining')\n","  #         print('k='+str(k), 'lr='+str(learning_rate), 'batch_size='+str(batch_size))\n","  #         print('\\nModel:', hyper_config,'\\n')\n","  #         model.train(train_x=train_x, k=k, epochs=epochs, batch_size=batch_size, \n","  #                     display_epoch=display_epoch, learning_rate=learning_rate)\n","  #         model.save_params(path_to_save_variables)\n","\n","\n","  #     if eval_IW:\n","  #         k_IW = 2000\n","  #         batch_size = 20\n","  #         print('\\nTesting with IW, Train set[:10000], B'+str(batch_size)+' k'+str(k_IW))\n","  #         model.test(data_x=train_x[:10000], batch_size=batch_size, display=100, k=k_IW)\n","\n","  #         print('\\nTesting with IW, Test set, B'+str(batch_size)+' k'+str(k_IW))\n","  #         model.test(data_x=test_x, batch_size=batch_size, display=100, k=k_IW)\n","\n","  #     if eval_AIS:\n","  #         k_AIS = 10\n","  #         batch_size = 100\n","  #         n_intermediate_dists = 100\n","  #         print('\\nTesting with AIS, Train set[:10000], B'+str(batch_size)+' k'+str(k_AIS)+' intermediates'+str(n_intermediate_dists))\n","  #         test_ais(model, data_x=train_x[:10000], batch_size=batch_size, display=10, k=k_AIS, n_intermediate_dists=n_intermediate_dists)\n","\n","  #         print('\\nTesting with AIS, Test set, B'+str(batch_size)+' k'+str(k_AIS)+' intermediates'+str(n_intermediate_dists))\n","  #         test_ais(model, data_x=test_x, batch_size=batch_size, display=10, k=k_AIS, n_intermediate_dists=n_intermediate_dists)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kYJrGSh_I9L","colab_type":"code","colab":{}},"source":["class standard(nn.Module):\n","\n","  def __init__(self, hyper_config):\n","    super(standard, self).__init__()\n","\n","    if torch.cuda.is_available():\n","      self.dtype = torch.cuda.FloatTensor\n","    else:\n","      self.dtype = torch.FloatTensor\n","\n","    self.hyper_config = hyper_config\n","\n","    self.z_size = hyper_config['z_size']\n","    self.x_size = hyper_config['x_size']\n","    self.act_func = hyper_config['act_func']\n","\n","\n","    #Encoder\n","    self.encoder_weights = []\n","    self.layer_norms = []\n","    for i in range(len(hyper_config['encoder_arch'])):\n","      self.encoder_weights.append(nn.Linear(hyper_config['encoder_arch'][i][0], hyper_config['encoder_arch'][i][1]))\n","        \n","        # if i != len(hyper_config['encoder_arch'])-1:\n","        #     self.layer_norms.append(LayerNorm(hyper_config['encoder_arch'][i][1]))\n","\n","    count =1\n","    for i in range(len(self.encoder_weights)):\n","      self.add_module(str(count), self.encoder_weights[i])\n","      count+=1\n","\n","        # if i != len(hyper_config['encoder_arch'])-1:\n","        #     self.add_module(str(count), self.layer_norms[i])\n","        #     count+=1         \n","\n","\n","\n","    # self.q = Gaussian(self.hyper_config) #, mean, logvar)\n","    # self.q = Flow(self.hyper_config)#, mean, logvar)\n","    self.q = hyper_config['q']\n","\n","\n","  def forward(self, k, x, logposterior):\n","    '''\n","    k: number of samples\n","    x: [B,X]\n","    logposterior(z) -> [P,B]\n","    '''\n","\n","    self.B = x.size()[0]\n","\n","    #Encode\n","    out = x\n","    for i in range(len(self.encoder_weights)-1):\n","      out = self.act_func(self.encoder_weights[i](out))\n","    # out = self.act_func(self.layer_norms[i].forward(self.encoder_weights[i](out)))\n","\n","    out = self.encoder_weights[-1](out)\n","    mean = out[:,:self.z_size]  #[B,Z]\n","    logvar = out[:,self.z_size:]\n","\n","    # #Sample\n","    # eps = Variable(torch.FloatTensor(k, self.B, self.z_size).normal_().type(self.dtype)) #[P,B,Z]\n","    # z = eps.mul(torch.exp(.5*logvar)) + mean  #[P,B,Z]\n","    # logqz = lognormal(z, mean, logvar) #[P,B]\n","\n","\n","    if 'hnf' in self.hyper_config:\n","      z, logqz = self.q.sample(mean, logvar, k, logposterior)\n","    else:\n","      z, logqz = self.q.sample(mean, logvar, k)\n","\n","    return z, logqz\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xeO6N6UA_JJB","colab_type":"code","colab":{}},"source":["class Gaussian(nn.Module):\n","\n","  def __init__(self, hyper_config): #, mean, logvar):\n","    #mean,logvar: [B,Z]\n","    super(Gaussian, self).__init__()\n","\n","    if torch.cuda.is_available():\n","      self.dtype = torch.cuda.FloatTensor\n","    else:\n","      self.dtype = torch.FloatTensor\n","\n","    \n","\n","    # self.B = mean.size()[0]\n","    # # self.z_size = mean.size()[1]\n","    self.z_size = hyper_config['z_size']\n","    self.x_size = hyper_config['x_size']\n","    # # dfas\n","\n","    # self.mean = mean\n","    # self.logvar = logvar\n","\n","\n","  def sample(self, mean, logvar, k):\n","\n","    self.B = mean.size()[0]\n","\n","    eps = Variable(torch.FloatTensor(k, self.B, self.z_size).normal_().type(self.dtype)) #[P,B,Z]\n","    z = eps.mul(torch.exp(.5*logvar)) + mean  #[P,B,Z]\n","    logqz = lognormal(z, mean, logvar) #[P,B]\n","\n","    return z, logqz\n","\n","\n","\n","  def logprob(self, z, mean, logvar):\n","\n","  # self.B = mean.size()[0]\n","\n","  # eps = Variable(torch.FloatTensor(k, self.B, self.z_size).normal_().type(self.dtype)) #[P,B,Z]\n","  # z = eps.mul(torch.exp(.5*logvar)) + mean  #[P,B,Z]\n","    logqz = lognormal(z, mean, logvar) #[P,B]\n","\n","    return logqz\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIQy_NlcD5RQ","colab_type":"code","colab":{}},"source":["class Flow(nn.Module):\n","\n","  def __init__(self, hyper_config):#, mean, logvar):\n","    #mean,logvar: [B,Z]\n","    super(Flow, self).__init__()\n","\n","    if torch.cuda.is_available():\n","      self.dtype = torch.cuda.FloatTensor\n","    else:\n","      self.dtype = torch.FloatTensor\n","\n","    self.hyper_config = hyper_config\n","    # self.B = mean.size()[0]\n","    self.z_size = hyper_config['z_size']\n","    self.x_size = hyper_config['x_size']\n","\n","    self.act_func = hyper_config['act_func']\n","    \n","\n","    count =1\n","\n","    # f(vT|x,vT)\n","    # rv_arch = [[self.x_size+self.z_size,200],[200,200],[200,self.z_size*2]]\n","    rv_arch = [[self.z_size,50],[50,50],[50,self.z_size*2]]\n","    self.rv_weights = []\n","    for i in range(len(rv_arch)):\n","      layer = nn.Linear(rv_arch[i][0], rv_arch[i][1])\n","      self.rv_weights.append(layer)\n","      self.add_module(str(count), layer)\n","      count+=1\n","\n","\n","    n_flows = 2\n","    self.n_flows = n_flows\n","    h_s = 50\n","\n","    \n","    self.flow_params = []\n","    for i in range(n_flows):\n","    # first is for v, second is for z\n","      self.flow_params.append([\n","                          [nn.Linear(self.z_size, h_s), nn.Linear(h_s, self.z_size), nn.Linear(h_s, self.z_size)],\n","                          [nn.Linear(self.z_size, h_s), nn.Linear(h_s, self.z_size), nn.Linear(h_s, self.z_size)]\n","                          ])\n","    \n","    for i in range(n_flows):\n","\n","      self.add_module(str(count), self.flow_params[i][0][0])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][1][0])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][0][1])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][1][1])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][0][2])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][1][2])\n","      count+=1\n","\n","\n","    # # q(v0)\n","    # self.q_v = Gaussian(self.hyper_config, torch.zeros(self.B, self.z_size), torch.zeros(self.B, self.z_size))\n","\n","    # # q(z0)\n","    # self.q_z = Gaussian(self.hyper_config, mean, logvar)\n","\n","\n","\n","\n","  def norm_flow(self, params, z, v):\n","    # print (z.size())\n","    h = F.tanh(params[0][0](z))\n","    mew_ = params[0][1](h)\n","    # sig_ = F.sigmoid(params[0][2](h)+5.) #[PB,Z]\n","    sig_ = F.sigmoid(params[0][2](h)) #[PB,Z]\n","\n","    v = v*sig_ + mew_\n","    logdet = torch.sum(torch.log(sig_), 1)\n","\n","\n","\n","    h = F.tanh(params[1][0](v))\n","    mew_ = params[1][1](h)\n","    # sig_ = F.sigmoid(params[1][2](h)+5.) #[PB,Z]\n","    sig_ = F.sigmoid(params[1][2](h)) #[PB,Z]\n","    z = z*sig_ + mew_\n","    logdet2 = torch.sum(torch.log(sig_), 1)\n","\n","\n","\n","    #[PB]\n","    logdet = logdet + logdet2\n","    #[PB,Z], [PB]\n","    return z, v, logdet\n","\n","\n","\n","  def sample(self, mean, logvar, k):\n","\n","    self.B = mean.size()[0]\n","    gaus = Gaussian(self.hyper_config)\n","\n","    # q(z0)\n","    z, logqz0 = gaus.sample(mean, logvar, k)\n","\n","    # q(v0)\n","    zeros = Variable(torch.zeros(self.B, self.z_size)).cuda()\n","    v, logqv0 = gaus.sample(zeros, zeros, k)\n","\n","\n","    #[PB,Z]\n","    z = z.view(-1,self.z_size)\n","    v = v.view(-1,self.z_size)\n","\n","    #Transform\n","    logdetsum = 0.\n","    for i in range(self.n_flows):\n","\n","      params = self.flow_params[i]\n","\n","      # z, v, logdet = self.norm_flow([self.flow_params[i]],z,v)\n","      z, v, logdet = self.norm_flow(params,z,v)\n","      logdetsum += logdet\n","\n","    logdetsum = logdetsum.view(k,self.B)\n","\n","    #r(vT|x,zT)\n","    #r(vT|zT)  try that\n","    out = z #[PB,Z]\n","    # print (out.size())\n","    # fasda\n","    for i in range(len(self.rv_weights)-1):\n","      out = self.act_func(self.rv_weights[i](out))\n","    out = self.rv_weights[-1](out)\n","    # print (out)\n","    mean = out[:,:self.z_size]\n","    logvar = out[:,self.z_size:]\n","    # r_vt = Gaussian(self.hyper_config, mean, logvar)\n","\n","\n","\n","    v = v.view(k, self.B, self.z_size)\n","    z = z.view(k, self.B, self.z_size)\n","\n","    mean = mean.contiguous().view(k, self.B, self.z_size)\n","    logvar = logvar.contiguous().view(k, self.B, self.z_size)\n","\n","    # print (mean.size()) #[PB,Z]\n","    # print (v.size())   #[P,B,Z]\n","    # print (self.B)\n","    # print (k)\n","\n","    # logrvT = gaus.logprob(v, mean, logvar)\n","    logrvT = lognormal333(v, mean, logvar)\n","\n","    # print (logqz0.size())\n","    # print (logqv0.size())\n","    # print (logdetsum.size())\n","    # print (logrvT.size())\n","    # fadsf\n","\n","    logpz = logqz0+logqv0-logdetsum-logrvT\n","\n","    return z, logpz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWUqydRb9DAE","colab_type":"code","colab":{}},"source":["class HNF(nn.Module):\n","\n","  def __init__(self, hyper_config):#, mean, logvar):\n","    #mean,logvar: [B,Z]\n","    super(HNF, self).__init__()\n","\n","    if torch.cuda.is_available():\n","      self.dtype = torch.cuda.FloatTensor\n","    else:\n","      self.dtype = torch.FloatTensor\n","\n","    self.hyper_config = hyper_config\n","    # self.B = mean.size()[0]\n","    self.z_size = hyper_config['z_size']\n","    self.x_size = hyper_config['x_size']\n","\n","    self.act_func = hyper_config['act_func']\n","    \n","\n","    count =1\n","\n","    # f(vT|x,vT)\n","    # rv_arch = [[self.x_size+self.z_size,200],[200,200],[200,self.z_size*2]]\n","    rv_arch = [[self.z_size,50],[50,50],[50,self.z_size*2]]\n","    self.rv_weights = []\n","    for i in range(len(rv_arch)):\n","      layer = nn.Linear(rv_arch[i][0], rv_arch[i][1])\n","      self.rv_weights.append(layer)\n","      self.add_module(str(count), layer)\n","      count+=1\n","\n","\n","    n_flows = 2\n","    self.n_flows = n_flows\n","    h_s = 50\n","\n","    \n","    self.flow_params = []\n","    for i in range(n_flows):\n","      #first is for v, second is for z\n","      self.flow_params.append([\n","                          [nn.Linear(self.z_size, h_s), nn.Linear(h_s, self.z_size), nn.Linear(h_s, self.z_size)],\n","                          [nn.Linear(self.z_size, h_s), nn.Linear(h_s, self.z_size), nn.Linear(h_s, self.z_size)]\n","                          ])\n","    \n","    for i in range(n_flows):\n","\n","      self.add_module(str(count), self.flow_params[i][0][0])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][1][0])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][0][1])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][1][1])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][0][2])\n","      count+=1\n","      self.add_module(str(count), self.flow_params[i][1][2])\n","      count+=1\n","\n","\n","    # # q(v0)\n","    # self.q_v = Gaussian(self.hyper_config, torch.zeros(self.B, self.z_size), torch.zeros(self.B, self.z_size))\n","\n","    # # q(z0)\n","    # self.q_z = Gaussian(self.hyper_config, mean, logvar)\n","\n","\n","\n","\n","  def norm_flow(self, params, z, v, logposterior):\n","\n","\n","    h = F.tanh(params[0][0](z))\n","    mew_ = params[0][1](h)\n","    sig_ = F.sigmoid(params[0][2](h)) #[PB,Z]\n","\n","    z_reshaped = z.view(self.P, self.B, self.z_size)\n","    gradients = torch.autograd.grad(outputs=logposterior(z_reshaped), inputs=z_reshaped,\n","                      grad_outputs=self.grad_outputs,\n","                      create_graph=True, retain_graph=True, only_inputs=True)[0]\n","    gradients = gradients.detach()\n","    gradients = gradients.view(-1,self.z_size)\n","\n","    gradients = torch.clamp(torch.abs(gradients), max=1000)\n","\n","\n","\n","    v = v*sig_ + mew_*gradients\n","    logdet = torch.sum(torch.log(sig_), 1)\n","\n","\n","\n","    h = F.tanh(params[1][0](v))\n","    mew_ = params[1][1](h)\n","    sig_ = F.sigmoid(params[1][2](h)) #[PB,Z]\n","    # z = z*sig_ + mew_\n","    z = z*sig_ + mew_  #*v  #which one is better?? this is more like HVI\n","    logdet2 = torch.sum(torch.log(sig_), 1)\n","\n","\n","    \n","    #[PB]\n","    logdet = logdet + logdet2\n","    #[PB,Z], [PB]\n","    return z, v, logdet\n","\n","\n","\n","  def sample(self, mean, logvar, k, logposterior):\n","    \n","    self.P = k\n","    self.B = mean.size()[0]\n","\n","    if torch.cuda.is_available():\n","      self.grad_outputs = torch.ones(k, self.B).cuda()\n","    else:\n","      self.grad_outputs = torch.ones(k, self.B)\n","\n","\n","    gaus = Gaussian(self.hyper_config)\n","\n","    # q(z0)\n","    z, logqz0 = gaus.sample(mean, logvar, k)\n","\n","    # q(v0)\n","    zeros = Variable(torch.zeros(self.B, self.z_size)).cuda()\n","    v, logqv0 = gaus.sample(zeros, zeros, k)\n","\n","\n","    #[PB,Z]\n","    z = z.view(-1,self.z_size)\n","    v = v.view(-1,self.z_size)\n","\n","    #Transform\n","    logdetsum = 0.\n","    for i in range(self.n_flows):\n","\n","      params = self.flow_params[i]\n","\n","      # z, v, logdet = self.norm_flow([self.flow_params[i]],z,v)\n","      z, v, logdet = self.norm_flow(params,z,v, logposterior)\n","      logdetsum += logdet\n","\n","    logdetsum = logdetsum.view(k,self.B)\n","\n","    #r(vT|x,zT)\n","    #r(vT|zT)  try that\n","    out = z #[PB,Z]\n","    # print (out.size())\n","    # fasda\n","    for i in range(len(self.rv_weights)-1):\n","      out = self.act_func(self.rv_weights[i](out))\n","    out = self.rv_weights[-1](out)\n","    # print (out)\n","    mean = out[:,:self.z_size]\n","    logvar = out[:,self.z_size:]\n","    # r_vt = Gaussian(self.hyper_config, mean, logvar)\n","\n","\n","\n","    v = v.view(k, self.B, self.z_size)\n","    z = z.view(k, self.B, self.z_size)\n","\n","    mean = mean.contiguous().view(k, self.B, self.z_size)\n","    logvar = logvar.contiguous().view(k, self.B, self.z_size)\n","\n","    # print (mean.size()) #[PB,Z]\n","    # print (v.size())   #[P,B,Z]\n","    # print (self.B)\n","    # print (k)\n","\n","    # logrvT = gaus.logprob(v, mean, logvar)\n","    logrvT = lognormal333(v, mean, logvar)\n","\n","    # print (logqz0.size())\n","    # print (logqv0.size())\n","    # print (logdetsum.size())\n","    # print (logrvT.size())\n","    # fadsf\n","\n","\n","\n","\n","    logpz = logqz0+logqv0-logdetsum-logrvT\n","\n","    return z, logpz\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FskO0-eJIvl5","colab_type":"code","colab":{}},"source":["def optimize_local_q_dist(logposterior, hyper_config, x, q):\n","\n","  B = x.size()[0] #batch size\n","  P = 50\n","\n","  z_size = hyper_config['z_size']\n","  x_size = hyper_config['x_size']\n","  if torch.cuda.is_available():\n","    dtype = torch.cuda.FloatTensor\n","  else:\n","    dtype = torch.FloatTensor\n","      \n","  mean = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n","  logvar = Variable(torch.zeros(B, z_size).type(dtype), requires_grad=True)\n","\n","  params = [mean, logvar]\n","  for aaa in q.parameters():\n","    params.append(aaa)\n","\n","\n","  optimizer = optim.Adam(params, lr=.001)\n","\n","  last_100 = []\n","  best_last_100_avg = -1\n","  consecutive_worse = 0\n","  for epoch in range(1, 999999):\n","\n","    # #Sample\n","    # eps = Variable(torch.FloatTensor(P, B, model.z_size).normal_().type(model.dtype)) #[P,B,Z]\n","    # z = eps.mul(torch.exp(.5*logvar)) + mean  #[P,B,Z]\n","    # logqz = lognormal(z, mean, logvar) #[P,B]\n","\n","    # fsadfad\n","    # z, logqz = q.sample(...)\n","    z, logqz = q.sample(mean, logvar, P)\n","\n","    logpx = logposterior(z)\n","\n","    optimizer.zero_grad()\n","\n","\n","    loss = -(torch.mean(logpx-logqz))\n","    loss_np = loss.data.cpu().numpy()\n","    # print (epoch, loss_np)\n","    # fasfaf\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    last_100.append(loss_np)\n","    if epoch % 100 ==0:\n","\n","      last_100_avg = np.mean(last_100)\n","      if last_100_avg< best_last_100_avg or best_last_100_avg == -1:\n","        consecutive_worse=0\n","        best_last_100_avg = last_100_avg\n","      else:\n","        consecutive_worse +=1 \n","        # print(consecutive_worse)\n","        if consecutive_worse> 10:\n","            # print ('done')\n","            break\n","\n","      if epoch % 2000 ==0:\n","        print (epoch, last_100_avg, consecutive_worse)#,mean)\n","      # print (torch.mean(logpx))\n","\n","      last_100 = []\n","\n","\n","\n","  # Compute VAE and IWAE bounds\n","\n","  # #Sample\n","  # eps = Variable(torch.FloatTensor(1000, B, model.z_size).normal_().type(model.dtype)) #[P,B,Z]\n","  # z = eps.mul(torch.exp(.5*logvar)) + mean  #[P,B,Z]\n","  # logqz = lognormal(z, mean, logvar) #[P,B]\n","  z, logqz = q.sample(mean, logvar, 5000)\n","\n","  # print (logqz)\n","  # fad\n","  logpx = logposterior(z)\n","\n","  elbo = logpx-logqz #[P,B]\n","  vae = torch.mean(elbo)\n","\n","  max_ = torch.max(elbo, 0)[0] #[B]\n","  elbo_ = torch.log(torch.mean(torch.exp(elbo - max_), 0)) + max_ #[B]\n","  iwae = torch.mean(elbo_)\n","\n","  return vae, iwae"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTSyORBc-p8M","colab_type":"text"},"source":["## Train MNIST with decoder of different sizes"]},{"cell_type":"code","metadata":{"id":"zC_pX7xaRum1","colab_type":"code","outputId":"69755a7d-33e5-4608-c54f-766f246d391c","executionInfo":{"status":"ok","timestamp":1585386494131,"user_tz":0,"elapsed":3222,"user":{"displayName":"Benedetta Mussati","photoUrl":"","userId":"03600955075043946033"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#Load data\n","print ('Loading data' )\n","data_location = './datasets/'\n","print(data_location)\n","with open(data_location + 'mnist_non_binarised.pkl', 'rb') as f:\n","  mnist_data = pickle.load(f, encoding='latin1')\n","  train_x = mnist_data[0][0]\n","  valid_x = mnist_data[1][0]\n","  test_x = mnist_data[2][0]\n","print ('Train', train_x.shape)\n","print ('Valid', valid_x.shape)\n","print ('Test', test_x.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data\n","./datasets/\n","Train (50000, 784)\n","Valid (10000, 784)\n","Test (10000, 784)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HVNuv1j_SXFK","colab_type":"code","colab":{}},"source":["def train_encoder_and_decoder(model, train_x, test_x, k, batch_size,\n","                    start_at, save_freq, display_epoch, \n","                    path_to_save_variables):\n","\n","  \n","  train_y = torch.from_numpy(np.zeros(len(train_x)))\n","  train_x = torch.from_numpy(train_x).float().type(model.dtype)\n","\n","  train_ = torch.utils.data.TensorDataset(train_x, train_y)\n","  train_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size, shuffle=True)\n","\n","  #IWAE paper training strategy\n","  time_ = time.time()\n","  total_epochs = 0\n","\n","  i_max = 7\n","\n","  warmup_over_epochs = 100.\n","\n","\n","  all_params = []\n","  for aaa in model.q_dist.parameters():\n","      all_params.append(aaa)\n","  for aaa in model.generator.parameters():\n","      all_params.append(aaa)\n","\n","  for i in range(0,i_max+1):\n","\n","      lr = .001 * 10**(-i/float(i_max))\n","      print (i, 'LR:', lr)\n","\n","      optimizer = optim.Adam(all_params, lr=lr)\n","\n","      epochs = 3**(i)\n","\n","      for epoch in range(1, epochs + 1):\n","\n","          for batch_idx, (data, target) in enumerate(train_loader):\n","\n","              batch = Variable(data)#.type(model.dtype)\n","\n","              optimizer.zero_grad()\n","\n","              warmup = total_epochs/warmup_over_epochs\n","              if warmup > 1.:\n","                  warmup = 1.\n","\n","              elbo, logpxz, logqz = model.forward(batch, k=k, warmup=warmup)\n","\n","              loss = -(elbo)\n","              loss.backward()\n","              optimizer.step()\n","\n","          total_epochs += 1\n","\n","\n","          if total_epochs%display_epoch==0:\n","              print ('Train Epoch: {}/{}'.format(epoch, epochs),\n","                  'total_epochs {}'.format(total_epochs),\n","                  'LL:{:.3f}'.format(-loss.data[0]),\n","                  'logpxz:{:.3f}'.format(logpxz.data[0]),\n","                  'logqz:{:.3f}'.format(logqz.data[0]),\n","                  'warmup:{:.3f}'.format(warmup),\n","                  'T:{:.2f}'.format(time.time()-time_),\n","                  )\n","              time_ = time.time()\n","\n","\n","          if total_epochs >= start_at and (total_epochs-start_at)%save_freq==0:\n","\n","              # save params\n","              save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n","              torch.save(model.q_dist.state_dict(), save_file)\n","              print ('saved variables ' + save_file)\n","              save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n","              torch.save(model.generator.state_dict(), save_file)\n","              print ('saved variables ' + save_file)\n","\n","\n","\n","  # save params\n","  save_file = path_to_save_variables+'_encoder_'+str(total_epochs)+'.pt'\n","  torch.save(model.q_dist.state_dict(), save_file)\n","  print ('saved variables ' + save_file)\n","  save_file = path_to_save_variables+'_generator_'+str(total_epochs)+'.pt'\n","  torch.save(model.generator.state_dict(), save_file)\n","  print ('saved variables ' + save_file)\n","\n","\n","  print ('done training')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GjwHEOwf6VGi","colab_type":"code","colab":{}},"source":["# 1039199: Decoder with 4 hidden layers\n","\n","# Which gpu\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n","\n","\n","x_size = 784\n","z_size = 50\n","batch_size = 20\n","k = 1\n","#save params \n","start_at = 100\n","save_freq = 100\n","display_epoch = 3\n","\n","hyper_config = { \n","              'x_size': x_size,\n","              'z_size': z_size,\n","              'act_func': F.tanh,# F.relu,\n","              'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n","              'decoder_arch': [[z_size,200],[200,200],[200,200],[200,200],[200,x_size]],\n","              'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","              'cuda': 1\n","          }\n","\n","\n","q = Gaussian(hyper_config)\n","# q = Flow(hyper_config)\n","hyper_config['q'] = q\n","\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","  model.cuda()\n","\n","print('\\nModel:', hyper_config,'\\n')\n","\n","\n","path_to_save_variables='./Exp4/HiddenLayers4' #.pt\n","\n","try:\n","  os.makedirs(path_to_save_variables)\n","except FileExistsError:\n","  # directory already exists\n","  pass\n","\n","print('\\nTraining')\n","\n","train_encoder_and_decoder(model=model, train_x=train_x, test_x=test_x, k=k, batch_size=batch_size,\n","                  start_at=start_at, save_freq=save_freq, display_epoch=display_epoch, \n","                  path_to_save_variables=path_to_save_variables)\n","\n","print ('Done.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"34g41OnfqbEL","colab_type":"code","colab":{}},"source":["# 1039199: Decoder with 0 hidden layers\n","\n","# Which gpu\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n","\n","\n","x_size = 784\n","z_size = 50\n","batch_size = 20\n","k = 1\n","#save params \n","start_at = 100\n","save_freq = 100\n","display_epoch = 3\n","\n","\n","hyper_config = {\n","                'x_size': x_size,\n","                'z_size': z_size,\n","                'act_func': F.tanh,# F.relu,\n","                'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n","                'decoder_arch': [[z_size,x_size]],\n","                'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","                'cuda': 1\n","            }\n","\n","q = Gaussian(hyper_config)\n","# q = Flow(hyper_config)\n","hyper_config['q'] = q\n","\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","  model.cuda()\n","\n","print('\\nModel:', hyper_config,'\\n')\n","\n","\n","path_to_save_variables='./Exp4/HiddenLayers0' #.pt\n","\n","try:\n","  os.makedirs(path_to_save_variables)\n","except FileExistsError:\n","  # directory already exists\n","  pass\n","\n","print('\\nTraining')\n","\n","train_encoder_and_decoder(model=model, train_x=train_x, test_x=test_x, k=k, batch_size=batch_size,\n","                  start_at=start_at, save_freq=save_freq, display_epoch=display_epoch, \n","                  path_to_save_variables=path_to_save_variables)\n","\n","print ('Done.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFvq7jyUqucr","colab_type":"code","colab":{}},"source":["# 1039199: Decoder with 2 hidden layers\n","\n","# Which gpu\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n","\n","\n","x_size = 784\n","z_size = 50\n","batch_size = 20\n","k = 1\n","#save params \n","start_at = 100\n","save_freq = 100\n","display_epoch = 3\n","\n","hyper_config = { \n","                'x_size': x_size,\n","                'z_size': z_size,\n","                'act_func': F.tanh,# F.relu,\n","                'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n","                'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n","                'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","                'cuda': 1\n","            }\n","\n","q = Gaussian(hyper_config)\n","# q = Flow(hyper_config)\n","hyper_config['q'] = q\n","\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","  model.cuda()\n","\n","print('\\nModel:', hyper_config,'\\n')\n","\n","\n","path_to_save_variables='./Exp4/HiddenLayers2' #.pt\n","\n","try:\n","  os.makedirs(path_to_save_variables)\n","except FileExistsError:\n","  # directory already exists\n","  pass\n","\n","print('\\nTraining')\n","\n","train_encoder_and_decoder(model=model, train_x=train_x, test_x=test_x, k=k, batch_size=batch_size,\n","                  start_at=start_at, save_freq=save_freq, display_epoch=display_epoch, \n","                  path_to_save_variables=path_to_save_variables)\n","\n","print ('Done.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0cu3FrRwI3II","colab_type":"text"},"source":["# Compute gaps"]},{"cell_type":"code","metadata":{"id":"KiWMi-ulGnud","colab_type":"code","colab":{}},"source":["def test_vae(model, data_x, batch_size, display, k):\n","\n","  time_ = time.time()\n","  elbos = []\n","  data_index= 0\n","  for i in range(int(len(data_x)/ batch_size)):\n","\n","    batch = data_x[data_index:data_index+batch_size]\n","    data_index += batch_size\n","\n","    batch = Variable(torch.from_numpy(batch)).type(model.dtype)\n","\n","    elbo, logpxz, logqz = model.forward2(batch, k=k)\n","\n","    elbos.append(elbo.data[0])\n","\n","  mean_ = np.mean(elbos)\n","\n","  return mean_#, time.time()-time_\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbKGmESTG00z","colab_type":"code","colab":{}},"source":["def test(model, data_x, batch_size, display, k):\n","\n","  time_ = time.time()\n","  elbos = []\n","  data_index= 0\n","  for i in range(int(len(data_x)/ batch_size)):\n","\n","    batch = data_x[data_index:data_index+batch_size]\n","    data_index += batch_size\n","\n","    batch = Variable(torch.from_numpy(batch)).type(model.dtype)\n","\n","    elbo, logpxz, logqz = model(batch, k=k)\n","\n","    elbos.append(elbo.data[0])\n","\n","  mean_ = np.mean(elbos)\n","\n","  return mean_#, time.time()-time_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQV7a4P2HM9o","colab_type":"code","outputId":"90be046d-c829-4cc8-bbf0-a72fd604ff5f","executionInfo":{"status":"ok","timestamp":1586875307168,"user_tz":-60,"elapsed":5992,"user":{"displayName":"Benedetta Mussati","photoUrl":"","userId":"03600955075043946033"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["###########################\n","#Load data\n","print ('Loading data' )\n","data_location = './datasets/'\n","with open(data_location + 'mnist_non_binarised.pkl', 'rb') as f:\n","  mnist_data = pickle.load(f, encoding='latin1')\n","train_x = mnist_data[0][0]\n","valid_x = mnist_data[1][0]\n","test_x = mnist_data[2][0]\n","train_x = np.concatenate([train_x, valid_x], axis=0)\n","print ('Train', train_x.shape)\n","print ('Test', test_x.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data\n","Train (60000, 784)\n","Test (10000, 784)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fhfsEnoqTHth","colab_type":"text"},"source":["### compute_local_opt and compute_amort"]},{"cell_type":"code","metadata":{"id":"FWap4OxdI5pV","colab_type":"code","colab":{}},"source":["###########################\n","# Load model. Decoder with 4 hidden layers. compute_local_opt and compute_amort.\n","\n","\n","x_size = 784\n","z_size = 50\n","# batch_size = 20\n","# k = 1\n","#save params \n","# start_at = 100\n","# save_freq = 300\n","# display_epoch = 3\n","\n","\n","# 4 hidden decoder\n","hyper_config = { \n","                'x_size': x_size,\n","                'z_size': z_size,\n","                'act_func': F.tanh,# F.relu,\n","                'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n","                'decoder_arch': [[z_size,200],[200,200],[200,200],[200,200],[200,x_size]],\n","                'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","                'cuda': 1\n","            }\n","\n","\n","\n","q = Gaussian(hyper_config)\n","# q = Flow(hyper_config)\n","hyper_config['q'] = q\n","\n","\n","\n","# Which gpu\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","print ('Load params for decoder')\n","path_to_load_variables= './Exp4/HiddenLayers4_generator_600.pt'\n","\n","model.generator.load_state_dict(torch.load(path_to_load_variables, map_location=lambda storage, loc: storage))\n","\n","compute_local_opt = 1\n","compute_amort = 1\n","\n","\n","if compute_amort:\n","\n","    print ('Load params for encoder')\n","    path_to_load_variables= './Exp4/HiddenLayers4_encoder_600.pt'\n","\n","\n","    model.q_dist.load_state_dict(torch.load(path_to_load_variables, map_location=lambda storage, loc: storage))\n","    print ('loaded variables ' + path_to_load_variables)\n","\n","\n","###########################\n","# For each datapoint, compute L[q], L[q*], log p(x)\n","\n","n_data = 100\n","\n","vaes = []\n","iwaes = []\n","vaes_flex = []\n","iwaes_flex = []\n","\n","\n","\n","if compute_local_opt:\n","    print ('optmizing local')\n","    for i in range(len(train_x[:n_data])):\n","\n","        print (i)\n","\n","        x = train_x[i]\n","        x = Variable(torch.from_numpy(x)).type(model.dtype)\n","        x = x.view(1,784)\n","\n","        logposterior = lambda aa: model.logposterior_func2(x=x,z=aa)\n","\n","        q_local = Gaussian(hyper_config) #, mean, logvar)\n","        vae, iwae = optimize_local_q_dist(logposterior, hyper_config, x, q_local)\n","        print (vae.data.cpu().numpy(),iwae.data.cpu().numpy(),'reg')\n","        vaes.append(vae.data.cpu().numpy())\n","        iwaes.append(iwae.data.cpu().numpy())\n","\n","    print()\n","    print ('opt vae',np.mean(vaes))\n","    print ('opt iwae',np.mean(iwaes))\n","    print()\n","\n","if compute_amort:\n","    VAE_train = test_vae(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, 50), display=10, k=5000)\n","    IW_train = test(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, 50), display=10, k=5000)\n","    print ('amortized VAE',VAE_train)\n","    print ('amortized IW',IW_train)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHQr1D6QToHm","colab_type":"code","colab":{}},"source":["###########################\n","# Load model. Decoder with 2 hidden layers. compute_local_opt and compute_amort.\n","\n","\n","x_size = 784\n","z_size = 50\n","# batch_size = 20\n","# k = 1\n","#save params \n","# start_at = 100\n","# save_freq = 300\n","# display_epoch = 3\n","\n","\n","\n","# 2 hidden decoder\n","hyper_config = { \n","                'x_size': x_size,\n","                'z_size': z_size,\n","                'act_func': F.tanh,# F.relu,\n","                'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n","                'decoder_arch': [[z_size,200],[200,200],[200,x_size]],\n","                'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","                'cuda': 1\n","            }\n","\n","\n","q = Gaussian(hyper_config)\n","# q = Flow(hyper_config)\n","hyper_config['q'] = q\n","\n","\n","# Which gpu\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","\n","print ('Load params for decoder')\n","path_to_load_variables= './Exp4/HiddenLayers2_generator_400.pt'\n","\n","model.generator.load_state_dict(torch.load(path_to_load_variables, map_location=lambda storage, loc: storage))\n","print ('loaded variables ' + path_to_load_variables)\n","print ()\n","\n","\n","\n","compute_local_opt = 1\n","compute_amort = 1\n","\n","\n","if compute_amort:\n","\n","    print ('Load params for encoder')\n","    path_to_load_variables= './Exp4/HiddenLayers2_encoder_400.pt'\n","   \n","    model.q_dist.load_state_dict(torch.load(path_to_load_variables, map_location=lambda storage, loc: storage))\n","    print ('loaded variables ' + path_to_load_variables)\n","\n","###########################\n","# For each datapoint, compute L[q], L[q*], log p(x)\n","\n","# # log it\n","# with open(experiment_log, \"a\") as myfile:\n","#     myfile.write('Checkpoint' +str(ckt)+'\\n')\n","\n","# start_time = time.time()\n","\n","n_data = 100\n","\n","vaes = []\n","iwaes = []\n","vaes_flex = []\n","iwaes_flex = []\n","\n","\n","\n","if compute_local_opt:\n","    print ('optmizing local')\n","    for i in range(len(train_x[:n_data])):\n","\n","        print (i)\n","\n","        x = train_x[i]\n","        x = Variable(torch.from_numpy(x)).type(model.dtype)\n","        x = x.view(1,784)\n","\n","        logposterior = lambda aa: model.logposterior_func2(x=x,z=aa)\n","\n","        q_local = Gaussian(hyper_config) #, mean, logvar)\n","        vae, iwae = optimize_local_q_dist(logposterior, hyper_config, x, q_local)\n","        print (vae.data.cpu().numpy(),iwae.data.cpu().numpy(),'reg')\n","        vaes.append(vae.data.cpu().numpy())\n","        iwaes.append(iwae.data.cpu().numpy())\n","\n","    print()\n","    print ('opt vae',np.mean(vaes))\n","    print ('opt iwae',np.mean(iwaes))\n","    print()\n","\n","if compute_amort:\n","    VAE_train = test_vae(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, 50), display=10, k=5000)\n","    IW_train = test(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, 50), display=10, k=5000)\n","    print ('amortized VAE',VAE_train)\n","    print ('amortized IW',IW_train)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zMy3BxhTXAq","colab_type":"code","colab":{}},"source":["###########################\n","# Load model. Decoder with 0 hidden layers, compute_local_opt and compute_amort.\n","\n","x_size = 784\n","z_size = 50\n","# batch_size = 20\n","# k = 1\n","#save params \n","# start_at = 100\n","# save_freq = 300\n","# display_epoch = 3\n","\n","\n","\n","#no hidden decoder\n","hyper_config = { \n","                'x_size': x_size,\n","                'z_size': z_size,\n","                'act_func': F.tanh,# F.relu,\n","                'encoder_arch': [[x_size,200],[200,200],[200,z_size*2]],\n","                'decoder_arch': [[z_size,x_size]],\n","                'q_dist': standard, #FFG_LN#,#hnf,#aux_nf,#flow1,#,\n","                'cuda': 1\n","            }\n","\n","\n","q = Gaussian(hyper_config)\n","# q = Flow(hyper_config)\n","hyper_config['q'] = q\n","\n","\n","\n","\n","# Which gpu\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n","\n","print ('Init model')\n","model = VAE(hyper_config)\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","\n","print ('Load params for decoder')\n","path_to_load_variables= './Exp4/HiddenLayers0_generator_1600.pt'\n","\n","model.generator.load_state_dict(torch.load(path_to_load_variables, map_location=lambda storage, loc: storage))\n","print ('loaded variables ' + path_to_load_variables)\n","print ()\n","\n","\n","\n","compute_local_opt = 1\n","compute_amort = 1\n","\n","\n","if compute_amort:\n","\n","    print ('Load params for encoder')\n","    path_to_load_variables= './Exp4/HiddenLayers0_encoder_1600.pt'\n","    \n","    model.q_dist.load_state_dict(torch.load(path_to_load_variables, map_location=lambda storage, loc: storage))\n","    print ('loaded variables ' + path_to_load_variables)\n","\n","###########################\n","# For each datapoint, compute L[q], L[q*], log p(x)\n","\n","n_data = 100\n","\n","vaes = []\n","iwaes = []\n","vaes_flex = []\n","iwaes_flex = []\n","\n","\n","\n","if compute_local_opt:\n","    print ('optmizing local')\n","    for i in range(len(train_x[:n_data])):\n","\n","        print (i)\n","\n","        x = train_x[i]\n","        x = Variable(torch.from_numpy(x)).type(model.dtype)\n","        x = x.view(1,784)\n","\n","        logposterior = lambda aa: model.logposterior_func2(x=x,z=aa)\n","        q_local = Gaussian(hyper_config) #, mean, logvar)\n","        \n","        vae, iwae = optimize_local_q_dist(logposterior, hyper_config, x, q_local)\n","        print (vae.data.cpu().numpy(),iwae.data.cpu().numpy(),'reg')\n","        vaes.append(vae.data.cpu().numpy())\n","        iwaes.append(iwae.data.cpu().numpy())\n","\n","    print()\n","    print ('opt vae',np.mean(vaes))\n","    print ('opt iwae',np.mean(iwaes))\n","    print()\n","\n","\n","if compute_amort:\n","    VAE_train = test_vae(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, 50), display=10, k=5000)\n","    IW_train = test(model=model, data_x=train_x[:n_data], batch_size=np.minimum(n_data, 50), display=10, k=5000)\n","    print ('amortized VAE',VAE_train)\n","    print ('amortized IW',IW_train)\n","\n"],"execution_count":0,"outputs":[]}]}