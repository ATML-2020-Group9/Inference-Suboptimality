{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5_2-Optimisers and Batch sizes Expansion.ipynb","provenance":[{"file_id":"1q71OfKEGTsLXHtDFEhHCm8ooDrCK-_Jd","timestamp":1588184471247}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMdSmUD/RCIPLtHkBvR0TJp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0Kg-5zoyLWew","colab_type":"text"},"source":["# Required terminal commands"]},{"cell_type":"code","metadata":{"id":"ey304gzkvSWJ","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AI3g82iYvYkD","colab_type":"code","colab":{}},"source":["%cd /content/drive/My\\ Drive/ATiML\n","!ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PIb1_iriva4n","colab_type":"code","colab":{}},"source":["!python --version"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JXAh466Ov_F6","colab_type":"code","colab":{}},"source":["!pip3 install tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JeTuRJRGwA6-","colab_type":"code","colab":{}},"source":["!pip3 install http://download.pytorch.org/whl/cu80/torch-0.2.0.post3-cp36-cp36m-manylinux1_x86_64.whl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvmOCu1ZwDOO","colab_type":"code","colab":{}},"source":["!pip3 install torchvision==0.2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0y5vuqxGwXRW","colab_type":"text"},"source":["# Files from lxuechen repository"]},{"cell_type":"markdown","metadata":{"id":"15nMfTMwwfPy","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"KetQE8V7whAJ","colab_type":"code","colab":{}},"source":["import argparse\n","\n","from tqdm import tqdm\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import math\n","import numpy as np\n","import numpy.linalg as linalg\n","\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15rhpkLJ6IMN","colab_type":"text"},"source":["## Log functions"]},{"cell_type":"code","metadata":{"id":"RXF5K-4Q6Kni","colab_type":"code","colab":{}},"source":["def log_normal(x, mean=None, logvar=None):\n","    \"\"\"Implementation WITHOUT constant, since the constants in p(z) \n","    and q(z|x) cancels out.\n","    Args:\n","        x: [B,Z]\n","        mean,logvar: [B,Z]\n","\n","    Returns:\n","        output: [B]\n","    \"\"\"\n","    if mean is None:\n","        mean = Variable(torch.zeros(x.size()).type(type(x.data)))\n","    if logvar is None:\n","        logvar = Variable(torch.zeros(x.size()).type(type(x.data)))\n","\n","    return -0.5 * (logvar.sum(1) + ((x - mean).pow(2) / torch.exp(logvar)).sum(1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7w0jpgDj6bW9","colab_type":"code","colab":{}},"source":["def log_bernoulli(logit, target):\n","    \"\"\"\n","    Args:\n","        logit:  [B, X, ?, ?]\n","        target: [B, X, ?, ?]\n","    \n","    Returns:\n","        output:      [B]\n","    \"\"\"\n","    loss = -F.relu(logit) + torch.mul(target, logit) - torch.log(1. + torch.exp( -logit.abs() ))\n","    while len(loss.size()) > 1:\n","        loss = loss.sum(-1)\n","\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4_B_j0b6l9s","colab_type":"code","colab":{}},"source":["def log_mean_exp(x):\n","    max_, _ = torch.max(x, 1, keepdim=True)\n","    return torch.log(torch.mean(torch.exp(x - max_), 1)) + torch.squeeze(max_)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-085PbM0UBw","colab_type":"text"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"zUyiz33-0V_7","colab_type":"code","colab":{}},"source":["import numpy as np\n","from scipy import io\n","import sys\n","import os\n","import time\n","\n","import torch\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.autograd import Variable\n","import collections\n","import pickle\n","from torch.autograd import Variable\n","\n","\n","class Larochelle_MNIST:\n","\n","    def __init__(self, part='train', batch_size=128, partial=1000):\n","        with open('datasets/mnist_non_binarised.pkl', 'rb') as f:\n","            # This is checking if you are using a version of Python < 3.0\n","            if sys.version_info[0] < 3:\n","                mnist = pickle.load(f)\n","            else:\n","                mnist = pickle.load(f, encoding='latin1')\n","            train = np.concatenate((mnist[0][0], mnist[1][0]))\n","            # clunky but this is how we'll turn on the binarising functionality for the moment\n","            # just change the below bool to true if you want to binarise the mnist file\n","            binarise = False\n","            if binarise:\n","              self.data = {\n","                  'train': static_binarise(train),\n","                  'test': static_binarise(mnist[2][0]),\n","                  'partial_train': static_binarise(mnist[0][0][:partial]),\n","                  'partial_test': static_binarise(mnist[2][0][:partial]),\n","              }[part]\n","            else:\n","              self.data = {\n","                  'train': train,\n","                  'test': mnist[2][0],\n","                  'partial_train': mnist[0][0][:partial],\n","                  'partial_test': mnist[2][0][:partial],\n","              }[part]\n","        self.size = self.data.shape[0]\n","        self.batch_size = batch_size\n","        self._construct()\n","\n","    def __iter__(self):\n","        return iter(self.batch_list)\n","\n","    def _construct(self):\n","        self.batch_list = []\n","        for i in range(self.size // self.batch_size):\n","            batch = self.data[self.batch_size*i:self.batch_size*(i+1)]\n","            batch = torch.from_numpy(batch)\n","            # placeholder for second entry\n","            self.batch_list.append((batch, None))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NVMXZVCt4qoO","colab_type":"text"},"source":["## maths_op"]},{"cell_type":"code","metadata":{"id":"NvzYuzt34uPq","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import math\n","import numpy as np\n","import numpy.linalg as linalg\n","\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","\n","\n","def log_normal_full_cov(x, mean, L):\n","    \"\"\"Log density of full covariance multivariate Gaussian.\n","    Note: results are off by the constant log(), since this \n","    quantity cancels out in p(z) and q(z|x).\"\"\"\n","\n","    def batch_diag(M):\n","        diag = [t.diag() for t in torch.functional.unbind(M)]\n","        diag = torch.functional.stack(diag)\n","        return diag\n","\n","    def batch_inverse(M, damp=False, eps=1e-6):\n","        damp_matrix = Variable(torch.eye(M[0].size(0)).type(M.data.type())).mul_(eps)\n","        inverse = []\n","        for t in torch.functional.unbind(M):\n","            # damping to ensure invertible due to float inaccuracy\n","            # this problem is very UNLIKELY when using double\n","            m = t if not damp else t + damp_matrix\n","            inverse.append(m.inverse())\n","        inverse = torch.functional.stack(inverse)\n","        return inverse\n","\n","    L_diag = batch_diag(L)\n","    term1 = -torch.log(L_diag).sum(1)\n","\n","    L_inverse = batch_inverse(L)\n","    scaled_diff = L_inverse.matmul((x - mean).unsqueeze(2)).squeeze()\n","    term2 = -0.5 * (scaled_diff ** 2).sum(1)\n","\n","    return term1 + term2\n","\n","\n","def mean_squared_error(prediction, target):\n","    prediction, target = flatten(prediction), flatten(target)\n","    diff = prediction - target\n","\n","    return -torch.sum(torch.mul(diff, diff), 1)\n","\n","\n","def discretized_logistic(mu, logs, x):\n","    \"\"\"Probability mass follow discretized logistic. \n","    https://arxiv.org/pdf/1606.04934.pdf. Assuming pixel values scaled to be\n","    within [0,1]. Follows implementation from OpenAI.\n","    \"\"\"\n","    sigmoid = torch.nn.Sigmoid()\n","\n","    s = torch.exp(logs).unsqueeze(-1).unsqueeze(-1)\n","    logp = torch.log(sigmoid((x + 1./256. - mu) / s) - sigmoid((x - mu) / s) + 1e-7)\n","\n","    return logp.sum(-1).sum(-1).sum(-1)\n","\n","\n","def flatten(x):\n","    return x.view(x.size(0), -1)\n","\n","\n","def numpy_nan_guard(arr):\n","    return np.all(arr == arr)\n","\n","\n","def safe_repeat(x, n):\n","    return x.repeat(n, *[1 for _ in range(len(x.size()) - 1)])\n","\n","\n","def sigmoidial_schedule(T, delta=4):\n","    \"\"\"From section 6 of BDMC paper.\"\"\"\n","\n","    def sigmoid(x):\n","        return np.exp(x) / (1. + np.exp(x))\n","\n","    def beta_tilde(t):\n","        return sigmoid(delta * (2.*t / T - 1.))\n","\n","    def beta(t):\n","        return (beta_tilde(t) - beta_tilde(1)) / (beta_tilde(T) - beta_tilde(1))\n","\n","    return [beta(t) for t in range(1, T+1)]\n","\n","\n","def linear_schedule(T):\n","    return np.linspace(0., 1., T)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36XknK6T4b8K","colab_type":"text"},"source":["## AIS"]},{"cell_type":"code","metadata":{"id":"j22Xbav74eos","colab_type":"code","colab":{}},"source":["from torch.autograd import Variable\n","from torch.autograd import grad as torchgrad\n","\n","\n","def ais_trajectory(\n","    model,\n","    loader,\n","    mode='forward',\n","    schedule=np.linspace(0., 1., 500),\n","    n_sample=100\n","):\n","    \"\"\"Compute annealed importance sampling trajectories for a batch of data. \n","    Could be used for *both* forward and reverse chain in bidirectional Monte Carlo\n","    (default: forward chain with linear schedule).\n","\n","    Args:\n","        model (vae.VAE): VAE model\n","        loader (iterator): iterator that returns pairs, with first component being `x`,\n","            second would be `z` or label (will not be used)\n","        mode (string): indicate forward/backward chain; must be either `forward` or \n","            'backward' schedule (list or 1D np.ndarray): temperature schedule,\n","            i.e. `p(z)p(x|z)^t`; foward chain has increasing values, whereas\n","            backward has decreasing values\n","        n_sample (int): number of importance samples (i.e. number of parallel chains \n","            for each datapoint)\n","\n","    Returns:\n","        A list where each element is a torch.autograd.Variable that contains the \n","        log importance weights for a single batch of data\n","    \"\"\"\n","\n","    assert mode == 'forward' or mode == 'backward', 'Should have forward/backward mode'\n","\n","    def log_f_i(z, data, t, log_likelihood_fn=log_bernoulli):\n","        \"\"\"Unnormalized density for intermediate distribution `f_i`:\n","            f_i = p(z)^(1-t) p(x,z)^(t) = p(z) p(x|z)^t\n","        =>  log f_i = log p(z) + t * log p(x|z)\n","        \"\"\"\n","        zeros = Variable(torch.zeros(B, z_size).type(mdtype))\n","        log_prior = log_normal(z, zeros, zeros)\n","        log_likelihood = log_likelihood_fn(model.decode(z), data)\n","\n","        return log_prior + log_likelihood.mul_(t)\n","\n","    model.eval()\n","\n","    # shorter aliases\n","    z_size = model.z_size\n","    mdtype = model.dtype\n","\n","    _time = time.time()\n","    logws = []  # for output\n","\n","    print ('In %s mode' % mode)\n","\n","    for i, (batch, post_z) in enumerate(loader):\n","\n","        B = batch.size(0) * n_sample\n","        batch = Variable(batch.type(mdtype))\n","        batch = safe_repeat(batch, n_sample)\n","\n","        # batch of step sizes, one for each chain\n","        epsilon = Variable(torch.ones(B).type(model.dtype)).mul_(0.01)\n","        # accept/reject history for tuning step size\n","        accept_hist = Variable(torch.zeros(B).type(model.dtype))\n","        # record log importance weight; volatile=True reduces memory greatly\n","        logw = Variable(torch.zeros(B).type(mdtype), volatile=True)\n","\n","        # initial sample of z\n","        if mode == 'forward':\n","            current_z = Variable(torch.randn(B, z_size).type(mdtype), requires_grad=True)\n","        else:\n","            current_z = Variable(safe_repeat(post_z, n_sample).type(mdtype), requires_grad=True)\n","\n","        for j, (t0, t1) in tqdm(enumerate(zip(schedule[:-1], schedule[1:]), 1), position=0, leave=True):\n","            # update log importance weight\n","            log_int_1 = log_f_i(current_z, batch, t0)\n","            log_int_2 = log_f_i(current_z, batch, t1)\n","            logw.add_(log_int_2 - log_int_1)\n","\n","            # resample speed\n","            current_v = Variable(torch.randn(current_z.size()).type(mdtype))\n","\n","            def U(z):\n","                return -log_f_i(z, batch, t1)\n","\n","            def grad_U(z):\n","                # grad w.r.t. outputs; mandatory in this case\n","                grad_outputs = torch.ones(B).type(mdtype)\n","                # torch.autograd.grad default returns volatile\n","                grad = torchgrad(U(z), z, grad_outputs=grad_outputs)[0]\n","                # avoid humongous gradients\n","                grad = torch.clamp(grad, -10000, 10000)\n","                # needs variable wrapper to make differentiable\n","                grad = Variable(grad.data, requires_grad=True)\n","                return grad\n","\n","            def normalized_kinetic(v):\n","                zeros = Variable(torch.zeros(B, z_size).type(mdtype))\n","                # this is superior to the unnormalized version\n","                return -log_normal(v, zeros, zeros)\n","\n","            z, v = hmc_trajectory(current_z, current_v, U, grad_U, epsilon)\n","\n","            # accept-reject step\n","            current_z, epsilon, accept_hist = accept_reject(\n","                current_z, current_v,\n","                z, v,\n","                epsilon,\n","                accept_hist, j,\n","                U, K=normalized_kinetic\n","            )\n","\n","        # IWAE lower bound\n","        logw = log_mean_exp(logw.view(n_sample, -1).transpose(0, 1))\n","        if mode == 'backward':\n","            logw = -logw\n","        logws.append(logw.data)\n","\n","        print ('Time elapse %.4f, last batch stats %.4f' % \\\n","            (time.time()-_time, logw.mean().cpu().data.numpy()))\n","\n","        _time = time.time()\n","        sys.stdout.flush()  # for debugging\n","\n","    return logws"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVjmuVyW5GiR","colab_type":"text"},"source":["## hmc trajectory"]},{"cell_type":"code","metadata":{"id":"_eNV35x65I2I","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import sys\n","import os\n","import math\n","import torch\n","import numpy as np\n","\n","import torch\n","from torch.autograd import Variable\n","\n","\n","def hmc_trajectory(current_z, current_v, U, grad_U, epsilon, L=10):\n","    \"\"\"This version of HMC follows https://arxiv.org/pdf/1206.1901.pdf.\n","\n","    Args:\n","        U: function to compute potential energy/minus log-density\n","        grad_U: function to compute gradients w.r.t. U\n","        epsilon: (adaptive) step size\n","        L: number of leap-frog steps\n","        current_z: current position\n","    \"\"\"\n","\n","    # as of `torch-0.3.0.post4`, there still is no proper scalar support\n","    assert isinstance(epsilon, Variable)\n","\n","    eps = epsilon.view(-1, 1)\n","    z = current_z\n","    v = current_v - grad_U(z).mul(eps).mul_(.5)\n","\n","    for i in range(1, L+1):\n","        z = z + v.mul(eps)\n","        if i < L:\n","            v = v - grad_U(z).mul(eps)\n","\n","    v = v - grad_U(z).mul(eps).mul_(.5)\n","    v = -v  # this is not needed; only here to conform to the math\n","\n","    return z.detach(), v.detach()\n","\n","\n","def accept_reject(current_z, current_v, \n","                  z, v, \n","                  epsilon, \n","                  accept_hist, hist_len, \n","                  U, K=lambda v: torch.sum(v * v, 1)):\n","    \"\"\"Accept/reject based on Hamiltonians for current and propose.\n","\n","    Args:\n","        current_z: position BEFORE leap-frog steps\n","        current_v: speed BEFORE leap-frog steps\n","        z: position AFTER leap-frog steps\n","        v: speed AFTER leap-frog steps\n","        epsilon: step size of leap-frog.\n","                (This is only needed for adaptive update)\n","        U: function to compute potential energy (MINUS log-density)\n","        K: function to compute kinetic energy (default: kinetic energy in physics w/ mass=1)\n","    \"\"\"\n","\n","    mdtype = type(current_z.data)\n","\n","    current_Hamil = K(current_v) + U(current_z)\n","    propose_Hamil = K(v) + U(z)\n","\n","    prob = torch.exp(current_Hamil - propose_Hamil)\n","    uniform_sample = torch.rand(prob.size())\n","    uniform_sample = Variable(uniform_sample.type(mdtype))\n","    accept = (prob > uniform_sample).type(mdtype)\n","    z = z.mul(accept.view(-1, 1)) + current_z.mul(1. - accept.view(-1, 1))\n","\n","    accept_hist = accept_hist.add(accept)\n","    criteria = (accept_hist / hist_len > 0.65).type(mdtype)\n","    adapt = 1.02 * criteria + 0.98 * (1. - criteria)\n","    epsilon = epsilon.mul(adapt).clamp(1e-4, .5)\n","\n","    # clear previous history & save memory, similar to detach\n","    z = Variable(z.data, requires_grad=True)\n","    epsilon = Variable(epsilon.data)\n","    accept_hist = Variable(accept_hist.data)\n","\n","    return z, epsilon, accept_hist\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bObpDM1P1bX_","colab_type":"text"},"source":["## VAE model"]},{"cell_type":"code","metadata":{"id":"do7DEdQa1foJ","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import time\n","import sys\n","import argparse\n","\n","import torch\n","import torch.utils.data\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils import weight_norm\n","from torch.autograd import Variable\n","from torch.autograd import grad as torchgrad\n","\n","\n","class VAE(nn.Module):\n","    \"\"\"Generic VAE for MNIST and Fashion datasets.\"\"\"\n","    def __init__(self, hps):\n","        super(VAE, self).__init__()\n","\n","        self.z_size = hps.z_size\n","        self.has_flow = hps.has_flow\n","        self.use_cuda = hps.cuda\n","        self.act_func = hps.act_func\n","        self.n_flows = hps.n_flows\n","        self.hamiltonian_flow = hps.hamiltonian_flow\n","\n","        self._init_layers(wide_encoder=hps.wide_encoder)\n","\n","        if self.use_cuda:\n","            self.cuda()\n","            self.dtype = torch.cuda.FloatTensor\n","            torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","        else:\n","            self.dtype = torch.FloatTensor\n","\n","    def _init_layers(self, wide_encoder=False):\n","        h_s = 500 if wide_encoder else 200\n","\n","        self.fc1 = nn.Linear(784, h_s)  # assume flattened\n","        self.fc2 = nn.Linear(h_s, h_s)\n","        self.fc3 = nn.Linear(h_s, self.z_size*2)\n","\n","        self.fc4 = nn.Linear(self.z_size, 200)\n","        self.fc5 = nn.Linear(200, 200)\n","        self.fc6 = nn.Linear(200, 784)\n","\n","        self.x_info_layer = nn.Linear(200, self.z_size)\n","\n","        if self.has_flow:\n","            self.q_dist = Flow(self, n_flows=self.n_flows)\n","            if self.use_cuda:\n","                self.q_dist.cuda()\n","\n","    def sample(self, mu, logvar, grad_fn=lambda x: 1, x_info=None):\n","        eps = Variable(torch.FloatTensor(mu.size()).normal_().type(self.dtype))\n","        z = eps.mul(logvar.mul(0.5).exp_()).add_(mu)\n","        logqz = log_normal(z, mu, logvar)\n","\n","        if self.has_flow:\n","            z, logprob = self.q_dist.forward(z, grad_fn, x_info)\n","            logqz += logprob\n","\n","        zeros = Variable(torch.zeros(z.size()).type(self.dtype))\n","        logpz = log_normal(z, zeros, zeros)\n","\n","        return z, logpz, logqz\n","\n","    def encode(self, net):\n","        net = self.act_func(self.fc1(net))\n","        net = self.act_func(self.fc2(net))\n","        x_info = self.act_func(self.x_info_layer(net))\n","        net = self.fc3(net)\n","\n","        mean, logvar = net[:, :self.z_size], net[:, self.z_size:]\n","\n","        return mean, logvar, x_info\n","\n","    def decode(self, net):\n","        net = self.act_func(self.fc4(net))\n","        net = self.act_func(self.fc5(net))\n","        logit = self.fc6(net)\n","\n","        return logit\n","\n","    def forward(self, x, k=1, warmup_const=1.):\n","        x = x.repeat(k, 1)\n","        mu, logvar, x_info = self.encode(x)\n","\n","        # posterior-aware inference\n","        def U(z):\n","            logpx = log_bernoulli(self.decode(z), x)\n","            logpz = log_normal(z)\n","            return -logpx - logpz  # energy as -log p(x, z)\n","\n","        def grad_U(z):\n","            grad_outputs = torch.ones(z.size(0)).type(self.dtype)\n","            grad = torchgrad(U(z), z, grad_outputs=grad_outputs, create_graph=True)[0]\n","            # gradient clipping avoid numerical issue\n","            norm = torch.sqrt(torch.norm(grad, p=2, dim=1))\n","            # neither grad clip methods consistently outperforms the other\n","            grad = grad / norm.view(-1, 1)\n","            # grad = torch.clamp(grad, -10000, 10000)\n","            return grad.detach()\n","\n","        if self.hamiltonian_flow:\n","            z, logpz, logqz = self.sample(mu, logvar, grad_fn=grad_U, x_info=x_info)\n","        else:\n","            z, logpz, logqz = self.sample(mu, logvar, x_info=x_info)\n","\n","        logit = self.decode(z)\n","        logpx = log_bernoulli(logit, x)\n","        elbo = logpx + logpz - warmup_const * logqz  # custom warmup\n","\n","        # need correction for Tensor.repeat\n","        elbo = log_mean_exp(elbo.view(k, -1).transpose(0, 1))\n","        elbo = torch.mean(elbo)\n","\n","        logpx = torch.mean(logpx)\n","        logpz = torch.mean(logpz)\n","        logqz = torch.mean(logqz)\n","\n","        return elbo, logpx, logpz, logqz\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"goYumpXQzid-","colab_type":"text"},"source":["## Loaders file"]},{"cell_type":"code","metadata":{"id":"BwdG1PWLzhzt","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","\n","def get_Larochelle_MNIST_loader(batch_size=100, partial=False, num=1000):\n","\n","    if partial:\n","        train_loader = Larochelle_MNIST(part='partial_train', batch_size=batch_size, partial=num)\n","        test_loader = Larochelle_MNIST(part='partial_test')\n","    else:\n","        train_loader = Larochelle_MNIST(part='train', batch_size=batch_size)\n","        test_loader = Larochelle_MNIST(part='test', batch_size=batch_size)\n","    \n","    return train_loader, test_loader\n","\n","\n","def get_loaders(dataset='mnist', evaluate=False, batch_size=100):\n","    if dataset == 'mnist':\n","        train_loader, test_loader = get_Larochelle_MNIST_loader(\n","            batch_size=batch_size,\n","            partial=evaluate, num=1000\n","        )\n","    return train_loader, test_loader\n","\n","\n","def get_model(dataset, hps):\n","    if dataset == 'mnist':\n","        model = VAE(hps)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UVqy7eST0uTc","colab_type":"text"},"source":["## HParams"]},{"cell_type":"code","metadata":{"id":"sABI1k9S0vyo","colab_type":"code","colab":{}},"source":["class HParams(object):\n","\n","    def __init__(self, **kwargs):\n","        self._items = {}\n","        for k, v in kwargs.items():\n","            self._set(k, v)\n","\n","    def _set(self, k, v):\n","        self._items[k] = v\n","        setattr(self, k, v)\n","\n","    def parse(self, str_value):\n","        hps = HParams(**self._items)\n","        for entry in str_value.strip().split(\",\"):\n","            entry = entry.strip()\n","            if not entry:\n","                continue\n","            key, sep, value = entry.partition(\"=\")\n","            if not sep:\n","                raise ValueError(\"Unable to parse: %s\" % entry)\n","            default_value = hps._items[key]\n","            if isinstance(default_value, bool):\n","                hps._set(key, value.lower() == \"true\")\n","            elif isinstance(default_value, int):\n","                hps._set(key, int(value))\n","            elif isinstance(default_value, float):\n","                hps._set(key, float(value))\n","            else:\n","                hps._set(key, value)\n","        return hps\n","\n","def get_default_hparams():\n","    return HParams(\n","        z_size=50,\n","        act_func=F.elu,\n","        has_flow=False,\n","        large_encoder=False,\n","        wide_encoder=False,\n","        cuda=True,\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3BPHJhNSkx7","colab_type":"text"},"source":["## Local FFG"]},{"cell_type":"code","metadata":{"id":"aZXZmHXaSoTr","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import time\n","import sys\n","from tqdm import tqdm\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.utils.data\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","parser = argparse.ArgumentParser(description='local_factorized_gaussian')\n","# action configuration flags\n","parser.add_argument('--no-cuda', '-nc', action='store_true')\n","parser.add_argument('--debug', action='store_true', help='debug mode')\n","\n","# model configuration flags\n","parser.add_argument('--z-size', '-zs', type=int, default=50)\n","parser.add_argument('--batch-size', '-bs', type=int, default=100)\n","parser.add_argument('--eval-path', '-ep', type=str, default='model.pth',\n","                    help='path to load evaluation ckpt (default: model.pth)')\n","parser.add_argument('--dataset', '-d', type=str, default='mnist',\n","                    choices=['mnist', 'fashion', 'cifar'], \n","                    help='dataset to train and evaluate on (default: mnist)')\n","parser.add_argument('--has-flow', '-hf', action='store_true', help='inference uses FLOW')\n","parser.add_argument('--n-flows', '-nf', type=int, default=2, help='number of flows')\n","parser.add_argument('--wide-encoder', '-we', action='store_true',\n","                    help='use wider layer (more hidden units for FC, more channels for CIFAR)')\n","\n","\n","def get_default_hparams():\n","    return HParams(\n","        z_size=args.z_size,\n","        act_func=F.elu,\n","        has_flow=args.has_flow,\n","        n_flows=args.n_flows,\n","        wide_encoder=args.wide_encoder,\n","        cuda=args.cuda,\n","        hamiltonian_flow=False\n","    )\n","\n","\n","def optimize_local_gaussian(\n","    log_likelihood,\n","    model,\n","    data_var,\n","    k=100,\n","    check_every=100,\n","    sentinel_thres=10,\n","    debug=False\n","):\n","    \"\"\"data_var should be (cuda) variable.\"\"\"\n","\n","    B = data_var.size()[0]\n","    z_size = model.z_size\n","\n","    data_var = safe_repeat(data_var, k)\n","    zeros = Variable(torch.zeros(B*k, z_size).type(model.dtype))\n","    mean = Variable(torch.zeros(B*k, z_size).type(model.dtype), requires_grad=True)\n","    logvar = Variable(torch.zeros(B*k, z_size).type(model.dtype), requires_grad=True)\n","\n","    optimizer = optim.Adam([mean, logvar], lr=1e-3)\n","    best_avg, sentinel, prev_seq = 999999, 0, []\n","\n","    # perform local opt\n","    time_ = time.time()\n","    for epoch in range(1, 999999):\n","\n","        eps = Variable(torch.FloatTensor(mean.size()).normal_().type(model.dtype))\n","        z = eps.mul(logvar.mul(0.5).exp_()).add_(mean)\n","        x_logits = model.decode(z)\n","\n","        logpz = log_normal(z, zeros, zeros)\n","        logqz = log_normal(z, mean, logvar)\n","        logpx = log_likelihood(x_logits, data_var)\n","\n","        optimizer.zero_grad()\n","        loss = -torch.mean(logpx + logpz - logqz)\n","        loss_np = loss.data.cpu().numpy()\n","        loss.backward()\n","        optimizer.step()\n","\n","        prev_seq.append(loss_np)\n","        if epoch % check_every == 0:\n","            last_avg = np.mean(prev_seq)\n","            if debug:  # debugging helper\n","                sys.stderr.write(\n","                    'Epoch %d, time elapse %.4f, last avg %.4f, prev best %.4f\\n' % \\\n","                    (epoch, time.time()-time_, -last_avg, -best_avg)\n","                )\n","            if last_avg < best_avg:\n","                sentinel, best_avg = 0, last_avg\n","            else:\n","                sentinel += 1\n","            if sentinel > sentinel_thres:\n","                break\n","            prev_seq = []\n","            time_ = time.time()\n","\n","    # evaluation\n","    eps = Variable(torch.FloatTensor(B*k, z_size).normal_().type(model.dtype))\n","    z = eps.mul(logvar.mul(0.5).exp_()).add_(mean)\n","\n","    logpz = log_normal(z, zeros, zeros)\n","    logqz = log_normal(z, mean, logvar)\n","    logpx = log_likelihood(model.decode(z), data_var)\n","    elbo = logpx + logpz - logqz\n","\n","    vae_elbo = torch.mean(elbo)\n","    iwae_elbo = torch.mean(log_mean_exp(elbo.view(k, -1).transpose(0, 1)))\n","\n","    return vae_elbo.data[0], iwae_elbo.data[0]\n","\n","\n","def main_ffg(arg_string):\n","    global args\n","    args = parser.parse_args(arg_string.split())\n","    args.cuda = not args.no_cuda and torch.cuda.is_available()\n","    train_loader, test_loader = get_loaders(\n","        dataset=args.dataset,\n","        evaluate=True, batch_size=args.batch_size\n","    )\n","    model = get_model(args.dataset, get_default_hparams())\n","    model.load_state_dict(torch.load(args.eval_path)['state_dict'])\n","    model.eval()\n","\n","    vae_record, iwae_record = [], []\n","    time_ = time.time()\n","    for i, (batch, _) in tqdm(enumerate(train_loader)):\n","        batch = Variable(batch.type(model.dtype))\n","        elbo, iwae = optimize_local_gaussian(log_bernoulli, model, batch, debug=args.debug)\n","        vae_record.append(elbo)\n","        iwae_record.append(iwae)\n","        print ('Local opt w/ ffg, batch %d, time elapse %.4f, ELBO %.4f, IWAE %.4f' % \\\n","            (i+1, time.time()-time_, elbo, iwae))\n","        print ('mean of ELBO so far %.4f, mean of IWAE so far %.4f' % \\\n","            (np.nanmean(vae_record), np.nanmean(iwae_record)))\n","        time_ = time.time()\n","\n","    print ('Finishing...')\n","    print ('Average ELBO %.4f, IWAE %.4f' % (np.nanmean(vae_record), np.nanmean(iwae_record)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_M2V7kxwoyc","colab_type":"text"},"source":["# Candidate 1039199: What I coded and adapted"]},{"cell_type":"code","metadata":{"id":"PQZy7p6kwH1c","colab_type":"code","colab":{}},"source":["parser = argparse.ArgumentParser(description='VAE')\n","# action configuration flags\n","parser.add_argument('--train', '-t', action='store_true')\n","parser.add_argument('--load-path', '-lp', type=str, default='NA',\n","                    help='path to load checkpoint to retrain')\n","parser.add_argument('--load-epoch', '-le', type=int, default=0,\n","                    help='epoch number to start recording when retraining')\n","parser.add_argument('--display-epoch', '-de', type=int, default=10,\n","                    help='print status every so many epochs (default: 10)')\n","parser.add_argument('--eval-iwae', '-ei', action='store_true')\n","parser.add_argument('--eval-ais', '-ea', action='store_true')\n","parser.add_argument('--n-iwae', '-ni', type=int, default=5000,\n","                    help='number of samples for IWAE evaluation (default: 5000)')\n","parser.add_argument('--n-ais-iwae', '-nai', type=int, default=100,\n","                    help='number of IMPORTANCE samples for AIS evaluation (default: 100). \\\n","                          This is different from MC samples.')\n","parser.add_argument('--n-ais-dist', '-nad', type=int, default=10000,\n","                    help='number of distributions for AIS evaluation (default: 10000)')\n","parser.add_argument('--ais-schedule', type=str, default='linear', help='schedule for AIS')\n","\n","parser.add_argument('--no-cuda', '-nc', action='store_true', help='force not use CUDA')\n","parser.add_argument('--visdom', '-v', action='store_true', help='visualize samples')\n","parser.add_argument('--port', '-p', type=int, default=8097, help='port for visdom')\n","parser.add_argument('--save-visdom', default='test', help='visdom save path')\n","parser.add_argument('--encoder-more', action='store_true', help='train the encoder more (5 vs 1)')\n","parser.add_argument('--early-stopping', '-es', action='store_true', help='apply early stopping')\n","parser.add_argument('--epochs', '-e', type=int, default=3280,\n","                    help='total num of epochs for training (default: 3280)')\n","parser.add_argument('--lr-schedule', '-lrs', action='store_true',\n","                    help='apply learning rate schedule')\n","\n","# model configuration flags\n","parser.add_argument('--z-size', '-zs', type=int, default=50,\n","                    help='dimensionality of latent code (default: 50)')\n","parser.add_argument('--batch-size', '-bs', type=int, default=100,\n","                    help='batch size (default: 100)')\n","parser.add_argument('--save-name', '-sn', type=str, default='model.pth',\n","                    help='name to save trained ckpt (default: model.pth)')\n","parser.add_argument('--eval-path', '-ep', type=str, default='model.pth',\n","                    help='path to load evaluation ckpt (default: model.pth)')\n","parser.add_argument('--dataset', '-d', type=str, default='mnist',\n","                    choices=['mnist', 'fashion', 'cifar'],\n","                    help='dataset to train and evaluate on (default: mnist)')\n","parser.add_argument('--wide-encoder', '-we', action='store_true',\n","                    help='use wider layer (more hidden units for FC, more channels for CIFAR)')\n","parser.add_argument('--has-flow', '-hf', action='store_true',\n","                    help='use flow for training and eval')\n","parser.add_argument('--hamiltonian-flow', '-hamil-f', action='store_true')\n","parser.add_argument('--n-flows', '-nf', type=int, default=2, help='number of flows')\n","parser.add_argument('--warmup', '-w', action='store_true',\n","                    help='apply warmup during training')\n","\n","# 1039199: I added these lines\n","parser.add_argument('--optimiser', type=str, default='Adam', \n","                    choices=['Adam', 'AdaDelta', 'SGD'],\n","                    help='Choose an optimiser for the VAE')\n","parser.add_argument('--init_lr', type=float, help='Initial learning rate ot the optimiser')\n","\n","\n","\n","def get_default_hparams():\n","    return HParams(\n","        z_size=args.z_size,\n","        act_func=F.elu,\n","        has_flow=args.has_flow,\n","        hamiltonian_flow=args.hamiltonian_flow,\n","        n_flows=args.n_flows,\n","        wide_encoder=args.wide_encoder,\n","        cuda=args.cuda,\n","    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3OQ5vWR-pwM","colab_type":"code","colab":{}},"source":["def load_checkpoint(model, optimizer, filename):\n","    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n","    start_epoch = 0\n","    if os.path.isfile(filename):\n","        print(\"=> loading checkpoint '{}'\".format(filename))\n","        checkpoint = torch.load(filename)\n","        start_epoch = checkpoint['epoch']\n","        model.load_state_dict(checkpoint['state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","        print(\"=> loaded checkpoint '{}' (epoch {})\"\n","                  .format(filename, checkpoint['epoch']))\n","    else:\n","        print(\"=> no checkpoint found at '{}'\".format(filename))\n","\n","    return model, optimizer, start_epoch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxWBkcIU0Nvq","colab_type":"code","colab":{}},"source":[" # Function to binarise the arrays in place\n","def static_binarise(d):\n","  ids = d < 0.5\n","  d[ids] = 0.\n","  d[~ids] = 1.\n","  return(d)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJ4bmfhRwurU","colab_type":"code","colab":{}},"source":["CUDA_LAUNCH_BLOCKING=1\n","\n","def train(model, train_loader, test_loader, \n","          optimiser_str, init_lr, # 1039199: I added these arguments\n","          k_train=1,  # num iwae sample for training\n","          k_eval=1,  # num iwae sample for eval\n","          epochs=3280, display_epoch=10, lr_schedule=True, \n","          warmup=True, warmup_thres=None,\n","          encoder_more=False,\n","          checkpoints=None, early_stopping=True, \n","          save=True, save_path='checkpoints/',\n","          patience=10):\n","  \n","  print('Training')\n","\n","  if args.load_path != 'NA':\n","    f = args.load_path\n","    model.load_state_dict(torch.load(f)['state_dict'])\n","\n","  # default warmup schedule\n","  if warmup_thres is None:\n","    if 'cifar' in save_path:\n","      warmup_thres = 50.\n","    elif 'mnist' in save_path or 'fashion' in save_path:\n","      warmup_thres = 400.\n","\n","  if checkpoints is None:  # save a checkpoint every display_epoch\n","    checkpoints = [1] + list(range(0, 3280, display_epoch))[1:] + [3280]\n","\n","  time_ = time.time()\n","\n","  if optimizer_str == 'Adam':\n","    current_lr = init_lr\n","    optimizer = optim.Adam(model.parameters(), lr=current_lr)\n","  elif optimizer_str == 'AdaDelta':\n","    current_lr = init_lr\n","    optimizer = optim.Adadelta(model.parameters(), lr=current_lr, \n","                                rho=0.9, eps=1e-06, weight_decay=0)\n","  elif optimizer_str == 'SGD':\n","    current_lr = init_lr\n","    optimizer = optim.SGD(model.parameters(), lr=current_lr,\n","                          momentum=0.9, dampening=1e-3)\n","\n","\n","  num_worse = 0  # compare against `patience` for early-stopping\n","  prev_valid_err = None\n","  best_valid_err = None\n","\n","  # 1039199: I fixed the lr_scheduler\n","  # 1039199: I fixed the load_checkpoints\n","  power = 0\n","  epoch_elapsed = 0\n","\n","  for epoch in tqdm(range(1, epochs+1), position=0, leave=True):\n","    warmup_const = min(1., epoch / warmup_thres) if warmup else 1.\n","    # lr schedule from IWAE: https://arxiv.org/pdf/1509.00519.pdf        \n","    if lr_schedule:\n","      if epoch_elapsed >= 3 ** power:\n","        # 1039199: I slowed the reduction of the learning rate\n","        if epoch_elapsed != 0:\n","          current_lr *= 10. ** (-1./20.)\n","        power += 1\n","        # 1039199: Correct way to do lr decay; also possible w/ `torch.optim.lr_scheduler`\n","        for param_group in optimizer.param_groups:\n","          param_group['lr'] = current_lr\n","      epoch_elapsed += 1\n","\n","    if epoch in checkpoints:\n","      model, optimizer, checkpoint_exists = load_checkpoint(model, optimizer, ('%s%d_%s' % (save_path, epoch, args.save_name)))\n","    \n","    if not checkpoint_exists:\n","      model.train()  # crucial for BN to work properly\n","      for _, (batch, _) in enumerate(train_loader):\n","        batch = Variable(batch)\n","        if args.cuda:\n","          batch = batch.cuda()\n","\n","        # train the encoder more\n","        if encoder_more:\n","          model.freeze_decoder()\n","          for _ in range(10):\n","            optimizer.zero_grad()\n","            elbo, _, _, _ = model.forward(batch, k_train, warmup_const)\n","            loss = -elbo\n","            loss.backward()\n","            optimizer.step()\n","          model.unfreeze_decoder()\n","\n","        optimizer.zero_grad()\n","        elbo, _, _, _ = model.forward(batch, k_train, warmup_const)\n","        loss = -elbo\n","        loss.backward()\n","        optimizer.step()\n","\n","      if epoch % display_epoch == 0:\n","        model.eval()  # crucial for BN to work properly\n","\n","        train_logpx, test_logpx = [], []\n","        train_logpz, test_logpz = [], []\n","        train_logqz, test_logqz = [], []\n","        train_stats, test_stats = [], []\n","        for _, (batch, _) in enumerate(train_loader):\n","          batch = Variable(batch)\n","          if args.cuda:\n","            batch = batch.cuda()\n","          elbo, logpx, logpz, logqz = model(batch, k=1)\n","          train_stats.append(elbo.data[0])\n","          train_logpx.append(logpx.data[0])\n","          train_logpz.append(logpz.data[0])\n","          train_logqz.append(logqz.data[0])\n","\n","        for _, (batch, _) in enumerate(test_loader):\n","          batch = Variable(batch)\n","          if args.cuda:\n","            batch = batch.cuda()\n","          # early stopping with iwae bound\n","          elbo, logpx, logpz, logqz = model(batch, k=k_eval)\n","          test_stats.append(elbo.data[0])\n","          test_logpx.append(logpx.data[0])\n","          test_logpz.append(logpz.data[0])\n","          test_logqz.append(logqz.data[0])\n","        print (\n","            'Train Epoch: [{}/{}]'.format(epoch, epochs),\n","            'Train set ELBO {:.4f}'.format(np.mean(np.asarray(train_stats))),\n","            'Test/Validation set IWAE {:.4f}'.format(np.mean(np.asarray(test_stats))),\n","            'Time: {:.2f}'.format(time.time()-time_),\n","        )\n","        time_ = time.time()\n","\n","        if early_stopping:\n","          curr_valid_err = np.mean(test_stats)\n","\n","          if prev_valid_err is None:  # don't have history yet\n","            prev_valid_err = curr_valid_err\n","          elif curr_valid_err >= prev_valid_err:  # performance improved\n","            prev_valid_err = curr_valid_err\n","            num_worse = 0\n","          else:\n","            num_worse += 1\n","\n","          if num_worse >= patience:\n","            print(\"Stopped early\")\n","            break\n","\n","      if save and (epoch in checkpoints):\n","        if not checkpoint_exists:\n","          state = {'epoch': epoch + args.load_epoch, 'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),}\n","          torch.save(state, '%s%d_%s' % (save_path, epoch + args.load_epoch, args.save_name))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CDXrw0QYy5x-","colab_type":"code","colab":{}},"source":["CUDA_LAUNCH_BLOCKING=1\n","\n","def run(arg_string):\n","    global args\n","    args = parser.parse_args(arg_string.split()) #parser.parse_args()\n","    # args.eval_path = '/content/drive/My Drive/ATiML' + args.eval_path\n","    # Sanity check to know which arguments are being parsed\n","    print(args)\n","    args.cuda = not args.no_cuda and torch.cuda.is_available()\n","    print(args.cuda)\n","    train_loader, test_loader = get_loaders(\n","        dataset=args.dataset,\n","        evaluate=args.eval_iwae or args.eval_ais, # HERE\n","        batch_size=args.batch_size\n","    )\n","    model = get_model(args.dataset, get_default_hparams())\n","\n","    if args.train:\n","        save_path = 'expansion2/%s/lr_%d/bs_%s/' % (\n","                        args.optimiser,\n","                        args.init_lr,\n","                        args.batch_size\n","                    )\n","        if not os.path.exists(save_path):\n","          os.makedirs(save_path)\n","\n","        train_AdaBound(\n","            model, train_loader, test_loader,\n","            display_epoch=args.display_epoch, epochs=args.epochs,\n","            lr_schedule=args.lr_schedule,\n","            warmup=args.warmup,\n","            early_stopping=args.early_stopping,\n","            encoder_more=args.encoder_more,\n","            save=True, save_path=save_path\n","        )\n","\n","    if args.visdom:\n","        vis = visdom.Visdom(env=args.save, port=args.port)\n","        model.load_state_dict(torch.load(args.eval_path)['state_dict'])\n","\n","        # plot original images\n","        batch, _ = train_loader.next()\n","        images = list(batch.numpy())\n","        win_samples = vis.images(images, 10, 2, opts={'caption': 'original images'}, win=None)\n","\n","        # plot reconstructions\n","        batch = Variable(batch.type(model.dtype))\n","        reconstruction = model.reconstruct_img(batch)\n","        images = list(reconstruction.data.cpu().numpy())\n","        win_samples = vis.images(images, 10, 2, opts={'caption': 'reconstruction'}, win=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2u5FQVVoSwf8","colab_type":"code","colab":{}},"source":["def eval_mod(arg_string):\n","    global args\n","    args = parser.parse_args(arg_string.split()) #parser.parse_args()\n","    # args.eval_path = '/content/drive/My Drive/ATiML' + args.eval_path\n","    # Sanity check to know which arguments are being parsed\n","    print(args)\n","    args.cuda = not args.no_cuda and torch.cuda.is_available()\n","    print(args.cuda)\n","    train_loader, test_loader = get_loaders(\n","        dataset=args.dataset,\n","        evaluate=args.eval_iwae or args.eval_ais, # HERE\n","        batch_size=args.batch_size\n","    )\n","    model = get_model(args.dataset, get_default_hparams())\n","\n","\n","    if args.eval_iwae:\n","        # VAE bounds computed w/ 100 MC samples to reduce variance\n","        train_res, test_res = [], []\n","        for _ in range(100):\n","            test_iwae(model, train_loader, k=1, f=args.eval_path)\n","            test_iwae(model, test_loader, k=1, f=args.eval_path)\n","            #print(\"about to append\")\n","            train_res.append(train_res)\n","            #print(\"appended train_res\")\n","            test_res.append(test_res)\n","            #print(\"appended test_res\")\n","\n","        print(\"exited for loop\")\n","        print(\"length of train_res = {}\".format(len(train_res[0])))\n","        train_mean = np.mean(train_res)\n","        print(\"finished calculating mean\")\n","        print ('Training set VAE ELBO w/ 100 MC samples: %.4f' % train_mean)\n","        print ('Test set VAE ELBO w/ 100 MC samples: %.4f' % np.mean(test_res))\n","\n","        # IWAE bounds\n","        test_iwae(model, train_loader, k=args.n_iwae, f=args.eval_path)\n","        test_iwae(model, test_loader, k=args.n_iwae, f=args.eval_path)\n","\n","    if args.eval_ais:\n","      print(\"Start evaluating\")\n","      model.load_state_dict(torch.load(args.eval_path)['state_dict'])\n","      schedule_fn = linear_schedule if args.ais_schedule == 'linear' else sigmoidial_schedule\n","      schedule = schedule_fn(args.n_ais_dist)\n","      ais_trajectory(\n","          model, train_loader,\n","          mode='forward', schedule=schedule, n_sample=args.n_ais_iwae\n","      )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFnIzeu4z0OB","colab_type":"text"},"source":["# Experiments"]},{"cell_type":"code","metadata":{"id":"FiqdM25N0mfX","colab_type":"code","colab":{}},"source":["# To activate GPU\n","use_cuda = True\n","print(torch.cuda.is_available())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFmW_EUW6unM","colab_type":"code","colab":{}},"source":["# SGD\n","run(\"--train --dataset mnist --lr-schedule --warmup --early-stopping --optimiser SGD --init_lr 0.001\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpn3ClBRW0Im","colab_type":"code","colab":{}},"source":["# AdaDelta lr=1e-3\n","run(\"--train --dataset mnist --lr-schedule --warmup --early-stopping --optimiser AdaDelta --init_lr 0.001\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jn7Wi2Hh3PZ6","colab_type":"code","colab":{}},"source":["eval_mod(\"--eval-ais  --dataset mnist --eval-path ./expansion2/SGD/lr_1e-3/bs100_3280_model.pth\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-wZj4v44OMk","colab_type":"code","colab":{}},"source":["eval_mod(\"--eval-ais  --dataset mnist --eval-path ./expansion2/AdaDelta/lr_1e-3/3280_model.pth\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjhWXuyf12k2","colab_type":"code","colab":{}},"source":["main_ffg(\"--dataset mnist --eval-path ./expansion2/SGD/lr_1e-3/bs100_3280_model.pth --debug\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUV4Jhcb8JQW","colab_type":"code","colab":{}},"source":["main_ffg(\"--dataset mnist --eval-path ./expansion2/AdaDelta/lr_1e-3/3280_model.pth --debug\")"],"execution_count":0,"outputs":[]}]}